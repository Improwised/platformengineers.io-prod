__NUXT_JSONP__("/blog", (function(a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w,x,y,z,A,B,C,D,E,F,G,H,I,J,K,L,M,N,O,P,Q,R,S,T,U,V,W,X,Y,Z,_,$,aa,ab,ac,ad,ae,af,ag,ah,ai,aj,ak,al,am,an,ao,ap,aq,ar,as,at,au,av,aw,ax,ay,az,aA,aB,aC,aD,aE,aF,aG,aH,aI,aJ,aK,aL,aM,aN,aO,aP,aQ,aR,aS,aT,aU,aV,aW,aX,aY,aZ,a_,a$,ba,bb,bc,bd,be,bf,bg,bh,bi,bj,bk,bl,bm,bn,bo,bp,bq,br,bs,bt,bu,bv){return {data:[{blogList:[{id:U,status:o,sort:a,date_created:"2024-09-10T05:56:19.973Z",date_updated:"2024-10-07T08:24:04.767Z",slug:"teleport-for-simplified-kubernetes-access-management",title:an,description:"\u003Cp\u003EFrom Complexity to Control: Using Teleport for Simplified Kubernetes Access Management\u003C\u002Fp\u003E",seo_title:an,seo_description:an,content:"\u003Cp dir=\"ltr\"\u003ESecurely managing access to modern infrastructure is a paramount concern, whether it's on-premises servers, cloud instances, Kubernetes clusters, databases, or other critical resources. The challenge lies in striking a balance between providing seamless access and maintaining airtight security, all while ensuring compliance with regulatory requirements. This is where Teleport steps in, offering a robust solution to streamline infrastructure access and management while prioritizing security, auditability, and compliance.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EKubernetes access management, also known as Role-Based Access Control (RBAC), is a method of controlling who can access the Kubernetes API and what actions they can perform across the cluster. It is a critical part of the overall Kubernetes security strategy. Here is a detailed demonstration of Kubernetes access management:\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003E\u003Cstrong\u003EUsers and Service Accounts:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EUsers: Kubernetes does not manage user identities directly. Instead, it relies on external identity management systems. Users are authenticated and then bound to roles or cluster roles through RoleBindings or ClusterRoleBindings.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EService Accounts: These are used to authenticate pods and services within the cluster. Each pod is associated with a service account, and the service account's credentials are used to authenticate the pod's requests to the Kubernetes API.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003E\u003Cstrong\u003ERoles and ClusterRoles:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003ERoles: Define a set of permissions within a namespace. They specify what actions can be performed on resources within that namespace.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EClusterRoles: Similar to roles but apply to cluster-wide resources and are not limited to a specific namespace.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003E\u003Cstrong\u003ERoleBindings and ClusterRoleBindings:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003ERoleBindings: Bind a role to a user or service account within a namespace, granting the specified permissions.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EClusterRoleBindings: Bind a cluster role to a user or service account, granting cluster-wide permissions.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\"\u003E\u003Cstrong\u003EChallenge\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003EManaging access to a Kubernetes cluster can be a complex and challenging task. It requires balancing security, scalability, and usability while ensuring compliance with regulatory requirements. As the number of users and clusters grows, managing access can become increasingly difficult, making it essential to have a robust access management system in place.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EHere are some of the challenges that organizations may face when giving access to a Kubernetes cluster:\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003ESecuring Kubernetes clusters presents significant challenges. Exposing the cluster to unauthorized access increases the risk of security breaches, data theft, and unauthorized modifications.&nbsp;\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003EThe complexity of Kubernetes environments makes managing access and ensuring appropriate permissions difficult, especially as the number of users and clusters grows. Multi-tenant setups amplify these challenges, requiring robust isolation of resources. Managing keys, certificates, and credentials adds overhead and potential error points.&nbsp;\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003EAuditing, compliance, and providing temporary access to external parties further complicate the security landscape. Implementing least privilege access and effective monitoring and logging are essential but require careful planning and execution.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch3 dir=\"ltr\"\u003E\u003Cstrong\u003ESolution\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003ETeleport is a robust and versatile tool that serves as a secure gateway to access SSH and Windows servers, Kubernetes clusters, databases, and web applications with unparalleled ease and reliability. It not only facilitates secure connectivity across diverse infrastructure components but also enhances operational efficiency by centralizing access management and enforcing stringent security policies.&nbsp;\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EWhether managing containerized applications in Kubernetes environments or ensuring controlled access to critical databases and web services, Teleport offers comprehensive solutions tailored to meet the complex security and compliance needs of modern enterprises.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cimg style=\"height: auto;\" src=\"https:\u002F\u002Fdata.improwised.com\u002Fassets\u002F796d1ec9-2846-4657-8342-a6e48a422cb5.png?width=900&amp;height=610\" alt=\"Teleport process\"\u003E\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003ETeleport provides a more efficient and secure way to manage access to Kubernetes clusters. It offers granular control over user permissions, audit logs, and session recordings. This allows us to streamline administration, reduce the attack surface, and ensure compliance with regulatory requirements.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\"\u003E\u003Cstrong\u003EDeployment Guide\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EStep 1: Add the Teleport Helm Repository\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fgravitational\u002Fteleport\u002Ftree\u002Fmaster\u002Fexamples\u002Fchart\u002Fteleport-cluster\"\u003Ehttps:\u002F\u002Fgithub.com\u002Fgravitational\u002Fteleport\u002Ftree\u002Fmaster\u002Fexamples\u002Fchart\u002Fteleport-cluster\u003C\u002Fa\u003E\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EOpen a terminal and run the following command to add the Teleport Helm repository\u003C\u002Fp\u003E\n\u003Cdiv dir=\"ltr\" align=\"left\"\u003E\n\u003Cpre\u003E\u003Ccode\u003Ehelm repo add teleport https:\u002F\u002Fcharts.releases.teleport.dev\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EStep 2: Update the Helm Repository\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003ERun the following command to update the Helm repository:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003Ehelm repo update\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003E\u003Cstrong\u003EStep 3: Install Teleport with following teleport-cluster-values.yaml\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003EapiVersion: helm.toolkit.fluxcd.io\u002Fv2beta1\nkind: HelmRelease\nmetadata:\n  name: teleport\n  namespace: teleport\nspec:\n  chart:\n    spec:\n      chart: teleport-cluster\n      reconcileStrategy: ChartVersion\n      version: 16.2.0\n      sourceRef:\n        kind: HelmRepository\n        name: teleport-helm-source\n        namespace: flux-system\n  interval: 10m0s\n  values:\n    clusterName: teleport.improwised.com\n    proxyListenerMode: multiplex\n    persistence:\n      volumeSize: 3Gi\n    proxy:\n      service:\n        type: ClusterIP\n      annotations:\n        ingress:\n          nginx.ingress.kubernetes.io\u002Fbackend-protocol: 'HTTPS'\n          cert-manager.io\u002Fcluster-issuer: letsencrypt-prod\n      highAvailability:\n        replicaCount: 1\n    ingress:\n      enabled: true\n      suppressAutomaticWildcards: true\n      spec:\n        ingressClassName: nginx\n    authentication:\n      secondFactor: \"off\"\n    kubeClusterName: \"cluster-name\"\n    highAvailability:\n      replicaCount: 1\n      certManager:\n        enabled: true\n        issuerName: \"letsencrypt-prod\"\n        issuerKind: ClusterIssuer\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp dir=\"ltr\"\u003ERun the following command to install Teleport to your Kubernetes cluster:\u003C\u002Fp\u003E\n\u003Cdiv dir=\"ltr\" align=\"left\"\u003E\n\u003Cpre\u003E\u003Ccode\u003Ehelm install teleport-cluster teleport\u002Fteleport-cluster \\\n  --create-namespace \\\n  --version 16.0.3 \\\n  --values teleport-cluster-values.yaml\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cimg src=\"https:\u002F\u002Flh7-rt.googleusercontent.com\u002Fdocsz\u002FAD_4nXc7JCUtHzyycAxjXrcuIMVw_-kYZA23MQXprDTA1FM7AWKMFUpqiIMerP_XtOQGoNqZ4dgc8Usszk8EIJeJs2w6qrfHnWW18ytTad6fzSskoQx_zY8eNQ-q6hshTvzH-Uoc4utynBLhmqYEpI32bij7I7c?key=YgVxM6yyLCGPHFNJRoippg\" width=\"800\" height=\"620\"\u003E\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\"\u003E\u003Cstrong\u003ECreating role based access using teleport roles:\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cimg src=\"https:\u002F\u002Flh7-rt.googleusercontent.com\u002Fdocsz\u002FAD_4nXdQrxN-NM6TQXuubiWHrQTY8hiL6Q-T1cHsE43zYox8a_InyQuSfRKjxwHYGnwTUMLsnN15i6__yGICssinc_LfEMWoHTDetsFi0uRaosroP31iOfr641p8ceXxuzSP68k0CmIljp1ZNR7oL2h7Yg1RNKMk?key=YgVxM6yyLCGPHFNJRoippg\" width=\"860\" height=\"340\"\u003E\u003Cstrong\u003E\u003Cbr\u003E\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EAfter adding the cluster into teleport One can find the the guide to add the user and their access roles from teleport itself\u003C\u002Fp\u003E\n\u003Cdiv dir=\"ltr\" align=\"left\"\u003E\n\u003Cpre\u003E\u003Ccode\u003Ekind: role\nversion: v7\nmetadata:\n  name: alice\nspec:\n  allow:\n    logins: ['admin']\n    kubernetes_groups: ['edit']\n    node_labels:\n      '*': '*'\n    kubernetes_labels:\n      '*': '*'\n    kubernetes_resources:\n      - kind: '*'\n        namespace: '*'\n        name: '*'\n        verbs: ['*']\n---\nkind: role\nversion: v7\nmetadata:\n  name: bob\nspec:\n  allow:\n    logins: ['ubuntu']\n    kubernetes_groups: ['view']\n    node_labels:\n      '*': '*'\n    kubernetes_labels:\n      '*': '*'\n    kubernetes_resources:\n      - kind: '*'\n        namespace: '*'\n        name: '*'\n        verbs: ['*']\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Ch2 style=\"font-size: 28px;\"\u003E\u003Cstrong\u003E&nbsp;Use Teleport for Kubernetes access management&nbsp;\u003C\u002Fstrong\u003E\u003C\u002Fh2\u003E\n\u003Ch3 dir=\"ltr\"\u003E1. Deploy Teleport\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EStep 1: Add the Teleport Helm Repository\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fgravitational\u002Fteleport\u002Ftree\u002Fmaster\u002Fexamples\u002Fchart\u002Fteleport-cluster\"\u003Ehttps:\u002F\u002Fgithub.com\u002Fgravitational\u002Fteleport\u002Ftree\u002Fmaster\u002Fexamples\u002Fchart\u002Fteleport-cluster\u003C\u002Fa\u003E\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EOpen a terminal and run the following command to add the Teleport Helm repository\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003Ehelm repo add teleport https:\u002F\u002Fcharts.releases.teleport.dev\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003E\u003Cstrong\u003E&nbsp;\u003C\u002Fstrong\u003E\u003Cstrong\u003EStep 2: Update the Helm Repository\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003E&nbsp;\u003C\u002Fstrong\u003ERun the following command to update the Helm repository:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003Ehelm repo update\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003E\u003Cstrong\u003E&nbsp;Step 3: Install Teleport with following teleport-cluster-values.yaml\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003ERun the following command to install Teleport to your Kubernetes cluster:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003Ehelm install teleport-cluster teleport\u002Fteleport-cluster \\\n  --create-namespace \\\n  --version 16.2.0 \\\n  --values teleport-cluster-values.yaml\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cdiv dir=\"ltr\" align=\"left\"\u003E\n\u003Cpre\u003E\u003Ccode\u003EapiVersion: helm.toolkit.fluxcd.io\u002Fv2beta1\nkind: HelmRelease\nmetadata:\n  name: teleport\n  namespace: teleport\nspec:\n  chart:\n    spec:\n      chart: teleport-cluster\n      reconcileStrategy: ChartVersion\n      version: 16.2.0\n      sourceRef:\n        kind: HelmRepository\n        name: teleport-helm-source\n        namespace: flux-system\n  interval: 10m0s\n  values:\n    clusterName: teleport.improwised.com\n    proxyListenerMode: multiplex\n    persistence:\n      volumeSize: 3Gi\n    proxy:\n      service:\n        type: ClusterIP\n      annotations:\n        ingress:\n          nginx.ingress.kubernetes.io\u002Fbackend-protocol: 'HTTPS'\n          cert-manager.io\u002Fcluster-issuer: letsencrypt-prod\n      highAvailability:\n        replicaCount: 1\n    ingress:\n      enabled: true\n      suppressAutomaticWildcards: true\n      spec:\n        ingressClassName: nginx\n    authentication:\n      secondFactor: \"off\"\n    kubeClusterName: \"cluster-name\"\n    highAvailability:\n      replicaCount: 1\n      certManager:\n        enabled: true\n        issuerName: \"letsencrypt-prod\"\n        issuerKind: ClusterIssuer\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Ch3 dir=\"ltr\"\u003E2. Register Kubernetes Clusters\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003EOnce the Teleport cluster is deployed, you can register your Kubernetes clusters with Teleport. One needs to configure the teleport-kube-agent in the cluster which you want to add in the teleport.&nbsp;\u003C\u002Fp\u003E\n\u003Ch4 dir=\"ltr\"\u003EManually Register a Kubernetes Cluster\u003C\u002Fh4\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003E\u003Cstrong\u003EDeploy the Teleport Kubernetes Service:\u003C\u002Fstrong\u003E\u003Cbr\u003E\u003Ca href=\"https:\u002F\u002Fwww.youtube.com\u002Fwatch?v=2diX_UAmJ1c\"\u003E\u003Cspan data-rich-links=\"{&quot;fple-t&quot;:&quot;Adding your first Kubernetes cluster to Teleport&quot;,&quot;fple-u&quot;:&quot;https:\u002F\u002Fwww.youtube.com\u002Fwatch?v=2diX_UAmJ1c&quot;,&quot;fple-mt&quot;:null,&quot;type&quot;:&quot;first-party-link&quot;}\"\u003EAdding your first Kubernetes cluster to Teleport\u003C\u002Fspan\u003E\u003C\u002Fa\u003E\u003Cbr\u003EHere you have to add&nbsp; The address of your Teleport proxy server in proxyAddr and the token of the teleport cluster in the authToken.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EAdd the Teleport Helm Repository:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003Ehelm repo add teleport https:\u002F\u002Fcharts.releases.teleport.dev\nhelm repo update\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cpre\u003E\u003Ccode\u003Ehelm install ${RELEASE_NAME} teleport\u002Fteleport-kube-agent \\\n  --create-namespace \\\n  --namespace ${NAMESPACE} \\\n  --set roles=\"kube\" \\\n  --set proxyAddr=${PROXY_ENDPOINT} \\\n  --set authToken=${JOIN_TOKEN} \\\n  --set kubeClusterName=${KUBERNETES_CLUSTER_NAME}\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003E\u003Cstrong\u003EVerify the Teleport Pod:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003Ekubectl -n teleport-agent get pods\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Ch3\u003E3. Configure Access Controls\u003C\u002Fh3\u003E\n\u003Cstrong\u003EStep 1: Configure Kubernetes ClusterRole and ClusterRoleBinding\u003C\u002Fstrong\u003E\n\u003Cp dir=\"ltr\"\u003ETo manage access to your Kubernetes clusters using RBAC, you'll create a ClusterRole and a ClusterRoleBinding.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003ECreate a ClusterRole\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003EapiVersion: rbac.authorization.k8s.io\u002Fv1\nkind: ClusterRole\nmetadata:\n  name: app-pod-viewer\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\", \"pods\u002Flog\"]\n  verbs: [\"get\", \"watch\", \"list\"]\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp dir=\"ltr\"\u003ECreate a file named clusterrole.yaml with the following configuration to define permissions for viewing pods and logs:\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003ECreate a ClusterRoleBinding\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003ECreate a file named clusterrolebinding.yaml to bind the ClusterRole to a specific group:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003EapiVersion: rbac.authorization.k8s.io\u002Fv1\nkind: ClusterRoleBinding\nmetadata:\n  name: app-pod-viewer\nsubjects:\n- kind: Group\n  name: app-developers\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: ClusterRole\n  name: app-pod-viewer\n  apiGroup: rbac.authorization.k8s.io\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003E\u003Cstrong\u003EApply the Configurations\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EApply the ClusterRole and ClusterRoleBinding using the following commands:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003Ekubectl apply -f clusterrole.yaml\nkubectl apply -f clusterrolebinding.yaml \u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003C\u002Fdiv\u003E\n\u003Cdiv dir=\"ltr\" align=\"left\"\u003E&nbsp;\u003C\u002Fdiv\u003E\n\u003Cdiv dir=\"ltr\" align=\"left\"\u003E\u003Cstrong\u003EStep 2: Configure Access Controls in Teleport\u003C\u002Fstrong\u003E\n\u003Cp dir=\"ltr\"\u003ETeleport can be used to manage Kubernetes access with additional controls. Follow these steps:\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003ECreate a Role in Teleport\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EGo to the Teleport UI, navigate to Roles, and add a new role with the following YAML configuration:\u003C\u002Fp\u003E\n\u003Cdiv dir=\"ltr\" align=\"left\"\u003E\n\u003Cpre\u003E\u003Ccode\u003Ekind: role\nmetadata:\n  name: app-pod-viewer\nspec:\n  allow:\n    kubernetes_groups:\n    - app-developers\n    kubernetes_labels:\n      '*': '*'\n    kubernetes_resources:\n    - kind: pod\n      name: '*'\n      namespace: app\n    kubernetes_users:\n    - app-pod-viewer\n  deny: {}\n  options:\n    cert_format: standard\n    create_db_user: false\n    create_desktop_user: false\n    desktop_clipboard: true\n    desktop_directory_sharing: true\n    enhanced_recording:\n    - command\n    - network\n    forward_agent: false\n    idp:\n      saml:\n        enabled: true\n    max_session_ttl: 30h0m0s\n    pin_source_ip: false\n    port_forwarding: true\n    record_session:\n      default: best_effort\n      desktop: true\n    ssh_file_copy: true\nversion: v7\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cstrong\u003EAccess Resources\u003C\u002Fstrong\u003E\n\u003Cp dir=\"ltr\"\u003EAuthenticate to your Kubernetes cluster via Teleport and test your access controls.&nbsp;\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003ECreate a User in Teleport\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003EGo to Users in the Teleport UI and create a new user.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003EA link will be provided to set the user's password.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003ELog in via CLI\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EUse the following commands in the CLI to authenticate and access the Kubernetes cluster:\u003C\u002Fp\u003E\n\u003Cdiv dir=\"ltr\" align=\"left\"\u003E\n\u003Cpre\u003E\u003Ccode\u003Etsh logout \ntsh login --proxy= --user=app \ntsh kube login  --as app-pod-viewer \u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EAccess Pods and Logs\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EAfter logging in, you can run the following commands to interact with the Kubernetes resources:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003Etsh kubectl get pods -n app \ntsh kubectl logs pods\u002Fapp-ui-svc-xyz -n app\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EAuditor and Monitor\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EUse Teleport's auditing capabilities to monitor and record kubectl sessions.\u003C\u002Fp\u003E\n\u003Ch4 dir=\"ltr\"\u003EView Audit Log\u003C\u002Fh4\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003EAccess the Teleport Web Interface: Navigate to the Teleport web interface.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003EView Audit Log: Go to the \"Audit Log\" section to view recorded kubectl sessions.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EBy following these steps, you can effectively use Teleport to manage access to your Kubernetes clusters, ensuring secure and controlled access with fine-grained RBAC and auditing capabilities.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EAfter deploying Teleport, you can configure it using various methods to suit your needs. Here are some key points to consider:\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\"\u003EConfiguration Methods\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003E\u003Cstrong\u003EStatic Configuration File:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EAt startup, Teleport reads a configuration file from the local filesystem (default path is \u002Fetc\u002Fteleport.yaml).\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EThis file controls aspects of the cluster that are not expected to change frequently, such as the ports that services listen on.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003E\u003Cstrong\u003EDynamic Resources:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EDynamic resources control aspects of your cluster that are likely to change over time, such as roles, local users, and registered infrastructure resources.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EThese can be managed using infrastructure as code tools like Terraform, Helm, and the tctl client tool.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\"\u003EConfiguration Fields\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EService Configuration:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EYou can enable or disable various Teleport services by including the appropriate configuration in your Teleport configuration file. For example:&nbsp;service_name: enabled: \"no\"\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003ESupported services include:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003EApplication Service (app_service)\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003EAuth Service (auth_service)\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003EDatabase Service (db_service)\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003EDiscovery Service (discovery_service)\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003EKubernetes Service (kubernetes_service)\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003EProxy Service (proxy_service)\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003ESSH Service (ssh_service)\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003EDesktop Service (windows_desktop_service)\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003EJamf Service (jamf_service).\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch3 dir=\"ltr\"\u003EHigh Availability Configuration\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003EStorage Configuration: The storage section configures the cluster state backend and session recording backend for the Auth Service.\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003EExample:\nversion: v3\nteleport:\n  Storage:\n# ...\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp dir=\"ltr\"\u003EConsult the Backends Reference for specific fields to set in the storage section.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EAuth Service Configuration:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EThe auth_service section configures the Auth Service.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EExample:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003Eversion: v3\nteleport:\n  storage:\n    # ...\n  auth_service:\n    enabled: true\n    cluster_name: \"mycluster.example.com\"\n    # Remove this if not using Teleport Enterprise\n    license_file: \"\u002Fvar\u002Flib\u002Fteleport\u002Flicense.pem\"\n  Proxy_service:\nenabled: false\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp dir=\"ltr\"\u003EThis configuration enables the Auth Service and specifies the cluster name and license file if using Teleport Enterprise.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\"\u003ECustom Configuration with Helm\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003ECustom YAML: You can provide custom YAML under proxy.teleportConfig and auth.teleportConfig to override elements of the default Teleport Proxy Service and Auth Service configurations.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EExample:\u003C\u002Fp\u003E\n\u003Cdiv dir=\"ltr\" align=\"left\"\u003E\n\u003Cpre\u003E\u003Ccode\u003Eproxy:\n  teleportConfig:\n    proxy_service:\n      public_addr: \"mycluster.example.com:443\"\nauth:\n  teleportConfig:\n    auth_service:\n      enabled: true\ncluster_name: \"mycluster.example.com\"\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp dir=\"ltr\"\u003EAny YAML provided under a teleportConfig section will be merged with the chart's default YAML configuration, with your overrides taking precedence.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\"\u003EAdditional Considerations\u003C\u002Fh3\u003E\n\u003Cul\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003ESingle Sign-On (SSO): For production deployments, it is recommended to set up Single Sign-On with your provider of choice.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003EKubernetes RBAC: Fine-tune your Kubernetes RBAC to enable fine-grained controls for accessing Kubernetes resources.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp dir=\"ltr\"\u003EBy following these guidelines, you can effectively configure your Teleport deployment to meet your specific requirements.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\"\u003EBest Practices\u003C\u002Fh3\u003E\n\u003Col\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003EMinimize API Server Flags:\u003Cbr\u003EAvoid using flags like --insecure-port, --insecure-bind-address, and --anonymous-auth to reduce security risks.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003EFollow Least Privilege:\u003Cbr\u003EAssign the minimum privileges necessary for users or service accounts to perform their tasks. Use roles instead of cluster roles whenever possible, and avoid using wildcards in permissions.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003EAvoid Default Service Accounts:\u003Cbr\u003ECreate custom service accounts instead of using default ones to ensure better control over permissions.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003C\u002Fol\u003E\n\u003Ch3 dir=\"ltr\"\u003EConclusion&nbsp;\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003ETeleport provides a scalable solution to manage Kubernetes access, balancing security, usability, and compliance. It addresses the complexity of Kubernetes environments and multi-tenant setups by streamlining key, certificate, and credential management.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EPrior to adopting Teleport, organizations encountered significant challenges with native Kubernetes RBAC. The manual management of RBAC across numerous clusters and users proved to be complex and prone to errors. As the number of users and clusters increased, the complexity and administrative overhead associated with RBAC also grew, leading to scalability issues. The default security settings and static credentials were vulnerable to unauthorized access and data breaches. Compliance and audit logging were difficult in multi-tenant environments, making it hard to meet regulations.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong id=\"docs-internal-guid-415eb79b-7fff-3fb9-3775-f4aa0babd1ab\"\u003E\u003Cbr\u003E\u003Cbr\u003E\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003C\u002Fdiv\u003E\n\u003C\u002Fdiv\u003E\n\u003C\u002Fdiv\u003E\n\u003C\u002Fdiv\u003E\n\u003C\u002Fdiv\u003E\n\u003C\u002Fdiv\u003E\n\u003C\u002Fdiv\u003E",tags:a,time_to_read:w,user_created:{id:f,first_name:g,last_name:h,email:k,password:b,location:a,title:l,description:a,tags:a,avatar:i,language:a,tfa_secret:a,status:c,role:j,token:a,last_access:m,last_page:n,provider:d,external_identifier:a,auth_data:a,email_notifications:e,appearance:a,theme_dark:a,theme_light:a,theme_light_overrides:a,theme_dark_overrides:a},user_updated:{id:f,first_name:g,last_name:h,email:k,password:b,location:a,title:l,description:a,tags:a,avatar:i,language:a,tfa_secret:a,status:c,role:j,token:a,last_access:m,last_page:n,provider:d,external_identifier:a,auth_data:a,email_notifications:e,appearance:a,theme_dark:a,theme_light:a,theme_light_overrides:a,theme_dark_overrides:a},image:{id:"300ef007-838f-4613-99e0-ab20b9ac6442",storage:p,filename_disk:"300ef007-838f-4613-99e0-ab20b9ac6442.png",filename_download:"Untitled_design__7_-removebg-preview (1).png",title:"Untitled Design  7  Removebg Preview (1)",type:r,folder:q,uploaded_by:f,created_on:aB,modified_by:a,modified_on:"2024-09-10T06:10:33.794Z",charset:a,filesize:"140273",width:I,height:I,duration:a,embed:a,description:a,location:a,tags:a,metadata:{},focal_point_x:a,focal_point_y:a,tus_id:a,tus_data:a,uploaded_on:aB},authors:[{id:39,pe_blog_id:U,directus_users_id:{first_name:g,last_name:h,avatar:i}},{id:40,pe_blog_id:U,directus_users_id:{first_name:"Shivani",last_name:"Rathod",avatar:"85df262b-c9e4-4277-a3d3-850554e43263"}}]},{id:V,status:o,sort:a,date_created:"2024-09-02T07:00:07.676Z",date_updated:"2024-09-06T09:28:00.450Z",slug:"implementing-feature-flags-with-flipt",title:aC,description:"\u003Cp\u003E\u003Cstrong\u003E Effectively implement feature flags in your Golang application using Flipt and gRPC.\u003C\u002Fstrong\u003E This comprehensive guide covers everything from setting up Flipt to integrating it with your Golang service using gRPC.\u003C\u002Fp\u003E",seo_title:aC,seo_description:"Effectively implement feature flags in your Golang application using Flipt and gRPC. This comprehensive guide covers everything from setting up Flipt to integrating it with your Golang service using gRPC.",content:"\u003Cp dir=\"ltr\"\u003EFeature flags are a powerful tool in software development, allowing developers to enable or disable features in their applications dynamically. This can be particularly useful for continuous delivery, A\u002FB testing, and managing the rollout of new features. In this blog, we will explore how to implement feature flags using Flipt, a self-contained feature flag solution, with Golang and gRPC. We will also delve into the benefits of using gRPC and how it can enhance internal communication and feature testing.\u003C\u002Fp\u003E\n\u003Ch2 dir=\"ltr\"\u003EWhat is Flipt?\u003C\u002Fh2\u003E\n\u003Cp dir=\"ltr\"\u003EFlipt is an open-source, self-hosted feature flag application that allows you to run experiments across services in your environment. It is built with Go and uses gRPC for communication, making it highly performant and scalable. The UI is built with Vue.js, providing a clean and intuitive interface for managing feature flags. (Here is the link to the GitHub page of Flipt: \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fflipt-io\u002Fflipt\"\u003Ehttps:\u002F\u002Fgithub.com\u002Fflipt-io\u002Fflipt\u003C\u002Fa\u003E )\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\"\u003E\u003Cstrong\u003EKey Features of Flipt\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cul\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003EFeature flags allow you to enable or disable features without modifying the source code or requiring a redeploy. This can be done at runtime, giving you the flexibility to control feature visibility without code changes.\u003Cbr\u003E\u003Cbr\u003E\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003EgRPC and REST APIs: Flipt supports both gRPC and REST APIs, allowing for high-performance and low-latency communication. This flexibility makes it easy to integrate with applications written in various languages.\u003Cbr\u003E\u003Cbr\u003E\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003EClient-Side Evaluation: Flipt supports client-side evaluation, which reduces the number of requests to the server and improves performance. This is particularly useful for applications that require real-time feature flag evaluations.\u003Cbr\u003E\u003Cbr\u003E\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003EGitOps Integration: Flipt is designed to integrate seamlessly with GitOps workflows, enabling continuous configuration and deployment. This ensures that feature flags are managed in a version-controlled manner, enhancing reliability and traceability. This is a reason alone to prefer Flipt because we are huge fans and \u003Ca href=\"https:\u002F\u002Fplatformengineers.io\u002Fblog\u002Fcontinuous-delivery-using-git-ops-principles-with-flux-cd\u002F\" target=\"_blank\" rel=\"noopener\"\u003Elongtime users of GitOps\u003C\u002Fa\u003E.\u003Cbr\u003E\u003Cbr\u003E\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003ESecurity and Control: Flipt supports various authentication methods, including HTTPS, OIDC, JWT, OAuth, and API tokens. This ensures that your feature flags are secure and can be managed without exposing your systems to external risks.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cimg src=\"https:\u002F\u002Flh7-rt.googleusercontent.com\u002Fdocsz\u002FAD_4nXcv3hY4IpUie3XTXa8z4X7u1qxVyklbFx9is84VjLwHcNWe2COJMVoAXuVNG_SMz5lha3844FrXc8XzzwZ2jB8tx40VP3ywM9Sr6PC6uU7m8xUF_vlXQ0t98LC9JMdBNNo_j5HOhF3qqVZO6_tJKGKt4_Lo?key=bvaOmXXC4jN_fIEr3ulIUw\" width=\"900\" height=\"340\"\u003E&nbsp;&nbsp;\u003C\u002Fp\u003E\n\u003Ch2 dir=\"ltr\"\u003EWhat is gRPC?\u003C\u002Fh2\u003E\n\u003Cp dir=\"ltr\"\u003EgRPC is a high-performance RPC framework that allows for efficient communication between microservices. It uses Protocol Buffers (protobuf) as the interface definition language and message format. gRPC is designed to work well with large-scale systems and is used by companies like Google and Netflix.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EAs a platform engineer, you may want to use gRPC for internal communication between services because it provides a number of benefits, including:\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EgRPC's explicit versioning and non-breaking changes ensure compatibility between different service versions. By defining API contracts in .proto files, clients and servers can negotiate capabilities during the RPC connection handshake. This allows for seamless communication even with different versions or extensions. gRPC supports non-breaking changes like adding new services, methods, or fields, ensuring existing clients continue to function without updates. This backward compatibility reduces the need for client notifications and updates, making it easier to manage service evolution.\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003E\u003Cstrong\u003EImproved performance: \u003C\u002Fstrong\u003EgRPC can help improve the performance of your services by reducing the overhead of serialization and deserialization.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003E\u003Cstrong\u003ESimplified error handling: \u003C\u002Fstrong\u003EgRPC provides a robust error handling mechanism that makes it easier to handle errors between services.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003E\u003Cstrong\u003EImproved security: \u003C\u002Fstrong\u003EgRPC provides built-in support for authentication and encryption, which can help improve the security of your services.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp dir=\"ltr\"\u003EgRPC offers several advantages that make it well-suited for microservices architecture. Its high performance and low latency design are ideal for real-time applications. The framework supports load balancing and failover, ensuring efficient scalability. gRPC's multi-language support allows for seamless integration with various programming languages, while its efficient protobuf serialization reduces data transfer overhead, resulting in faster communication between services.\u003C\u002Fp\u003E\n\u003Ch2 dir=\"ltr\"\u003EImplementing Feature Flags with Flipt Using Golang and gRPC&nbsp;\u003C\u002Fh2\u003E\n\u003Cp\u003E(\u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fflipt-io\u002Fflipt\u002Ftree\u002Fmain\u002Fsdk\u002Fgo\"\u003Ehttps:\u002F\u002Fgithub.com\u002Fflipt-io\u002Fflipt\u002Ftree\u002Fmain\u002Fsdk\u002Fgo\u003C\u002Fa\u003E)\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\" style=\"font-size: 20px;\"\u003EStep 1: Setting Up Flipt\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003ETo start using Flipt, you need to set up the server. You can do this by following the installation documentation provided by Flipt (\u003Ca href=\"https:\u002F\u002Fdocs.flipt.io\u002Fintroduction\"\u003Ehttps:\u002F\u002Fdocs.flipt.io\u002Fintroduction\u003C\u002Fa\u003E). Once the server is up and running, you can manage your feature flags through the UI or via the REST or gRPC APIs.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\" style=\"font-size: 20px;\"\u003EStep 2: Creating a Golang Client\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003ETo integrate Flipt with your Golang application using gRPC, you need to generate the gRPC client. Flipt provides a pre-generated gRPC client for Go, which you can use directly.\u003C\u002Fp\u003E\n\u003Ch4 dir=\"ltr\"\u003EExample Golang Client Code\u003C\u002Fh4\u003E\n\u003Cdiv dir=\"ltr\" align=\"left\"\u003E\n\u003Cpre\u003E\u003Ccode\u003Epackage main\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log\"\n\n\tflipt \"go.flipt.io\u002Fflipt\u002Frpc\u002Fflipt\"\n\t\"go.flipt.io\u002Fflipt\u002Frpc\u002Fflipt\u002Fevaluation\"\n\tsdk \"go.flipt.io\u002Fflipt\u002Fsdk\u002Fgo\"\n\tfliptgrpc \"go.flipt.io\u002Fflipt\u002Fsdk\u002Fgo\u002Fgrpc\"\n\t\"google.golang.org\u002Fgrpc\"\n\t\"google.golang.org\u002Fgrpc\u002Fcredentials\u002Finsecure\"\n)\n\nfunc main() {\n\tconn, err := grpc.NewClient(\"localhost:9000\", grpc.WithTransportCredentials(insecure.NewCredentials()))\n\n\tif err != nil {\n\t\tlog.Fatal(\"error while connecting: \", err)\n\t}\n\tdefer conn.Close()\n\n\ttransport := fliptgrpc.NewTransport(conn)\n\n\tsdk := sdk.New(transport)\n\n\t\u002F\u002F For boolean flags\n\tfliptClient := sdk.Flipt()\n\tflag, err := fliptClient.GetFlag(context.Background(), &amp;flipt.GetFlagRequest{\n\t\t\u002F\u002F NamespaceKey: \"my_namespace\", \u002F\u002F optional - uses the default namespace\n\t\tKey: \"test\",\n\t})\n\tif err != nil {\n\t\tlog.Fatalf(\"error in fetching flag: %v\", err)\n\t}\n\n\tfmt.Println(\"Flag name: \", flag.Name)\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Ch3 dir=\"ltr\" style=\"font-size: 20px;\"\u003EStep 3: Integrating Feature Flags into Your Application\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003EOnce you have the gRPC client set up, you can integrate feature flags into your application. This involves calling the EvalFlag method to determine whether a feature should be enabled or disabled for a given user.\u003C\u002Fp\u003E\n\u003Ch4 dir=\"ltr\"\u003EExample Integration\u003C\u002Fh4\u003E\n\u003Cpre\u003E\u003Ccode\u003Epackage main\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log\"\n\n\tflipt \"go.flipt.io\u002Fflipt\u002Frpc\u002Fflipt\"\n\t\"go.flipt.io\u002Fflipt\u002Frpc\u002Fflipt\u002Fevaluation\"\n\tsdk \"go.flipt.io\u002Fflipt\u002Fsdk\u002Fgo\"\n\tfliptgrpc \"go.flipt.io\u002Fflipt\u002Fsdk\u002Fgo\u002Fgrpc\"\n\t\"google.golang.org\u002Fgrpc\"\n\t\"google.golang.org\u002Fgrpc\u002Fcredentials\u002Finsecure\"\n)\n\nfunc main() {\n\tconn, err := grpc.NewClient(\"localhost:9000\", grpc.WithTransportCredentials(insecure.NewCredentials()))\n\n\tif err != nil {\n\t\tlog.Fatal(\"error while connecting: \", err)\n\t}\n\tdefer conn.Close()\n\n\ttransport := fliptgrpc.NewTransport(conn)\n\n\tsdk := sdk.New(transport)\n\n\t\u002F\u002F For boolean flags\n\tfliptClient := sdk.Flipt()\n\tflag, err := fliptClient.GetFlag(context.Background(), &amp;flipt.GetFlagRequest{\n\t\t\u002F\u002F NamespaceKey: \"my_namespace\", \u002F\u002F optional - uses the default namespace\n\t\tKey: \"test\",\n\t})\n\tif err != nil {\n\t\tlog.Fatalf(\"error in fetching flag: %v\", err)\n\t}\n\n\tfmt.Println(\"Flag name: \", flag.Name)\n\n\t\u002F\u002F For variant flags\n\tevaluationClient := sdk.Evaluation()\n\tvariantFlag, err := evaluationClient.Variant(context.Background(), &amp;evaluation.EvaluationRequest{\n\t\t\u002F\u002F NamespaceKey: \"my_namespace\",\n\t\tFlagKey:  \"Color\",\n\t\tEntityId: \"my_entity_id\",\n\t})\n\n\tif err != nil {\n\t\tlog.Fatalf(\"error in fetching variant flag: %v\", err)\n\t}\n\tfmt.Println(\"Variant name: \", variantFlag.FlagKey) \n}\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Ch3 dir=\"ltr\"\u003EAdding features in application (feature testing)\u003C\u002Fh3\u003E\n\u003Cp\u003EFeature testing is the process of testing new features in your application before they are released to production. This can be done using a variety of techniques, including:\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\u003Cstrong\u003ECanaries:\u003C\u002Fstrong\u003E deploying a new feature to a small percentage of users to test its performance and behavior.\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\u003Cstrong\u003EA\u002FB testing:\u003C\u002Fstrong\u003E deploying two versions of a feature to different groups of users to compare their performance and behavior.\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\u003Cstrong\u003EFeature flags: \u003C\u002Fstrong\u003Eusing feature flags to enable or disable features in your application, and testing their performance and behavior.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp\u003EBy using feature flags, you can test new features in your application without affecting the user experience. You can also use feature flags to roll back features if they are not performing well.\u003Cstrong\u003E&nbsp;\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\"\u003EWhat is OpenFeature?\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003EOpenFeature is a CNCF (Cloud Native Computing Foundation) Sandbox project aimed at developing an open standard for feature flag management. It provides a unified API and SDK, allowing for a developer-first, cloud-native implementation with extensibility for open-source and enterprise use cases.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EFlipt's OpenFeature provider offers a standardized and extensible way to manage feature flags. It integrates with Flipt's feature flags using the OpenFeature API, supporting both HTTP(s) and gRPC transports. The provider ensures compliance with the OpenFeature specification and includes hooks for custom actions. This integration benefits users by providing a standardized approach, fostering community-driven development, and simplifying integration with existing applications.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003ECommunity and Support\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EFlipt encourages community involvement and support for its OpenFeature integration. Users can reach out through Discord or email for feedback, to request specific language support, or to learn more about how Flipt can help their organization with feature flags.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EFlipt's support for OpenFeature enhances its feature management capabilities by providing a standardized, extensible, and developer-friendly way to manage feature flags, aligning with the principles of cloud-native and DevOps practices.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\"\u003E\u003Cstrong\u003EConclusion\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003EImplementing feature flags with Flipt using Golang and gRPC offers a robust and scalable solution for managing the rollout of new features, conducting A\u002FB testing, and ensuring continuous delivery. Flipt's use of gRPC ensures high-performance and low-latency communication, while its support for both gRPC and REST APIs enables easy integration with various applications. The client-side evaluation feature reduces server load, and GitOps integration enhances reliability and traceability. Flipt provides robust security measures and simplified error handling.&nbsp;\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EHowever, setting up Flipt and managing code complexity are initial challenges. Ensuring scalability and properly managing toggle configuration are also crucial considerations.\u003C\u002Fp\u003E\n\u003C\u002Fdiv\u003E",tags:a,time_to_read:w,user_created:{id:f,first_name:g,last_name:h,email:k,password:b,location:a,title:l,description:a,tags:a,avatar:i,language:a,tfa_secret:a,status:c,role:j,token:a,last_access:m,last_page:n,provider:d,external_identifier:a,auth_data:a,email_notifications:e,appearance:a,theme_dark:a,theme_light:a,theme_light_overrides:a,theme_dark_overrides:a},user_updated:{id:f,first_name:g,last_name:h,email:k,password:b,location:a,title:l,description:a,tags:a,avatar:i,language:a,tfa_secret:a,status:c,role:j,token:a,last_access:m,last_page:n,provider:d,external_identifier:a,auth_data:a,email_notifications:e,appearance:a,theme_dark:a,theme_light:a,theme_light_overrides:a,theme_dark_overrides:a},image:{id:"42f5d362-2eb5-44aa-8586-4e7bef5c211c",storage:p,filename_disk:"42f5d362-2eb5-44aa-8586-4e7bef5c211c.webp",filename_download:"p59dmijgeh8tdefpisjx.webp",title:"P59dmijgeh8tdefpisjx",type:W,folder:q,uploaded_by:f,created_on:aD,modified_by:a,modified_on:"2024-09-02T06:59:54.408Z",charset:a,filesize:"13518",width:aE,height:aE,duration:a,embed:a,description:a,location:a,tags:a,metadata:{},focal_point_x:a,focal_point_y:a,tus_id:a,tus_data:a,uploaded_on:aD},authors:[{id:37,pe_blog_id:V,directus_users_id:{first_name:ao,last_name:ap,avatar:aq}},{id:38,pe_blog_id:V,directus_users_id:{first_name:g,last_name:h,avatar:i}}]},{id:X,status:o,sort:a,date_created:"2024-07-23T05:30:42.787Z",date_updated:"2024-07-25T05:31:12.317Z",slug:"secrets-management-with-gitops-and-sops",title:ar,description:"\u003Cp\u003E\u003Cstrong id=\"docs-internal-guid-488fb0b3-7fff-d06c-41c6-b05b3f8640c7\"\u003ESecrets Management in CD Environments with GitOps and SOPS\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E",seo_title:ar,seo_description:ar,content:"\u003Cp dir=\"ltr\"\u003EModern software development practices heavily rely on continuous integration and continuous delivery (CI\u002FCD) pipelines for automated deployments. These pipelines automate various tasks, including building, testing, and deploying applications. However, a critical challenge in CD environments is managing secrets securely. Secrets, such as API keys, passwords, and database credentials, are essential for application functionality, but their exposure can lead to security vulnerabilities.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003ESOPS is a text file editor that automates the encryption and decryption of files. It supports various encryption methods, including GPG, AWS KMS, GCP KMS, AGE and HashiCorp Vault. SOPS integrates with VSCode and other editors, allowing users to edit encrypted files seamlessly.\u003C\u002Fp\u003E\n\u003Ch2 dir=\"ltr\"\u003EWhat is GitOps?\u003C\u002Fh2\u003E\n\u003Cp dir=\"ltr\"\u003EGitOps is a DevOps practice that uses Git as the single source of truth for infrastructure and application configurations. It enables developers to manage infrastructure and applications using familiar tools and workflows. GitOps workflows typically involve using a CI\u002FCD pipeline to automatically deploy changes to infrastructure and applications based on changes to Git repositories.\u003C\u002Fp\u003E\n\u003Ch2 dir=\"ltr\"\u003EApproaches for secret Management\u003C\u002Fh2\u003E\n\u003Cp dir=\"ltr\"\u003EWhen it comes to Continuous Deployment (CD), managing secrets securely is crucial. While GitOps and SOPS are popular approaches, there are other methods worth exploring. In this post, we'll discuss the importance of secrets management and alternative strategies using Vault and cloud providers. There are several approaches to secret management beyond HashiCorp Vault and cloud provider-specific solutions:\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003E\u003Cstrong\u003EExternal Secrets Operator (ESO): \u003C\u002Fstrong\u003EThis is an open-source Kubernetes operator that integrates with external secret stores like AWS Secrets Manager, Google Cloud Secret Manager, and Azure Key Vault. It provides a user-friendly abstraction for managing secrets across multiple environments.\u003Cbr\u003E\u003Cbr\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Cstrong\u003EHashicorp Vault: \u003C\u002Fstrong\u003EHashiCorp Vault is a comprehensive secret management solution that provides granular access controls, audit logs, and dynamic secret generation. It is widely used but can be complex to set up and maintain.\u003Cbr\u003E\u003Cbr\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Cstrong\u003EIn-house Tools: \u003C\u002Fstrong\u003ESome organizations develop their own in-house tools for managing secrets and configurations. These tools can be tailored to the specific needs of the organization and can integrate with various secret management solutions.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp dir=\"ltr\"\u003EWe chose to use SOPS over other secret management options because of its unique approach to encryption and version control. Unlike other solutions that encrypt both the secret values and keys, SOPS only encrypts the values, leaving the keys in plain text. This allows us to track changes to secrets in our Git repository, ensuring that all environment files have the same keys.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EThis approach also simplifies the management of secrets across different environments, as the keys remain consistent. SOPS integrates seamlessly with our existing Git workflow, making it easy to adopt and maintain. By using SOPS, we can ensure the security of our secrets while also maintaining transparency and control over changes to those secrets.\u003C\u002Fp\u003E\n\u003Ch2 dir=\"ltr\"\u003EWhat is SOPS?\u003C\u002Fh2\u003E\n\u003Cp dir=\"ltr\"\u003ESOPS is an open-source tool that enables developers to encrypt secrets in Git repositories. It uses AWS KMS, GCP KMS, or Azure Key Vault to encrypt secrets and stores the encrypted secrets in Git. SOPS also enables developers to decrypt secrets using the same key management service.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003ESecrets management is crucial for ensuring the security and integrity of sensitive data and systems within an organization. Here are some key reasons why secrets are important and what can be done for effective secret management:\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003E\u003Cstrong\u003EPreventing Data Breaches: \u003C\u002Fstrong\u003EProper secrets management helps avoid costly data breaches by ensuring that sensitive information, such as passwords, keys, and tokens, are securely stored and transmitted. This minimizes the risk of unauthorized access to critical systems and data.\u003Cbr\u003E\u003Cbr\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Cstrong\u003EProtecting Critical Assets:\u003C\u002Fstrong\u003E Secrets management safeguards critical assets, including user and application accounts, devices, and other network elements, from intrusions and unauthorized access.\u003Cbr\u003E\u003Cbr\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Cstrong\u003ECompliance with Regulations:\u003C\u002Fstrong\u003E Effective secrets management helps organizations meet the requirements of security regulations and standards, such as NIST, FIPS, and HIPAA, by ensuring secure storage, transmission, and auditing of secrets.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch3 dir=\"ltr\"\u003EWhy use SOPS with GitOps?\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003EUsing SOPS with GitOps provides several benefits: SOPS encrypts secrets before they are committed to Git, ensuring that they are not stored in plaintext.&nbsp;Git provides version control for secrets, enabling developers to track changes and roll back to previous versions if necessary. SOPS enables developers to manage secrets using familiar Git workflows, reducing the complexity of secrets management.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\"\u003EHow Does It Work?\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003ESOPS encrypts files automatically when they are written and decrypts them when they are read. It supports various encryption methods, making it highly customizable. SOPS can be used with text files or structured data such as YAML, JSON, INI, and ENV files.\u003C\u002Fp\u003E\n\u003Ch2 dir=\"ltr\"\u003EHow to use SOPS with GitOps\u003C\u002Fh2\u003E\n\u003Cp dir=\"ltr\"\u003ETo use SOPS with GitOps, we will use FluxCD's kustomize-controller. This controller decrypts secrets encrypted with SOPS using in-cluster secrets or cloud provider integrations, ensuring secure management within your GitOps workflow.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\"\u003E\u003Cstrong\u003ESOPS with AWS KMS\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EStep 1: Install SOPS\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E1. Download SOPS:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003Ecurl -LO https:\u002F\u002Fgithub.com\u002Fgetsops\u002Fsops\u002Freleases\u002Fdownload\u002Fv3.7.3\u002Fsops-v3.7.3.linux.amd64\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003E2. Copy the Downloaded file to local\u002Fbin:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003Emv sops-v3.7.3.linux.amd64 \u002Fusr\u002Flocal\u002Fbin\u002Fsops\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003E3. Make SOPS Executable:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003Echmod +x \u002Fusr\u002Flocal\u002Fbin\u002Fsops\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EStep 2: Create a KMS Key\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003E1. Sign in to AWS Management Console:&nbsp;Open the AWS Management Console and navigate to the KMS console.\u003C\u002Fp\u003E\n\u003Cp\u003E2. Create a KMS Key:\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli dir=\"ltr\" aria-level=\"2\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003EChoose \"Create key\".\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" aria-level=\"2\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003EEnter an alias for the key (e.g., \"sops-key\").\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" aria-level=\"2\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003EChoose \"AWS KMS generates the key material\".\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" aria-level=\"2\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003EAdd the AWS users or roles that will manage the key.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" aria-level=\"2\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003ESet the key usage permissions.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" aria-level=\"2\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003EReview and create the key.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EStep 3: Configure SOPS\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003ECreate a Configuration File: Create a file \u003Ccode\u003E~\u002F.sops.yaml\u003C\u002Fcode\u003E with the following content:\u003Cbr\u003Ecreation_rules:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003E- kms: arn:aws:kms:{region}:{account-id}:alias\u002F{alias}\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp dir=\"ltr\"\u003EReplace \u003Ccode\u003E{region}\u003C\u002Fcode\u003E, \u003Ccode\u003E{account-id}\u003C\u002Fcode\u003E, and \u003Ccode\u003E{alias}\u003C\u002Fcode\u003E with your AWS region, account ID, and the alias of your KMS key.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EStep 4: Encrypt Secrets\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003E1. Create a YAML File with Secrets: Create a file&nbsp;\u003Ccode\u003Esecrets.yaml\u003C\u002Fcode\u003E containing your secrets.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003E2. Encrypt the File:\u003C\u002Fp\u003E\n\u003Cdiv dir=\"ltr\" align=\"left\"\u003E\n\u003Cpre\u003E\u003Ccode\u003Esops --encrypt --kms arn:aws:kms:us-east-1:001301278159:key\u002Fb3f4dd5b-a217-46b5-aef2-152fa66be8f4 --encryption-context Role secrets.yaml\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EStep 5: Configure FluxCD\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E1. Create a Kubernetes Secret: Create a Kubernetes secret with the PGP keys on each cluster.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003E2. Add GitRepository and Kustomization Manifests: Add the GitRepository and Kustomization manifests to the fleet repository.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003E3. Configure Decryption: Configure the Kustomization manifest to use SOPS for decryption:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003EapiVersion: kustomize.toolkit.fluxcd.io\u002Fv1\nkind: Kustomization\nmetadata:\nname: my-secrets\nnamespace: flux-system\nspec:\ninterval: 10m0s\nsourceRef:\n  kind: GitRepository\n  name: my-secrets\npath: .\u002F\nprune: true\ndecryption:\n  provider: sops\n  secretRef:\n    name: sops-gpg\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003E\u003Cstrong\u003E&nbsp;Step 6: Deploy and Sync\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003E1. Commit and Push Changes: Commit the encrypted secrets and manifests to the Git repository.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003E2. Sync with FluxCD: FluxCD will pull the changes from the Git repository, decrypt the secrets using SOPS and AWS KMS, and apply them to the cluster.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EBy following these steps, you can securely manage and deploy Kubernetes secrets using SOPS and AWS KMS with FluxCD.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\"\u003E\u003Cstrong\u003ESOPS with AGE encryption\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EStep 1: Generate an AGE Key\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EGenerate an AGE key: Use the \u003Ccode\u003Eage-keygen\u003C\u002Fcode\u003E command to create a key pair. This command generates a public and private key.\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003Eage-keygen -o age.agekey\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EStep 2: Configure SOPS\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003ECreate .sops.yaml file: This file configures SOPS to use the AGE key for encryption and decryption.\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003Ecreation_rules:\n- encrypted_regex: '^(data|stringData)$'\n  age: &lt; public_key &gt;\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EStep 3: Encrypt Secrets\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EEncrypt a Kubernetes secret: Use SOPS with the AGE public key to encrypt a secret.\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003Esops --age=&lt; public_key &gt; --encrypt --encrypted-regex '^(data|stringData)$' --in-place basic-auth.yaml\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EStep 4: Create a Secret in Kubernetes\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003ECreate a secret in Kubernetes: Use the AGE private key to create a secret in the \u003Ccode\u003Eflux-system\u003C\u002Fcode\u003E namespace.\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003Ecat age.agekey | kubectl create secret generic sops-age --namespace=flux-system --from-file=age.agekey=\u002Fdev\u002Fstdin\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EStep 5: Configure Flux\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EUpdate the Flux Kustomization: Add the decryption configuration to the Flux Kustomization.\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003EapiVersion: kustomize.toolkit.fluxcd.io\u002Fv1beta2\nkind: Kustomization\nmetadata:\n name: flux-system\n namespace: flux-system\nspec:\n interval: 10m0s\n path: .\u002Fcluster\u002Fbase\n prune: true\n sourceRef:\n   kind: GitRepository\n   name: flux-system\n validation: client\n decryption:\n   provider: sops\n   secretRef:\n       name: sops-age\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EStep 6: Verify and Use\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EVerify and use the encrypted secrets: Flux will now use SOPS to decrypt the secrets in the Kubernetes cluster.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EThis setup ensures that your secrets are encrypted and decrypted securely using AGE and SOPS, and Flux can manage these secrets in your GitOps workflow.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\"\u003E\u003Cstrong\u003ESOPS with Text Files\u003C\u002Fstrong\u003E&nbsp;\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EInstall SOPS and Age: \u003C\u002Fstrong\u003EInstall SOPS and Age on the machine running Flux. You can get the binaries from their GitHub release pages.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003ECreate a PGP Key:\u003C\u002Fstrong\u003E Generate a PGP key using OpenGPG. Make sure not to specify a passphrase if you plan to use this key with Flux.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EConfigure SOPS:\u003C\u002Fstrong\u003E Create a \u003Ccode\u003E.sops.yaml\u003C\u002Fcode\u003E file in your Git repository to specify the encryption settings. This file defines the rules for encrypting and decrypting files.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EEncrypt Files: \u003C\u002Fstrong\u003EUse the \u003Ccode\u003Esops\u003C\u002Fcode\u003E command to encrypt your text files. For example, you can encrypt a file \u003Ccode\u003Esecret.yaml\u003C\u002Fcode\u003E using \u003Ccode\u003Esops -e -i secret.yaml.\u003C\u002Fcode\u003E\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EStore Encrypted Files:\u003C\u002Fstrong\u003E Store the encrypted files in your Git repository.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EConfigure Flux: \u003C\u002Fstrong\u003EUpdate your Flux Kustomization to use SOPS for decryption. This involves specifying the \u003Ccode\u003Edecryption\u003C\u002Fcode\u003E provider as \u003Ccode\u003Esops\u003C\u002Fcode\u003E and referencing the secret containing the decryption key.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EFlux will automatically decrypt the secrets when reconciling the cluster state. This ensures that the encrypted secrets are decrypted and applied to the cluster only when needed.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EBy following these steps, you can securely manage and deploy encrypted text files using SOPS and FluxCD.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\"\u003E\u003Cstrong\u003ESOPS with Structured Data\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003ETo use SOPS with structured data using Fluxcd, you can follow these steps:\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003E1. Configure SOPS:\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli dir=\"ltr\" role=\"presentation\"\u003EInstall SOPS and age using their documentation.\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" role=\"presentation\"\u003EGenerate a key using \u003Ccode\u003Eage-keygen\u003C\u002Fcode\u003E and create a \u003Ccode\u003E.sops.yaml\u003C\u002Fcode\u003E file with the public key.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003E2. Encrypt Secrets:\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EEncrypt your Kubernetes secrets using SOPS. For example, you can encrypt a secret file like this:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003EapiVersion: v1\nkind: Secret\nmetadata:\n name: mysqlcreds\ntype: Opaque\ndata: null\nstringData:\n DB_USER: bugs\n DB_PASSWORD: bunny\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EEncrypt this file using SOPS:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003Esops -e mysqlcreds.yaml &gt; mysqlcreds-encrypted.yaml\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003E3. Integrate with Flux:\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EConfigure Flux to use SOPS for decryption. In your \u003Ccode\u003Egotk-sync.yaml\u003C\u002Fcode\u003E file, add the \u003Ccode\u003Edecryption\u003C\u002Fcode\u003E property:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003EapiVersion: kustomize.toolkit.fluxcd.io\u002Fv1beta2\nkind: Kustomization\nmetadata:\n name: flux-system\nspec:\n interval: 10m0s\n path: .\u002Fclusters\u002Fmy-cluster\n prune: true\n sourceRef:\n   kind: GitRepository\n   name: flux-system\n decryption:\n   provider: sops\n   secretRef:\n     name: my-private-key\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp dir=\"ltr\"\u003EEnsure the secret \u003Ccode\u003Emy-private-key\u003C\u002Fcode\u003E is created correctly and contains the private key.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003E4. Apply Encrypted Secrets:\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli dir=\"ltr\" role=\"presentation\"\u003EPush the encrypted secret file to your Git repository.\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" role=\"presentation\"\u003EFlux will pick up the change and reconcile the cluster. The encrypted values should be decrypted and applied correctly.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp dir=\"ltr\"\u003EIf decryption does not work, check the \u003Ccode\u003EFlux logs\u003C\u002Fcode\u003E using flux logs to identify any issues.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EBy following these steps, you can effectively use SOPS with structured data using Fluxcd for secure and automated secret management in your Kubernetes cluster.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\"\u003E\u003Cstrong\u003ESOPS with HashiCorp Vault\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003EHashiCorp Vault is a secrets manager that provides secure storage and retrieval of secrets. SOPS can be used with HashiCorp Vault for encryption and decryption.\u003C\u002Fp\u003E\n\u003Cstrong\u003EA Short Introduction to Vault\u003C\u002Fstrong\u003E\n\u003Cp dir=\"ltr\"\u003EHashiCorp Vault is a secrets manager that provides secure storage and retrieval of secrets. It can be used for encryption, decryption, and signing of data. Vault can be used with various backends, including AWS KMS, GCP KMS, and local file systems.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003ETo manage Kubernetes secrets using SOPS with HashiCorp Vault and Flux, follow these steps:\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003E1. Configure HashiCorp Vault:\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli dir=\"ltr\" role=\"presentation\"\u003ESet up HashiCorp Vault as your identity-based secrets and encryption management system.\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" role=\"presentation\"\u003EEnable the transit backend in Vault to use it for encryption.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003E2. Encrypt Secrets with SOPS: Export the&nbsp;\u003Ccode\u003EVAULT_ADDR\u003C\u002Fcode\u003E and \u003Ccode\u003EVAULT_TOKEN\u003C\u002Fcode\u003E environment variables to your shell.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EUse \u003Ccode\u003Esops\u003C\u002Fcode\u003E to encrypt a Kubernetes Secret, specifying the Vault address and token:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003Esops --hc-vault-transit $VAULT_ADDR\u002Fv1\u002Fsops\u002Fkeys\u002Fmy-encryption-key --encrypt \\\n--encrypted-regex '^(data|stringData)$' --in-place basic-auth.yaml\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003E3. Create a Secret for the Vault Token: Create a secret for the Vault token using&nbsp;\u003Ccode\u003Ekubectl\u003C\u002Fcode\u003E:\u003C\u002Fp\u003E\n\u003Cdiv dir=\"ltr\" align=\"left\"\u003E\n\u003Cpre\u003E\u003Ccode\u003Eecho $VAULT_TOKEN | kubectl create secret generic sops-hcvault \\\n--namespace=flux-system --from-file=sops.vault-token=\u002Fdev\u002Fstdin\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003E4. Set Up Flux Kustomization: Set the decryption secret in the Flux Kustomization to&nbsp;\u003Ccode\u003Esops-hcvault\u003C\u002Fcode\u003E.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003E5. Authorize Kustomize Controller: Configure the kustomize-controller to use workload identity by adding patches to the flux-system kustomization.yaml file.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003E6. Integrate with Flux:\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli dir=\"ltr\" role=\"presentation\"\u003EFlux supports decrypting secrets stored in Flux sources using SOPS without additional controllers.\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" role=\"presentation\"\u003EFlux can synchronize secrets from Vault to a Kubernetes secret using the Vault CSI provider, allowing applications to reference the secret without refactoring.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp dir=\"ltr\"\u003EBy following these steps, you can securely manage Kubernetes secrets using SOPS with HashiCorp Vault and Flux.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\"\u003EConclusion\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003EWhen using cloud services, it is generally recommended to utilize the cloud provider's key management service for better access control and management of encryption keys. For instance, Google Cloud offers Cloud Key Management Service (Cloud KMS), which allows users to create and manage customer-managed encryption keys (CMEKs) for various Google Cloud services and applications. Similarly, Azure provides Azure Key Vault, which supports customer-managed keys (CMKs) and offers different options for storing and managing keys in the cloud.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EIf the cloud provider does not offer a key management service, alternatives include using Advanced Encryption Standard (AES) encryption or deploying an independent vault. This approach provides full control over encryption keys and can be used in various environments.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003ETools like SOPS can be used to manage secrets securely and scalably in a GitOps environment. SOPS supports multiple encryption methods, including AWS KMS, GCP KMS, and HashiCorp Vault, making it highly customizable.\u003C\u002Fp\u003E\n\u003C\u002Fdiv\u003E\n\u003C\u002Fdiv\u003E",tags:a,time_to_read:w,user_created:{id:f,first_name:g,last_name:h,email:k,password:b,location:a,title:l,description:a,tags:a,avatar:i,language:a,tfa_secret:a,status:c,role:j,token:a,last_access:m,last_page:n,provider:d,external_identifier:a,auth_data:a,email_notifications:e,appearance:a,theme_dark:a,theme_light:a,theme_light_overrides:a,theme_dark_overrides:a},user_updated:{id:f,first_name:g,last_name:h,email:k,password:b,location:a,title:l,description:a,tags:a,avatar:i,language:a,tfa_secret:a,status:c,role:j,token:a,last_access:m,last_page:n,provider:d,external_identifier:a,auth_data:a,email_notifications:e,appearance:a,theme_dark:a,theme_light:a,theme_light_overrides:a,theme_dark_overrides:a},image:{id:"2d44b1fa-25d3-48ef-9f31-f4f04000e6ce",storage:p,filename_disk:"2d44b1fa-25d3-48ef-9f31-f4f04000e6ce.png",filename_download:"Untitled design (4).png",title:"Untitled Design (4)",type:r,folder:q,uploaded_by:f,created_on:aF,modified_by:a,modified_on:"2024-07-24T07:18:39.559Z",charset:a,filesize:"478237",width:J,height:J,duration:a,embed:a,description:a,location:a,tags:a,metadata:{},focal_point_x:a,focal_point_y:a,tus_id:a,tus_data:a,uploaded_on:aF},authors:[{id:35,pe_blog_id:X,directus_users_id:{first_name:g,last_name:h,avatar:i}},{id:36,pe_blog_id:X,directus_users_id:{first_name:x,last_name:y,avatar:z}}]},{id:Y,status:o,sort:O,date_created:"2024-07-08T05:53:17.339Z",date_updated:"2024-07-08T06:39:05.153Z",slug:"infrastructure-testing-with-open-tofu-and-acceptance-tests",title:as,description:"\u003Cp\u003EInfrastructure Testing with OpenTofu and Acceptance Tests\u003C\u002Fp\u003E",seo_title:as,seo_description:as,content:"\u003Cp dir=\"ltr\"\u003EOpenTofu is an open-source tool that enables infrastructure testing by providing a framework for defining and executing tests against infrastructure as code (IaC) artifacts. It supports multiple IaC tools, including Terraform, CloudFormation, and Kubernetes. OpenTofu tests can be written in any language that supports the OpenTofu SDK, including Python, Go, and Ruby.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EAcceptance tests are a type of functional test that verify that the system meets the business requirements. They are typically written from the perspective of the end-user and focus on the overall behavior of the system. Acceptance tests can be used to validate that the infrastructure is correctly configured and that the application is functioning as expected.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003ELet's dive into how to use OpenTofu and acceptance tests to perform infrastructure testing.\u003C\u002Fp\u003E\n\u003Ch2 dir=\"ltr\"\u003ESetting up OpenTofu\u003C\u002Fh2\u003E\n\u003Cp dir=\"ltr\"\u003ETo get started with OpenTofu, you'll need to install the OpenTofu CLI and SDK. The CLI is used to run tests, while the SDK is used to write tests. You can install the CLI using pip:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003Esudo snap install opentofu --classic\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp dir=\"ltr\"\u003EWith the SDK installed, you can create a new OpenTofu project using the \u003Ccode\u003Etofu init\u003C\u002Fcode\u003E command:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003Etofu init myproject\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp dir=\"ltr\"\u003EThis will create a new directory called \u003Ccode\u003Emyproject\u003C\u002Fcode\u003E with a basic project structure.\u003C\u002Fp\u003E\n\u003Ch2 dir=\"ltr\"\u003EWriting OpenTofu tests\u003C\u002Fh2\u003E\n\u003Cp dir=\"ltr\"\u003EOpenTofu tests are written in a declarative language that describes the expected state of the infrastructure. Tests are organized into suites, which can be run together or individually.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EHere's an example of an OpenTofu test written in HCL:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003E# test.tftest.hcl\nrun \"test_ec2_instance_exists\" {\n  module {\n    source = \".\u002Fpath\u002Fto\u002Fmy\u002Fec2\u002Fmodule\"\n  }\n  \n  assert {\n    condition     = resource(\"aws_instance.my_instance\") != null\n    error_message = \"EC2 instance does not exist\"\n  }\n}\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003E\u003Ccode\u003Etofu test -f test.tftest.hcl\u003C\u002Fcode\u003E\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EIn this example, the test defines a Terraform module located at \u003Ccode\u003E.\u002Fpath\u002Fto\u002Fmy\u002Fec2\u002Fmodule\u003C\u002Fcode\u003E and asserts that the \u003Ccode\u003Eaws_instance.my_instance\u003C\u002Fcode\u003E resource exists. If the resource does not exist, the test will fail with the specified error message.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EHere's an example of a more complex test that asserts that an EC2 instance has the correct security group and that it is running:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003E# test-running.tftest.hcl\nrun \"test_ec2_instance_configured_correctly\" {\n  module {\n    source = \".\u002Fpath\u002Fto\u002Fmy\u002Fec2\u002Fmodule\"\n  }\n\n\n  # Assert that the EC2 instance exists\n  assert {\n    condition     = resource(\"aws_instance.my_instance\") != null\n    error_message = \"EC2 instance does not exist\"\n  }\n\n\n  # Assert that the EC2 instance has the correct security group\n  assert {\n    condition     = contains(resource(\"aws_instance.my_instance\").attribute(\"security_groups\"), \"my-security-group\")\n    error_message = \"EC2 instance does not have the correct security group\"\n  }\n\n\n  # Assert that the EC2 instance is running\n  assert {\n    condition     = resource(\"aws_instance.my_instance\").attribute(\"instance_state\") == \"running\"\n    error_message = \"EC2 instance is not running\"\n  }\n}\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003E\u003Ccode\u003Etofu test -f test-running.tftest.hcl\u003C\u002Fcode\u003E\u003Cstrong\u003E&nbsp;\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EThis test asserts that the \u003Ccode\u003Eaws_instance.my_instance\u003C\u002Fcode\u003E resource exists, has the correct security group, and is running. If any of these assertions fail, the test will fail with the specified error message.\u003C\u002Fp\u003E\n\u003Ch2 dir=\"ltr\"\u003ERunning OpenTofu tests\u003C\u002Fh2\u003E\n\u003Cp dir=\"ltr\"\u003EOnce you have written your OpenTofu tests, you can run them using the tofu run command:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003Etofu run myproject\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp dir=\"ltr\"\u003EThis will run all the tests in the \u003Ccode\u003Emyproject\u003C\u002Fcode\u003E directory. You can also run individual tests or test suites using the \u003Ccode\u003E--test\u003C\u002Fcode\u003E or \u003Ccode\u003E--suite\u003C\u002Fcode\u003E flags, respectively.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EOpenTofu provides detailed output for each test, including the test name, duration, and result. If a test fails, OpenTofu will provide information about the failure, including the assertion that failed and the expected and actual values.\u003C\u002Fp\u003E\n\u003Ch2 dir=\"ltr\"\u003EIntegrating OpenTofu with CI\u002FCD\u003C\u002Fh2\u003E\n\u003Cp dir=\"ltr\"\u003ETo ensure that your infrastructure is tested consistently, it's crucial to integrate OpenTofu with your CI\u002FCD pipeline. This integration can be achieved using the OpenTofu CLI and a CI\u002FCD tool such as Jenkins, CircleCI, or GitHub Actions.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EHere's an example of a GitHub Actions workflow that runs OpenTofu tests as part of a pull request:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003Ename: OpenTofu Tests\n\n\non: [pull_request]\n\n\njobs:\n test:\n   runs-on: ubuntu-latest\n\n\n   steps:\n   - name: Checkout code\n     uses: actions\u002Fcheckout@v2\n   - name: Installing OpenTofu CLI\n     uses: opentofu\u002Fsetup-opentofu@v1\n     with: \n       tofu_version: 1.6.0\n   - name: Run OpenTofu tests\n     run: |\n\ttofu test -f test.tftest.hcl\n\ttofu test -f test-running.tftest.hcl\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp dir=\"ltr\"\u003EThis workflow runs the OpenTofu tests whenever a pull request is opened or updated. If any of the tests fail, the workflow will fail, and the pull request will not be merged.\u003C\u002Fp\u003E\n\u003Ch2 dir=\"ltr\"\u003EUsing acceptance tests to validate infrastructure\u003C\u002Fh2\u003E\n\u003Cp dir=\"ltr\"\u003EWhile OpenTofu tests are useful for validating the state of the infrastructure, they do not necessarily validate that the infrastructure meets the business requirements. Acceptance tests can be used to validate that the infrastructure is correctly configured and that the application is functioning as expected.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EAcceptance tests can be written using a tool such as Behave or Cucumber. These tools provide a way to write tests in a natural language that is easy for non-technical stakeholders to understand.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EHere's an example of an acceptance test written using Behave:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003EFeature: EC2 instance\n\n\n Scenario: EC2 instance is running\n   Given an EC2 instance with name \"my-instance\"\n   When I check the instance status\n   Then the instance should be  \"my-security-group\"\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp dir=\"ltr\"\u003EThis acceptance test defines two scenarios: one that checks that the EC2 instance is running, and another that checks that the EC2 instance has the correct security group.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003ETo run these acceptance tests, you can use a tool such as Behave or Cucumber to execute the tests and validate the results. These tools provide a way to execute the tests and generate reports that can be shared with stakeholders.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\"\u003EIntegrating acceptance tests with OpenTofu\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003EWhile OpenTofu and acceptance tests serve different purposes, they can be integrated to provide a more comprehensive testing strategy.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EOne way to integrate OpenTofu and acceptance tests is to use OpenTofu to provision the infrastructure and then use acceptance tests to validate that the infrastructure is correctly configured and that the application is functioning as expected.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EHere's an example of how this might work:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E1. Write OpenTofu tests to validate the state of the infrastructure.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E2. Use OpenTofu to provision the infrastructure.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E3. Use acceptance tests to validate that the infrastructure is correctly configured and that the application is functioning as expected.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E4. If any of the acceptance tests fail, update the OpenTofu tests to reflect the new requirements and repeat the process.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EBy integrating OpenTofu and acceptance tests in this way, you can ensure that your infrastructure is tested comprehensively and that it meets the business requirements.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\"\u003E\u003Cstrong\u003EConclusion\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003EInfrastructure testing is a critical aspect of modern software development. OpenTofu provides a powerful framework for testing infrastructure as code artifacts, while acceptance tests can be used to validate that the infrastructure meets the business requirements. By integrating OpenTofu and acceptance tests, you can ensure that your infrastructure is tested comprehensively and that it is reliable, scalable, and secure.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EAcceptance Tests with OpenTofu:\u003C\u002Fstrong\u003E Acceptance tests are a crucial aspect of infrastructure testing, as they ensure that the deployed infrastructure meets the required specifications and functions as expected. OpenTofu's testing framework is designed to support acceptance tests, allowing users to define assertions that check the state of the infrastructure after the \u003Ccode\u003Eapply\u003C\u002Fcode\u003E or \u003Ccode\u003Eplan\u003C\u002Fcode\u003E operation. These assertions can be used to verify the correctness of the infrastructure, such as checking the ARN value of an AWS S3 bucket.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EComplexities and Challenges:\u003C\u002Fstrong\u003E While OpenTofu's testing framework offers a robust approach to infrastructure testing, there are complexities and challenges to consider. One of the main challenges is the need to balance the scope and granularity of tests, as higher-level tests can be time-consuming and costly. Setting up and tearing down resources for integration tests can be a lengthy process, making it essential to implement unit and contract testing to fail quickly on wrong configurations.\u003C\u002Fp\u003E",tags:a,time_to_read:w,user_created:{id:f,first_name:g,last_name:h,email:k,password:b,location:a,title:l,description:a,tags:a,avatar:i,language:a,tfa_secret:a,status:c,role:j,token:a,last_access:m,last_page:n,provider:d,external_identifier:a,auth_data:a,email_notifications:e,appearance:a,theme_dark:a,theme_light:a,theme_light_overrides:a,theme_dark_overrides:a},user_updated:{id:f,first_name:g,last_name:h,email:k,password:b,location:a,title:l,description:a,tags:a,avatar:i,language:a,tfa_secret:a,status:c,role:j,token:a,last_access:m,last_page:n,provider:d,external_identifier:a,auth_data:a,email_notifications:e,appearance:a,theme_dark:a,theme_light:a,theme_light_overrides:a,theme_dark_overrides:a},image:{id:"b9c66e4c-b98e-4cd2-bb7e-05a10829e476",storage:p,filename_disk:"b9c66e4c-b98e-4cd2-bb7e-05a10829e476.png",filename_download:"64e46b9733b82fb8cc6cebf1_Layer_1.png",title:"64e46b9733b82fb8cc6cebf1 Layer 1",type:r,folder:q,uploaded_by:f,created_on:aG,modified_by:a,modified_on:"2024-07-08T06:21:37.200Z",charset:a,filesize:"77212",width:899,height:935,duration:a,embed:a,description:a,location:a,tags:a,metadata:{},focal_point_x:a,focal_point_y:a,tus_id:a,tus_data:a,uploaded_on:aG},authors:[{id:33,pe_blog_id:Y,directus_users_id:{first_name:g,last_name:h,avatar:i}},{id:34,pe_blog_id:Y,directus_users_id:{first_name:x,last_name:y,avatar:z}}]},{id:Z,status:o,sort:P,date_created:"2024-06-25T07:59:40.788Z",date_updated:"2024-06-25T12:59:49.330Z",slug:"seeding-a-mongo-db-database-using-docker-compose",title:at,description:"\u003Cp\u003ESeeding a MongoDB Database using Docker Compose\u003C\u002Fp\u003E",seo_title:at,seo_description:at,content:"\u003Cp dir=\"ltr\"\u003EWhen working on a development project, it is often necessary to have a pre-populated database for testing and development purposes. This process is known as seeding the database.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003ESeeding, in the context of databases, refers to the process of populating a database with initial data sets. This data serves as a foundation for testing functionalities, demonstrating application behavior, and providing a starting point for development. Seeding is particularly useful during:\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003EApplication Development: Seeding allows developers to test application logic against a populated database, ensuring functionalities interact with data as intended.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003EIntegration Testing: During integration testing, seed data can be used to simulate real-world scenarios and verify proper data flow between different components of the system.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003EDemonstration Environments: Seeding a database with sample data can be beneficial for showcasing application features and functionalities in a demonstration environment.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp\u003E\u003Cstrong\u003EPrerequisites : Before we begin, ensure you have the following prerequisites:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Col\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003EDocker and Docker Compose are installed on your machine.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003EA MongoDB image pulled from Docker Hub.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003EA JSON file containing the data you want to seed into the database.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003C\u002Fol\u003E\n\u003Ch3 dir=\"ltr\"\u003E\u003Cstrong\u003ECreating a Docker Compose File\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003EThe first step is to create a Docker Compose file that defines the services required for our project. In this case, we need a MongoDB service. Create a file named \u003Ccode\u003E docker-compose.yml\u003C\u002Fcode\u003E and add the following code:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003Eversion: '3'\nservices:\n  mongodb:\n    image: mongo\n    container_name: mongodb\n    ports:\n      - \"27017:27017\"\n    volumes:\n      - .\u002Fdata:\u002Fdata\u002Fdb\n      - .\u002Fseed.js:\u002Fdocker-entrypoint-initdb.d\u002Fseed.js\n    environment:\n      MONGO_INITDB_ROOT_USERNAME: root\n      MONGO_INITDB_ROOT_PASSWORD: example\n    command: [\"mongod\", \"--auth\"]\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp dir=\"ltr\"\u003EIn the above code, we define a service named \u003Ccode\u003Emongodb\u003C\u002Fcode\u003E that uses the official MongoDB image from Docker Hub. We also specify a container name, expose port 27017, and mount two volumes. The first volume mounts the \u003Ccode\u003E.\u002Fdata\u003C\u002Fcode\u003E directory on the host machine to the \u003Ccode\u003E\u002Fdata\u002Fdb\u003C\u002Fcode\u003E directory inside the container. This allows us to persist data between container restarts. The second volume mounts the \u003Ccode\u003Eseed.js\u003C\u002Fcode\u003E file to the \u003Ccode\u003E\u002Fdocker-entrypoint-initdb.d\u003C\u002Fcode\u003E directory inside the container. This directory is used by the MongoDB image to execute scripts during container startup.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EWe also specify an empty command to disable MongoDB's built-in authentication. This is not recommended for production environments, but it simplifies our setup for development purposes.\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003E\u002F\u002F seed.js\ndb = db.getSiblingDB('mydb');\n\n\ndb.mycollection.insert({\n   \"name\": \"John Doe\",\n   \"age\": 30,\n   \"city\": \"New York\",\n   \"has_children\": false,\n   \"children\": null\n});\n\n\nprint(\"Data has been written to the collection\");\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003E\u003Cstrong\u003EExplanation:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Col\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003EImport the \u003Ccode\u003Ejson\u003C\u002Fcode\u003E module: This module provides functionalities to work with JSON data.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003EPrepare the data: Ensure your data is in a dictionary or list format.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003ESpecify the file path: Determine where you want to save the JSON file.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003EWrite the data to a JSON file:\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cul\u003E\n\u003Cli dir=\"ltr\"\u003E\n\u003Cp\u003EOpen the file in write mode.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\"\u003E\n\u003Cp\u003EUse \u003Ccode\u003Ejson.dump()\u003C\u002Fcode\u003E to write the data to the file.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli\u003EThe \u003Ccode\u003Eindent\u003C\u002Fcode\u003E parameter is optional but makes the JSON file more readable.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003C\u002Fol\u003E\n\u003Ch3 dir=\"ltr\"\u003E\u003Cstrong\u003EStarting the MongoDB Service\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003ENow that we have our Docker Compose file, seed script, and JSON data file, we can start our MongoDB service. Run the following command in the same directory as your docker-compose.yml file:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003Edocker-compose up -d\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp dir=\"ltr\"\u003EThis will start the MongoDB service in detached mode, which means it will run in the background.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\"\u003E\u003Cstrong\u003EChecking the Database\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003ETo verify that our seed script has populated the database with data, we can use the MongoDB shell to connect to our instance and query the \u003Ccode\u003Emycollection\u003C\u002Fcode\u003E collection. Run the following command:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003Edocker exec -it mongodb mongosh -u root -p example\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp dir=\"ltr\"\u003EThis will open the MongoDB shell and connect to our \u003Ccode\u003Emydb\u003C\u002Fcode\u003E database. Once connected, run the following command to query the \u003Ccode\u003Emycollection\u003C\u002Fcode\u003E collection:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003Euse mydb;\ndb.mycollection.find().pretty();\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EThis should output the following:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003E[\n  {\n    _id: ObjectId('667936bf4690147f33a26a13'),\n    name: 'John Doe',\n    age: 30,\n    city: 'New York',\n    has_children: false,\n    children: null\n  }\n]\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp dir=\"ltr\"\u003EThis confirms that our seed script has successfully populated the database with data.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\"\u003E\u003Cstrong\u003EHandling Errors\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003EIt is important to handle errors that may occur during the seeding process. In the above code, we have added error handling to ensure that the script exits with a non-zero exit code if an error occurs. This can be useful for integrating the seeding process into a larger build or deployment pipeline.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EFor example, we can use the \u003Ccode\u003E&amp;&amp;\u003C\u002Fcode\u003E operator to ensure that the MongoDB service is only started if the seed script completes successfully. Add the following code to the end of your \u003Ccode\u003Edocker-compose.yml\u003C\u002Fcode\u003E file:\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EThis will run the \u003Ccode\u003Enpm install\u003C\u002Fcode\u003E command to install any dependencies required by the seed script, followed by the \u003Ccode\u003Enpm run seed\u003C\u002Fcode\u003E command to execute the seed script. Finally, the \u003Ccode\u003Eexec mongod\u003C\u002Fcode\u003E command will start the MongoDB service.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003ESeeding a MongoDB database using Docker Compose offers several significant advantages for development and testing workflows. Here are some key benefits:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EAutomated Initialization\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EDocker Compose allows you to automate the database initialization process by defining seed data and executing it alongside other containerized services. This ensures that the database is populated with the necessary data every time a new environment is spun up, which is particularly useful when setting up new development environments or testing scenarios.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EConsistent Environments\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EBy using Docker Compose, you can ensure consistent database states across different environments (development, testing, production) by seeding data as part of your container setup. This consistency is crucial for predictable behavior during application development and testing phases.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EHighly Repeatable and Quick Environment Setup\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EDocker Compose simplifies the process of setting up a MongoDB container and seeding it with initial data. This makes it easy to quickly spin up new development environments with the exact same configuration, reducing the time spent on setup and ensuring that all team members work with the same data.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EReduced Development Setup Time\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EBy automating the seeding process, Docker Compose significantly reduces the time spent on setting up a development environment. This allows developers to focus on writing code rather than manually populating the database every time they start a new project or environment.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EDefault Data in Repository\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EWhen using Docker Compose, the seed data is stored in the repository, ensuring that new environments automatically receive the default data. This eliminates the need for manual data population and ensures that all environments start with the same baseline data.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\"\u003EConclusion\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003EWe have explored how to seed a MongoDB database using Docker Compose. We have created a Docker Compose file that defines a MongoDB service, a seed script that populates the database with data, and a JSON data file containing the data we want to seed.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EBy following these steps, you can easily pre-populate your MongoDB database for testing and development purposes. It is important to note that the above approach is suitable for development and testing environments only. In production environments, it is recommended to use a more secure and robust method for seeding data, such as using a dedicated seeding service or using MongoDB's built-in tools for data import and export.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EIt is important to ensure that the data being seeded is properly sanitized and validated to prevent any potential security vulnerabilities or data inconsistencies. This can be achieved by using a data validation library or by implementing custom validation logic in the seed script.\u003Cstrong id=\"docs-internal-guid-f442dc52-7fff-0b43-b394-20fb952978bb\"\u003E\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E",tags:a,time_to_read:w,user_created:{id:f,first_name:g,last_name:h,email:k,password:b,location:a,title:l,description:a,tags:a,avatar:i,language:a,tfa_secret:a,status:c,role:j,token:a,last_access:m,last_page:n,provider:d,external_identifier:a,auth_data:a,email_notifications:e,appearance:a,theme_dark:a,theme_light:a,theme_light_overrides:a,theme_dark_overrides:a},user_updated:{id:f,first_name:g,last_name:h,email:k,password:b,location:a,title:l,description:a,tags:a,avatar:i,language:a,tfa_secret:a,status:c,role:j,token:a,last_access:m,last_page:n,provider:d,external_identifier:a,auth_data:a,email_notifications:e,appearance:a,theme_dark:a,theme_light:a,theme_light_overrides:a,theme_dark_overrides:a},image:{id:"da202f75-7671-4a1d-afc0-2f9852f8c6ee",storage:p,filename_disk:"da202f75-7671-4a1d-afc0-2f9852f8c6ee.png",filename_download:"Untitled_design__3_-removebg-preview.png",title:"Untitled Design  3  Removebg Preview",type:r,folder:q,uploaded_by:f,created_on:aH,modified_by:a,modified_on:"2024-06-25T07:25:12.361Z",charset:a,filesize:"221194",width:I,height:I,duration:a,embed:a,description:a,location:a,tags:a,metadata:{},focal_point_x:a,focal_point_y:a,tus_id:a,tus_data:a,uploaded_on:aH},authors:[{id:30,pe_blog_id:Z,directus_users_id:{first_name:g,last_name:h,avatar:i}},{id:31,pe_blog_id:Z,directus_users_id:{first_name:x,last_name:y,avatar:z}}]},{id:O,status:o,sort:Q,date_created:"2024-06-17T11:51:36.151Z",date_updated:"2024-06-17T12:23:43.029Z",slug:"securing-kubernetes-beyond-rbac-and-pod-security-policies-psp",title:aI,description:"\u003Cp dir=\"ltr\"\u003ERole-Based Access Control (RBAC) and Pod Security Policies (PSP) are essential components of Kubernetes security, they are not enough to provide comprehensive protection against threats. Let&rsquo;s explore advanced techniques for securing Kubernetes beyond RBAC and PSP.\u003C\u002Fp\u003E",seo_title:aI,seo_description:"Role-Based Access Control (RBAC) and Pod Security Policies (PSP) are essential components of Kubernetes security, they are not enough to provide comprehensive protection against threats. Lets explore advanced techniques for securing Kubernetes beyond RBAC and PSP.\n",content:"\u003Cp dir=\"ltr\"\u003EAs more organizations adopt Kubernetes to manage their containerized workloads, securing Kubernetes clusters has become a top priority. While Role-Based Access Control (RBAC) and Pod Security Policies (PSP) are essential components of Kubernetes security, they are not enough to provide comprehensive protection against threats. Let&rsquo;s explore advanced techniques for securing Kubernetes beyond RBAC and PSP.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\"\u003E\u003Cstrong\u003ENetwork Policies\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003ENetwork policies are a powerful feature of Kubernetes that provides fine-grained control over network traffic between pods. By default, all pods in a Kubernetes cluster can communicate with each other, which can create security risks. Network policies allow you to define rules that restrict network traffic based on labels, IP addresses, and ports.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EHere's an example of a network policy that allows traffic only between pods with the label \"app: my-app\" and denies all other traffic:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003EapiVersion: networking.k8s.io\u002Fv1\nkind: NetworkPolicy\nmetadata:\n name: allow-my-app\nspec:\n podSelector:\n   matchLabels:\n     app: my-app\n policyTypes:\n - Ingress\n ingress:\n - from:\n   - podSelector:\n       matchLabels:\n         app: my-app\n   ports:\n   - protocol: TCP\n     port: 80\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp dir=\"ltr\"\u003EThis network policy uses the \"podSelector\" field to select pods with the label \"app: my-app\" and allows incoming traffic only from those pods on port 80.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\"\u003E\u003Cstrong\u003EAdmission Controllers\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003EAdmission controllers are plugins that intercept requests to the Kubernetes API server and enforce policies before allowing or denying the request. Admission controllers can be used to enforce a wide range of policies, including resource quotas, image validation, and pod security policies.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EHere's an example of an admission controller that enforces a policy that requires all containers to run as a non-root user:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003EapiVersion: admissionregistration.k8s.io\u002Fv1\nkind: MutatingWebhookConfiguration\nmetadata:\n name: run-as-non-root\nWebhooks:\n- admissionReviewVersions:\n  - v1\n  name: run-as-non-root.example.com\n  rules:\n  - apiGroups: [&ldquo;&rdquo;]\n    apiVersions: [&ldquo;v1&rdquo;]\n    operations: [&ldquo;CREATE&rdquo;]\n    resources: [&ldquo;pods&rdquo;]\n    failurePolicy: Fail\n    clientConfig:\n\tservice:\n\t  name: run-as-non-root-webhook\n\t  namespace: kube-system\n\t  Path: &ldquo;\u002Fmutate&rdquo;\n     caBundle: ${CA_BUNDLE}\n  sideEffects: None\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp dir=\"ltr\"\u003EThis admission controller uses a mutating webhook to modify the pod specification and set the \"runAsUser\" field to a non-root user. The \"failurePolicy\" field is set to \"Fail\" to ensure that the pod is not created if the webhook fails.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\"\u003E\u003Cstrong\u003ESecurity Contexts\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003ESecurity contexts allow you to define security settings for a pod or container. Security contexts can be used to set privileges, capabilities, and SELinux contexts for containers.&nbsp;\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EHere's an example of a security context that sets the \"runAsUser\" field to 1000 and the \"runAsGroup\" field to 3000:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003EapiVersion: v1\nkind: Pod\nmetadata:\n name: my-pod\nspec:\n  containers:\n  - image: ubuntu\n    command:\n      - \"sleep\"\n      - \"604800\"\n    imagePullPolicy: IfNotPresent\n    name: ubuntu\n    securityContext:\n      runAsUser: 1000\n      runAsGroup: 3000\n  restartPolicy: Always\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp dir=\"ltr\"\u003EThis security context sets the \"runAsUser\" field to 1000 and the \"runAsGroup\" field to 3000, ensuring that the container runs with the specified user and group IDs.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\"\u003E\u003Cstrong\u003EPod Security Standards\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003EPod security standards are a set of guidelines for configuring pod security policies. Pod security standards define a set of baseline requirements for pod security, including requirements for container images, network policies, and security contexts.&nbsp;\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EHere's an example of a pod security standard that requires all containers to run as a non-root user and use a read-only root file system:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003EapiVersion: policy\u002Fv1beta1\nkind: PodSecurityPolicy\nmetadata:\n name: restricted\nspec:\n privileged: false\n allowPrivilegeEscalation: false\n requiredDropCapabilities:\n - ALL\n allowedCapabilities: []\n volumes:\n - 'configMap'\n - 'emptyDir'\n - 'projected'\n - 'secret'\n - 'downwardAPI'\n runAsUser:\n   rule: 'MustRunAsNonRoot'\n fsGroup:\n   rule: 'MustRunAs'\n   ranges:\n   - min: 1\n     max: 65535\n seLinux:\n   rule: 'RunAsAny'\n supplementalGroups:\n   rule: 'MustRunAs'\n   ranges:\n   - min: 1\n     max: 65535\n readOnlyRootFilesystem: true\n hostNetwork: false\n hostPorts: false\n hostPID: false\n hostIpc: false\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp dir=\"ltr\"\u003EThis pod security standard requires all containers to run as a non-root user, use a read-only root filesystem, and disables privileged mode. It also restricts access to host networking, host ports, host PIDs, and host IPC.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\"\u003E\u003Cstrong\u003EImage Scanning\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003EContainer images often contain vulnerabilities that can be exploited by attackers. Image scanning tools can help identify known vulnerabilities in container images and alert you to potential security issues. Kubernetes provides built-in support for image scanning through its Container Runtime Interface (CRI). You can configure CRI to scan images automatically when they are pushed to a registry or manually trigger scans using command-line tools.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EHere's an example of how to manually trigger a scan of a container image using \u003Ca href=\"https:\u002F\u002Fengine.anchore.io\u002Fdocs\u002Finstall\u002Fanchore_cli\u002F\"\u003EAnchore Engine\u003C\u002Fa\u003E, an open-source image scanning tool:\u003C\u002Fp\u003E\n\u003Col\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003E\u003Cstrong\u003EInstall Anchore Engine in your environment.\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" style=\"font-weight: bold;\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003E\u003Cstrong\u003ECreate a new analysis: \u003C\u002Fstrong\u003E\u003Ccode\u003E$ anchore-cli image add --name &lt;registry&gt;\u002F&lt;image&gt;:&lt;tag&gt;\u003C\u002Fcode\u003E\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003C\u002Fol\u003E\n\u003Col\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003E\u003Cstrong\u003EWait for the analysis to complete.\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" style=\"font-weight: bold;\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003E\u003Cstrong\u003EView the results: \u003C\u002Fstrong\u003E\u003Ccode\u003E$ anchore-cli image analyze &lt;registry&gt;\u002F&lt;image&gt;:&lt;tag&gt;\u003C\u002Fcode\u003E\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003C\u002Fol\u003E\n\u003Cp\u003E\u003Cstrong\u003E&nbsp;\u003C\u002Fstrong\u003EThe output will include information about any vulnerabilities found in the image and recommendations for remediation.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\"\u003E\u003Cstrong\u003ESeccomp Profiles\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003ESeccomp profiles limit the syscalls that a container can make, reducing the risk of container breakouts and privilege escalations. Seccomp profiles can be applied to individual containers or entire pods. Kubernetes supports two types of Seccomp profiles: strict and runtime. Strict profiles deny all syscalls except for a whitelist of approved syscalls, while runtime profiles allow all syscalls by default and require explicit denials.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EHere's an example of how to apply a Seccomp profile to a container:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003ECreate a Seccomp profile:\n{\n   \"defaultAction\": \"SCMP_ACT_ERRNO\",\n   \"syscalls\": [\n       {\n           \"names\": [\"open\"],\n           \"action\": \"SCMP_ACT_ALLOW\",\n           \"args\": []\n       },\n       \u002F\u002F Add more syscall rules here\n   ]\n}\nSave the profile to a file, such as my-profile.json.\nApply the profile to a container:\napiVersion: v1\nkind: Pod\nmetadata:\n name: audit-pod\n labels:\n   app: audit-pod\nspec:\n securityContext:\n   seccompProfile:\n     type: Localhost\n     localhostProfile: \u002Fpath\u002Fto\u002Fmy-profile.json\n containers:\n - name: test-container\n   image: hashicorp\u002Fhttp-echo:1.0\n   args:\n   - \"-text=just made some syscalls!\"\n   securityContext:\n     allowPrivilegeEscalation: false\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp dir=\"ltr\"\u003EThis security context applies the Seccomp profile specified in my-profile.json to the container.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\"\u003E\u003Cstrong\u003ESecrets Management\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003EAnother critical aspect of securing Kubernetes clusters is secrets management. Secrets such as passwords, tokens, and certificates must be stored securely and accessed only by authorized components. Kubernetes provides several ways to manage secrets, including storing them as environment variables, config maps, or secret objects. Secret objects are recommended because they provide encryption at rest and in transit, access control, and fine-grained permissions.&nbsp;\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EHere's an example of creating a secret object containing a database password:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003Ekubectl create secret generic db-secret --from-literal=password=mysecretpassword\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp dir=\"ltr\"\u003EOnce created, secrets can be mounted as files within pods or accessed directly via environment variables. Access control can be implemented using RBAC rules, denying access to unauthorized users.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\"\u003E\u003Cstrong\u003ERuntime Security\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003EFinally, runtime security is essential for detecting and preventing attacks against your cluster. Runtime security tools such as container runtimes, Kubernetes admission controllers, and service meshes can monitor and enforce policies at runtime. Container runtimes such as Docker, containerd, and cri-o provide runtime isolation, resource limits, and network policies for each container. They can also detect and prevent attacks such as container escapes, privilege escalations, and malicious processes. Here's an example of setting up runtime security using Docker:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003Eyaml{\n   \"security\": {\n       \"apparmor\": {\n           \"profile\": \"unconfined\"\n       },\n       \"selinux\": {\n           \"enabled\": false\n       }\n   }\n}\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp dir=\"ltr\"\u003EWe disable SELinux and set AppArmor to unconfined mode, providing maximum flexibility for our containers. However, you may want to restrict these settings further based on your security requirements.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\"\u003E\u003Cstrong\u003EService meshes&nbsp;\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003EService meshes are another option for runtime security. Service meshes provide features such as traffic management, observability, and security for microservices architectures. Service meshes typically operate at the network layer, injecting proxies into each pod to manage traffic flow and enforce security policies.&nbsp;\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EHere's an example of setting up a service mesh using Istio:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003EapiVersion: networking.istio.io\u002Fv1alpha3\nkind: VirtualService\nmetadata:\n name: myservice\nspec:\n hosts:\n   - myservice.default.svc.cluster.local\n http:\n   - match:\n       - headers:\n           host:\n             exact: myservice.example.com\n     route:\n       - destination:\n           host: myservice.default.svc.cluster.local\n           port:\n             number: 80\n         weight: 100\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp dir=\"ltr\"\u003EWe define a virtual service called myservice that routes incoming requests to our backend service based on HTTP headers. We specify the hostname of our frontend service and the backend service to route requests to.Conclusion:Securing Kubernetes clusters requires advanced techniques beyond RBAC and Pod Security Policies. Network security, secrets management, and runtime security are critical aspects of securing Kubernetes clusters. Using network policies, secret objects, container runtimes, admission controllers, and service meshes, you can enforce fine-grained policies, detect and prevent attacks, and monitor your cluster's health.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\"\u003E\u003Cstrong\u003EConclusion\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003ESecuring Kubernetes clusters requires a multi-layered approach that goes beyond RBAC and Pod Security Policies. Network policies, admission controllers, security contexts, pod security standards, image scanning, and Seccomp profiles are advanced techniques that can help you secure your Kubernetes clusters and protect against threats. By implementing these techniques, you can ensure that your Kubernetes clusters are secure and compliant with industry best practices.&nbsp;\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong id=\"docs-internal-guid-99160d5c-7fff-ce92-3b99-ae25845f7306\"\u003ERemember: Security is an ongoing process. Regularly review and update your security practices to adapt to evolving threats and maintain a robust security posture for your Kubernetes deployments.\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E",tags:a,time_to_read:w,user_created:{id:f,first_name:g,last_name:h,email:k,password:b,location:a,title:l,description:a,tags:a,avatar:i,language:a,tfa_secret:a,status:c,role:j,token:a,last_access:m,last_page:n,provider:d,external_identifier:a,auth_data:a,email_notifications:e,appearance:a,theme_dark:a,theme_light:a,theme_light_overrides:a,theme_dark_overrides:a},user_updated:{id:f,first_name:g,last_name:h,email:k,password:b,location:a,title:l,description:a,tags:a,avatar:i,language:a,tfa_secret:a,status:c,role:j,token:a,last_access:m,last_page:n,provider:d,external_identifier:a,auth_data:a,email_notifications:e,appearance:a,theme_dark:a,theme_light:a,theme_light_overrides:a,theme_dark_overrides:a},image:{id:"97e4ca74-ff1d-4859-8948-bf0f7152f44a",storage:p,filename_disk:"97e4ca74-ff1d-4859-8948-bf0f7152f44a.png",filename_download:"virtual-machines-operating-system-data-storage-min.png",title:"Virtual Machines Operating System Data Storage Min",type:r,folder:q,uploaded_by:f,created_on:aJ,modified_by:a,modified_on:"2024-06-17T11:46:19.670Z",charset:a,filesize:"491660",width:au,height:1333,duration:a,embed:a,description:a,location:a,tags:a,metadata:{},focal_point_x:a,focal_point_y:a,tus_id:a,tus_data:a,uploaded_on:aJ},authors:[{id:U,pe_blog_id:O,directus_users_id:{first_name:g,last_name:h,avatar:i}},{id:29,pe_blog_id:O,directus_users_id:{first_name:x,last_name:y,avatar:z}}]},{id:P,status:o,sort:R,date_created:"2024-06-10T10:35:32.094Z",date_updated:"2024-06-13T11:16:53.550Z",slug:"multi-stage-build-for-ci-cd-pipeline-using-dockerfile",title:aK,description:"\u003Cp\u003E\u003Cstrong id=\"docs-internal-guid-78da0e51-7fff-f7e6-b4ae-e7a38613e20f\"\u003EDockerfile builds create monolithic images containing the entire application environment &ndash; application code, build tools, and runtime dependencies.\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E",seo_title:aK,seo_description:"Dockerfile builds create monolithic images containing the entire application environment  application code, build tools, and runtime dependencies.",content:"\u003Cp dir=\"ltr\"\u003EDockerfile builds create monolithic images containing the entire application environment &ndash; application code, build tools, and runtime dependencies. While simple to understand, this approach leads to bulky images with a significant drawback: inefficiency within CI\u002FCD pipelines.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EMulti-stage builds are necessary for Continuous Integration and Continuous Deployment (CI\u002FCD) pipelines because they allow for more efficient and secure deployment processes. By breaking down the build process into stages, each stage can perform specific tasks, such as building the application, running tests, and creating a production-ready image. This approach reduces the risk of errors and vulnerabilities in the final image, as each stage can be tested and validated before moving on to the next stage. Multi-stage builds can reduce the size of the final image by removing unnecessary files and dependencies, improving the overall performance and security of the application.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\"\u003ECore Concepts:\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003ESeparation of Concerns: Build vs. Runtime Stages\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EMulti-stage builds establish a clear distinction between the build environment and the runtime environment:\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003EBuild Stage: Focuses on building the application. It typically utilizes a base image containing the necessary build tools (compilers, linkers) and libraries for the specific programming language (e.g., golang:1.19-alpine for Go). The application code is copied into this stage, and build commands are executed to compile, link, and generate application binaries or artifacts. These artifacts are then staged for inclusion in the final image.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003ERuntime Stage: Focuses on running the application. It utilizes a minimal base image, often a lightweight image like alpine:3.16. The required application artifacts are carefully selected and copied from the build stage into the runtime image. Any additional runtime dependencies (e.g., system libraries) are installed in this stage to ensure the application functions properly within the container. This final image serves as the basis for deploying your application.\u003Cstrong\u003E\u003Cbr\u003E\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch3 dir=\"ltr\"\u003EUse Multi-Stage Builds\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003EMulti-stage builds involve employing multiple \u003Ccode\u003E`FROM`\u003C\u002Fcode\u003E statements in a Dockerfile, with each statement initiating a new stage of the build. These stages can utilize different bases, and artifacts can be selectively copied from one stage to another, excluding unwanted elements in the final image.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EHere's an example demonstrating the separation of build and runtime environments:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003E# syntax=docker\u002Fdockerfile:1\nFROM golang:1.21 as build\nWORKDIR \u002Fsrc\nCOPY &lt;\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003EIn this example, the first stage utilizes Golang to compile the source code, while the second stage employs the \u003Ccode\u003E`scratch`\u003C\u002Fcode\u003E image, which contains only the compiled binary. This approach ensures a small, secure production image without superfluous build tools.\u003C\u002Fp\u003E\n\u003Ch3\u003EName Your Build Stages\u003C\u002Fh3\u003E\n\u003Cp\u003EBy default, stages are referred to by their integer number, starting with zero for the initial \u003Ccode\u003E`FROM`\u003C\u002Fcode\u003E instruction. However, stages can be named explicitly using the \u003Ccode\u003E`AS`\u003C\u002Fcode\u003E keyword, improving readability and resilience against reordering of instructions:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003E\n# syntax=docker\u002Fdockerfile:1\nFROM golang:1.21 as build\nWORKDIR \u002Fsrc\nCOPY &lt;\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Ch3\u003EStop at a Specific Build Stage\u003C\u002Fh3\u003E\n\u003Cp\u003EWhen constructing an image, it is possible to stop at a particular stage using the \u003Ccode\u003E`--target`\u003C\u002Fcode\u003E flag during the build process. This capability is helpful for various scenarios, such as debugging a specific stage or managing distinct development and production environments:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003E$ docker build --target build -t hello .\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Ch3\u003EUse External Image as a Stage\u003C\u002Fh3\u003E\n\u003Cp\u003EStages in multi-stage builds are not confined to previous stages within the same Dockerfile; they can also reference external images. This feature allows for greater flexibility and modularity in the build process:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003ECOPY --from=nginx:latest \u002Fetc\u002Fnginx\u002Fnginx.conf \u002Fnginx.conf\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003EIn multi-stage builds, you aren't limited to copying from stages created earlier in your Dockerfile. You can use the \u003Ccode\u003E`COPY --from`\u003C\u002Fcode\u003E instruction to copy from a separate image:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003E# syntax=docker\u002Fdockerfile:1\n\n\nFROM alpine:latest AS builder\nRUN apk --no-cache add build-base\n\n\nFROM builder AS build1\nCOPY &lt;\n\nint main() {\n    std::cout &lt;&lt; \"Hello from source1!\" &lt;&lt; std::endl;\n    return 0;\n}\nEOF\nRUN g++ -o \u002Fbinary source1.cpp\n\n\nFROM builder AS build2\nCOPY &lt;\n\n\nint main() {\n    std::cout &lt;&lt; \"Hello from source2!\" &lt;&lt; std::endl;\n    return 0;\n}\nEOF\nRUN g++ -o \u002Fbinary source2.cpp\n\n\nFROM nginx:latest\nCOPY --from=build1 \u002Fbinary \u002Fusr\u002Fshare\u002Fnginx\u002Fhtml\u002Findex.html\nCOPY --from=build2 \u002Fbinary \u002Fusr\u002Fshare\u002Fnginx\u002Fhtml\u002Flogin.html\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Ch2\u003EAdvanced Multi-Stage Build Techniques\u003C\u002Fh2\u003E\n\u003Ch3\u003EBuilding Multiple Artifacts\u003C\u002Fh3\u003E\n\u003Cp\u003EMulti-stage builds can be used to build multiple artifacts in a single Dockerfile. This technique is particularly useful when dealing with microservices or applications composed of multiple components.\u003C\u002Fp\u003E\n\u003Cp\u003EConsider the following example, demonstrating a multi-stage build for two Golang applications:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003E# syntax=docker\u002Fdockerfile:1\n\nFROM golang:1.21 AS build1\nWORKDIR \u002Fsrc1\nCOPY &lt;\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003EHere, two separate stages are used to build two distinct Golang applications. The final stage copies both binaries from their respective build stages and executes them together.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\"\u003EBuilding for Multiple Architectures\u003C\u002Fh3\u003E\n\u003Cp\u003EMulti-stage builds can also be used to build for multiple architectures. This technique is particularly useful when targeting different platforms or environments.\u003C\u002Fp\u003E\n\u003Cp\u003EConsider the following example, demonstrating a multi-stage build for two different architectures:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003E# syntax=docker\u002Fdockerfile:1\n\n\nFROM golang:1.21 AS build-amd64\nWORKDIR \u002Fsrc\nCOPY &lt;\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003EHere, two separate stages are used to build the same application for two different architectures. The final stage copies both binaries from their respective build stages and executes them together.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\"\u003EBuilding image using BuildKit\u003C\u002Fh3\u003E\n\u003Cp\u003ETo build the image using BuildKit, you would run the following command:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003EDOCKER_BUILDKIT=1 docker build -t hello\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003EBy setting \u003Ccode\u003E`DOCKER_BUILDKIT=1`\u003C\u002Fcode\u003E, you instruct Docker to use BuildKit for the build process. Without BuildKit, the Docker engine would attempt to build all stages leading up to the specified target, potentially wasting resources.\u003C\u002Fp\u003E\n\u003Cp\u003EWith BuildKit, only the stages that the target stage depends on are processed. In this case, since the \u003Ccode\u003E`runtime`\u003C\u002Fcode\u003E stage depends on the `build` stage, only those stages will be built. This leads to faster build times and reduced resource usage.\u003C\u002Fp\u003E\n\u003Cp\u003EYou can specify a target build stage using the \u003Ccode\u003E`--target`\u003C\u002Fcode\u003E option:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003EDOCKER_BUILDKIT=1 docker build --target build -t hello\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003EThis will stop at the \u003Ccode\u003E`build`\u003C\u002Fcode\u003E stage and skip the \u003Ccode\u003E`runtime`\u003C\u002Fcode\u003E stage, which may be useful for debugging purposes or when creating separate build and runtime images.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\"\u003EOptimizing CI\u002FCD Workflows:\u003C\u002Fh3\u003E\n\u003Cp\u003EThe final image size is significantly smaller compared to a traditional single-stage build, as unnecessary build tools and libraries are excluded. This translates to faster image transfers between the build server and container registry during the deployment process. Caching of build stages combined with the smaller final image size contributes to faster overall build times within CI\u002FCD pipelines. Smaller images inherently reduce the attack surface, potentially minimizing security vulnerabilities within your containerized application. \u003Ca href=\"https:\u002F\u002Fplatformengineers.io\u002Fblog\u002Fbest-practices-for-writing-dockerfiles\u002F\" target=\"_blank\" rel=\"noopener\"\u003EExplore best practices for writing efficient and secure Dockerfiles.\u003C\u002Fa\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EConsiderations:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003EMulti-stage builds introduce additional complexity compared to single-stage builds. Understanding the separation of concerns and managing dependencies across stages becomes crucial.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003EEnsure the runtime base image has the necessary tools to handle tasks like copying files or setting environment variables.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003EUsing minimal base images in the runtime stage might require additional security hardening steps to mitigate potential risks.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp\u003E&nbsp;\u003C\u002Fp\u003E",tags:a,time_to_read:w,user_created:{id:f,first_name:g,last_name:h,email:k,password:b,location:a,title:l,description:a,tags:a,avatar:i,language:a,tfa_secret:a,status:c,role:j,token:a,last_access:m,last_page:n,provider:d,external_identifier:a,auth_data:a,email_notifications:e,appearance:a,theme_dark:a,theme_light:a,theme_light_overrides:a,theme_dark_overrides:a},user_updated:{id:f,first_name:g,last_name:h,email:k,password:b,location:a,title:l,description:a,tags:a,avatar:i,language:a,tfa_secret:a,status:c,role:j,token:a,last_access:m,last_page:n,provider:d,external_identifier:a,auth_data:a,email_notifications:e,appearance:a,theme_dark:a,theme_light:a,theme_light_overrides:a,theme_dark_overrides:a},image:{id:"84fa5dbb-6e76-4e31-8a69-127bd1d7fa58",storage:p,filename_disk:"84fa5dbb-6e76-4e31-8a69-127bd1d7fa58.png",filename_download:"hand-drawn-flat-design-sql-illustration.png",title:"Hand Drawn Flat Design SQL Illustration",type:r,folder:q,uploaded_by:f,created_on:aL,modified_by:a,modified_on:"2024-06-10T10:32:02.204Z",charset:a,filesize:"837133",width:au,height:au,duration:a,embed:a,description:a,location:a,tags:a,metadata:{},focal_point_x:a,focal_point_y:a,tus_id:a,tus_data:a,uploaded_on:aL},authors:[{id:X,pe_blog_id:P,directus_users_id:{first_name:x,last_name:y,avatar:z}},{id:V,pe_blog_id:P,directus_users_id:{first_name:g,last_name:h,avatar:i}}]},{id:Q,status:o,sort:aM,date_created:"2024-06-04T13:26:57.728Z",date_updated:"2024-06-13T11:11:41.097Z",slug:"best-practices-for-writing-dockerfiles",title:aN,description:"\u003Cp\u003EDocker has become an essential tool for developers and system administrators, providing a lightweight and portable way to package and distribute applications.\u003C\u002Fp\u003E",seo_title:aN,seo_description:"Docker has become an essential tool for developers and system administrators, providing a lightweight and portable way to package and distribute applications.",content:"\u003Cp dir=\"ltr\"\u003EDocker has become an essential tool for developers and system administrators, providing a lightweight and portable way to package and distribute applications. Writing a Dockerfile is the first step in creating a Docker image, and following best practices can help ensure that your images are efficient, secure, and easy to maintain. In this article, we will explore some of the most important best practices for writing Dockerfiles, with in-depth technical explanations.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\"\u003E1. Use a minimal base image\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003EThe base image is the starting point for your Docker image, and it's important to choose one that is as small and minimal as possible. This will help reduce the size of your final image, which can improve build times and reduce storage costs.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EFor example, instead of using the \u003Ccode\u003Eubuntu\u003C\u002Fcode\u003E base image, which includes a full Linux distribution, you can use the \u003Ccode\u003Ealpine\u003C\u002Fcode\u003E image, which is based on the lightweight Alpine Linux distribution. The \u003Ccode\u003Ealpine\u003C\u002Fcode\u003E image is typically much smaller than the \u003Ccode\u003Eubuntu\u003C\u002Fcode\u003E image, which can result in faster build times and smaller image sizes.\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003EFROM alpine:latest\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Ch3 dir=\"ltr\"\u003E2. Use multi-stage builds\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003EMulti-stage builds allow you to use multiple base images in a single Dockerfile, which can help reduce the size of your final image and improve build times. With multi-stage builds, you can use one base image to compile your application, and then copy the compiled binary to a smaller base image for deployment.&nbsp;\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EFor example, you can use the \u003Ccode\u003Enode:14-alpine\u003C\u002Fcode\u003E image to compile your Node.js application, and then copy the compiled binary to the \u003Ccode\u003Ealpine\u003C\u002Fcode\u003E image for deployment. \u003Ca href=\"https:\u002F\u002Fplatformengineers.io\u002Fblog\u002Fmulti-stage-build-for-ci-cd-pipeline-using-dockerfile\u002F\" target=\"_blank\" rel=\"noopener\"\u003ERead more here about multi stage build\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003E# Build stage\nFROM node:14-alpine as build\n\n\nWORKDIR \u002Fapp\n\n\nCOPY package.json yarn.lock .\u002F\nRUN yarn install\n\n\nCOPY . .\nRUN yarn build\n\n\n# Deploy stage\nFROM alpine:latest\n\n\nWORKDIR \u002Fapp\n\n\nCOPY --from=build \u002Fapp\u002Fdist .\u002Fdist\n\n\nCMD [\"node\", \"dist\u002Findex.js\"]\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Ch3 dir=\"ltr\"\u003E3. Use the \u003Ccode\u003E.dockerignore\u003C\u002Fcode\u003E file\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003EThe \u003Ccode\u003E.dockerignore\u003C\u002Fcode\u003E file allows you to exclude files and directories from the build context, which can help reduce build times and improve security. By default, Docker includes the entire contents of the build context in the final image, which can result in large image sizes and potential security vulnerabilities.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EFor example, you can use the \u003Ccode\u003E.dockerignore\u003C\u002Fcode\u003E file to exclude the \u003Ccode\u003Enode_modules\u003C\u002Fcode\u003E directory, which is not needed in the final image.\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003Enode_modules\u002F\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Ch3 dir=\"ltr\"\u003E4. Use environment variables\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003EEnvironment variables allow you to configure your application at runtime, without hardcoding values in your code. This can help improve security and make it easier to manage your application in different environments.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EFor example, you can use environment variables to configure the database connection string for your application.\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003EENV DATABASE_URL=postgresql:\u002F\u002Fuser:password@host:port\u002Fdatabase\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Ch3 dir=\"ltr\"\u003E5. Use the \u003Ccode\u003ECOPY\u003C\u002Fcode\u003E command carefully\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003EThe \u003Ccode\u003ECOPY\u003C\u002Fcode\u003E command is used to copy files and directories from the build context to the Docker image. However, it's important to use it carefully, as it can result in large image sizes and potential security vulnerabilities.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EFor example, instead of copying the entire contents of your project directory, you can use the \u003Ccode\u003E.dockerignore\u003C\u002Fcode\u003E file to exclude unnecessary files and directories, and then copy only the files that are needed for your application.\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003ECOPY package.json yarn.lock .\u002F\nRUN yarn install\n\n\nCOPY . .\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Ch3 dir=\"ltr\"\u003E6. Use the \u003Ccode\u003ERUN\u003C\u002Fcode\u003E command to install dependencies\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003EThe \u003Ccode\u003ERUN\u003C\u002Fcode\u003E command is used to execute commands in the Docker image. It's important to use it to install dependencies, rather than installing them manually on the host system. This can help ensure that your application is portable and consistent across different environments.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EFor example, you can use the \u003Ccode\u003ERUN\u003C\u002Fcode\u003E command to install the \u003Ccode\u003Ebuild-essential\u003C\u002Fcode\u003E package, which is needed to compile C++ code.\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003ERUN apk add --no-cache build-essential\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Ch3 dir=\"ltr\"\u003E7. Use the \u003Ccode\u003EENTRYPOINT\u003C\u002Fcode\u003E command\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003EThe \u003Ccode\u003EENTRYPOINT\u003C\u002Fcode\u003E command is used to specify the command that should be executed when the Docker container starts. This can help ensure that your application starts correctly, and it can also make it easier to manage your application in different environments.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EFor example, you can use the \u003Ccode\u003EENTRYPOINT\u003C\u002Fcode\u003E command to start your Node.js application.\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003EENTRYPOINT [\"node\", \"dist\u002Findex.js\"]\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Ch3 dir=\"ltr\"\u003E8. Use the \u003Ccode\u003EHEALTHCHECK\u003C\u002Fcode\u003E command\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003EThe \u003Ccode\u003EHEALTHCHECK\u003C\u002Fcode\u003E command is used to specify a command that should be executed periodically to check the health of the Docker container. This can help ensure that your application is running correctly, and it can also help you identify and troubleshoot issues more quickly.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EFor example, you can use the HEALTHCHECK command to check that your web server is responding to requests.\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003EHEALTHCHECK --interval=5s --timeout=3s --retries=3 CMD curl --fail http:\u002F\u002Flocalhost:3000\u002Fhealthz || exit 1\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Ch3 dir=\"ltr\"\u003E9. Use the \u003Ccode\u003EUSER\u003C\u002Fcode\u003E command\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003EThe \u003Ccode\u003EUSER\u003C\u002Fcode\u003E command is used to specify the user that should be used to run the Docker container. This can help improve security by reducing the privileges of the user running the container.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EFor example, you can use the \u003Ccode\u003EUSER\u003C\u002Fcode\u003E command to switch to the \u003Ccode\u003Enode\u003C\u002Fcode\u003E user, which has limited privileges.\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003EUSER node\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Ch3 dir=\"ltr\"\u003E10. Use the \u003Ccode\u003EVOLUME\u003C\u002Fcode\u003E command\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003EThe \u003Ccode\u003EVOLUME\u003C\u002Fcode\u003E command is used to specify a directory that should be used to store data outside of the Docker container. This can help improve performance and make it easier to manage your data in different environments.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EFor example, you can use the \u003Ccode\u003EVOLUME\u003C\u002Fcode\u003E command to store your application's logs outside of the container.\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003EVOLUME [\"\u002Fapp\u002Flogs\"]\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Ch3 dir=\"ltr\"\u003EConclusion\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003EWriting a Dockerfile is an important step in creating a Docker image, and following best practices can help ensure that your images are efficient, secure, and easy to maintain. By using a minimal base image, multi-stage builds, environment variables, and other best practices, you can create Docker images that are optimized for performance and security.\u003C\u002Fp\u003E",tags:a,time_to_read:w,user_created:{id:f,first_name:g,last_name:h,email:k,password:b,location:a,title:l,description:a,tags:a,avatar:i,language:a,tfa_secret:a,status:c,role:j,token:a,last_access:m,last_page:n,provider:d,external_identifier:a,auth_data:a,email_notifications:e,appearance:a,theme_dark:a,theme_light:a,theme_light_overrides:a,theme_dark_overrides:a},user_updated:{id:f,first_name:g,last_name:h,email:k,password:b,location:a,title:l,description:a,tags:a,avatar:i,language:a,tfa_secret:a,status:c,role:j,token:a,last_access:m,last_page:n,provider:d,external_identifier:a,auth_data:a,email_notifications:e,appearance:a,theme_dark:a,theme_light:a,theme_light_overrides:a,theme_dark_overrides:a},image:{id:"fd790772-386c-4d31-baa3-1bdeb3914fe6",storage:p,filename_disk:"fd790772-386c-4d31-baa3-1bdeb3914fe6.png",filename_download:"Untitled design (2).png",title:"Untitled Design (2)",type:r,folder:q,uploaded_by:f,created_on:aO,modified_by:a,modified_on:"2024-06-04T13:35:49.348Z",charset:a,filesize:"208624",width:J,height:J,duration:a,embed:a,description:a,location:a,tags:a,metadata:{},focal_point_x:a,focal_point_y:a,tus_id:a,tus_data:a,uploaded_on:aO},authors:[{id:24,pe_blog_id:Q,directus_users_id:{first_name:g,last_name:h,avatar:i}},{id:Y,pe_blog_id:Q,directus_users_id:{first_name:x,last_name:y,avatar:z}}]},{id:R,status:o,sort:S,date_created:"2024-05-30T10:55:00.000Z",date_updated:"2024-06-05T05:55:45.094Z",slug:"debugging-open-tofu-with-remote-execution",title:aP,description:"\u003Cp dir=\"ltr\"\u003ERemote execution is a feature of OpenTofu that allows you to execute commands on a remote machine instead of your local machine. This can be useful when you need to debug issues that are specific to the remote environment, such as permissions or network connectivity.\u003C\u002Fp\u003E",seo_title:aP,seo_description:"Remote execution is a feature of OpenTofu that allows you to execute commands on a remote machine instead of your local machine.",content:"\u003Cp dir=\"ltr\"\u003EOpenTofu is an open-source fork of Terraform, a popular infrastructure-as-code (IaC) tool. It was created in response to HashiCorp's decision to change the license for Terraform from the Mozilla Public License (MPL) to the Business Source License (BSL) in August 2023. This change restricted the use of Terraform in production environments, leading to concerns among the open-source community. OpenTofu is a drop-in replacement for Terraform v1.6.x and is fully backward compatible with all prior versions. It employs a declarative syntax similar to the Hashicorp Configuration Language (HCL) and supports the same workflow as Terraform, including the \u003Cem\u003Ewrite -&gt; plan -&gt; apply\u003C\u002Fem\u003E process.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EOpenTofu is committed to being truly open source, community-driven, impartial, layered and modular, and backward compatible. The project aims to provide a reliable and accessible IaC tool for the tech community, ensuring that the essential building blocks of the modern internet remain truly open source. We are committed to using OpenTofu as our IaC tool of choice, leveraging its open-source nature and community-driven development to ensure the long-term sustainability and reliability of our infrastructure deployments.&nbsp;\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003ERemote execution is a feature of OpenTofu that allows you to execute commands on a remote machine instead of your local machine. This can be useful when you need to debug issues that are specific to the remote environment, such as permissions or network connectivity.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\"\u003E\u003Cstrong\u003EHow Remote Execution Works in OpenTofu\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003ETo use remote execution, you will need to have a remote machine that is running the OpenTofu binary and has access to the necessary resources. You can use any machine that meets these requirements, such as a virtual machine or a remote server.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EOnce you have a remote machine set up, you can use the OpenTofu CLI to execute commands on it. The following command will execute the OpenTofu plan command on the remote machine:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003Eopentofu plan -out=tfplan -remote=ssh:\u002F\u002Fuser@remote_machine_ip\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp dir=\"ltr\"\u003EThis command will connect to the remote machine using SSH and execute the OpenTofu plan command. The \u003Ccode\u003E-out\u003C\u002Fcode\u003E flag specifies the name of the plan file that will be generated, and the \u003Ccode\u003E-remote\u003C\u002Fcode\u003Eflag specifies the remote machine to connect to.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EYou can also use the \u003Ccode\u003E-var\u003C\u002Fcode\u003E flag to pass variables to the remote machine. For example, the following command will pass the \u003Ccode\u003Eaws_access_key\u003C\u002Fcode\u003E and \u003Ccode\u003Eaws_secret_key\u003C\u002Fcode\u003E variables to the remote machine:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003Eopentofu plan -out=tfplan -remote=ssh:\u002F\u002Fuser@remote_machine_ip -var 'aws_access_key=&lt;&nbsp;access_key&nbsp;&gt;' -var 'aws_secret_key=&lt;&nbsp;secret_key&nbsp;&gt;'\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp dir=\"ltr\"\u003EOnce the plan command has completed, you can use the OpenTofu apply command to apply the changes to the remote environment. The following command will apply the changes specified in the \u003Ccode\u003Etfplan\u003C\u002Fcode\u003E file:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003Eopentofu apply tfplan -remote=ssh:\u002F\u002Fuser@remote_machine_ip\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp dir=\"ltr\"\u003EIf you encounter any errors during the deployment process, you can use the OpenTofu output command to view the output of the remote machine.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EThe following command will display the output of the&nbsp;\u003Ccode\u003Eaws_instance\u003C\u002Fcode\u003E resource:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003Eopentofu output aws_instance_public_ip -remote=ssh:\u002F\u002Fuser@remote_machine_ip\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp dir=\"ltr\"\u003EYou can also use the OpenTofu state command to view the current state of the remote environment.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EThe following command will display the current state of the&nbsp;\u003Ccode\u003Eaws_instance\u003C\u002Fcode\u003E resource:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003Eopentofu state show aws_instance.example -remote=ssh:\u002F\u002Fuser@remote_machine_ip\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp dir=\"ltr\"\u003EIf you need to make changes to the remote environment, you can use the OpenTofu console command to open an interactive console session on the remote machine.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EThe following command will open a console session on the remote machine:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003Eopentofu console -remote=ssh:\u002F\u002Fuser@remote_machine_ip\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003EOnce you are in the console session, you can use the OpenTofu CLI to make changes to the remote environment. \u003Cstrong\u003EFor example, the following command will create a new AWS instance:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003Eaws_instance.example = aws_instance.new(\nami           = \"ami-0c94855ba95c574c8\",\ninstance_type = \"t2.micro\",\nsubnet_id     = \"subnet-0123456789abcdef0\",\n)\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003EWhen you are finished making changes, you can use the OpenTofu apply command to apply the changes to the remote environment.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EHere is an example of a OpenTofu configuration file that uses remote execution to deploy an AWS instance:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003Eprovider \"aws\" {\n region = \"us-west-2\"\n}\n\n\nresource \"aws_instance\" \"example\" {\n ami           = \"ami-0c94855ba95c574c8\"\n instance_type = \"t2.micro\"\n subnet_id     = \"subnet-0123456789abcdef0\"\n\n\n provisioner \"remote-exec\" {\n   inline = [\n     \"sudo yum update -y\",\n     \"sudo amazon-linux-extras install -y nginx1\",\n     \"sudo service nginx start\",\n   ]\n }\n}\n\n\noutput \"aws_instance_public_ip\" {\n value = aws_instance.example.public_ip\n }\n}\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003E\u003Cstrong\u003E&nbsp;\u003C\u002Fstrong\u003EThis configuration file uses the \u003Ccode\u003Eremote-exec\u003C\u002Fcode\u003E provisioner to execute commands on the remote AWS instance after it has been created. The \u003Ccode\u003Einline\u003C\u002Fcode\u003E argument specifies a list of commands to execute, which in this case updates the package manager, installs Nginx, and starts the Nginx service.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003ETo deploy this configuration using remote execution, you can use the following command:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003Eopentofu apply -out=tfplan -remote=ssh:\u002F\u002Fuser@remote_machine_ip\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp dir=\"ltr\"\u003EThis command will connect to the remote machine using SSH and execute the OpenTofu apply command. The \u003Ccode\u003E-out\u003C\u002Fcode\u003E flag specifies the name of the plan file that will be generated, and the \u003Ccode\u003E-remote\u003C\u002Fcode\u003E flag specifies the remote machine to connect to.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EOnce the deployment has completed, you can use the OpenTofu output command to view the public IP address of the AWS instance:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003Eopentofu output aws_instance_public_ip -remote=ssh:\u002F\u002Fuser@remote_machine_ip\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp dir=\"ltr\"\u003EThis will display the public IP address of the AWS instance, which you can use to access the Nginx service\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EHere is an example of a OpenTofu configuration file that uses the null_resource resource to execute commands on the remote machine during the deployment process:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003Eprovider \"aws\" {\n region = \"us-west-2\"\n}\n\n\nresource \"aws_instance\" \"example\" {\n ami           = \"ami-0c94855ba95c574c8\"\n instance_type = \"t2.micro\"\n subnet_id     = \"subnet-0123456789abcdef0\"\n}\n\n\nresource \"null_resource\" \"example\" {\n provisioner \"remote-exec\" {\n   inline = [\n     \"sudo yum update -y\",\n     \"sudo amazon-linux-extras install -y nginx1\",\n     \"sudo service nginx start\",\n   ]\n\n\n   connection {\n     type        = \"ssh\"\n     user        = \"ec2-user\"\n     host        = aws_instance.example.public_ip\n     private_key = file(\"~\u002F.ssh\u002Fid_rsa\")\n   }\n }\n}\n\n\noutput \"aws_instance_public_ip\" {\n value = aws_instance.example.public_ip\n}\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003E\u003Cstrong\u003E&nbsp;\u003C\u002Fstrong\u003EThis configuration file uses the \u003Ccode\u003Enull_resource\u003C\u002Fcode\u003E resource to execute commands on the remote AWS instance during the deployment process. The \u003Ccode\u003Eprovisioner\u003C\u002Fcode\u003E block specifies the commands to execute, and the connection block specifies the \u003Ccode\u003Econnection\u003C\u002Fcode\u003E details for the remote machine.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EHere are some best practices for using remote execution in OpenTofu:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003EUse the \u003Ccode\u003Eremote-exec\u003C\u002Fcode\u003E provisioner for one-time setup tasks, such as installing packages or configuring services.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003EUse the \u003Ccode\u003Enull_resource\u003C\u002Fcode\u003E resource for tasks that need to be executed multiple times during the deployment process, such as running scripts or updating configuration files.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003EUse the \u003Ccode\u003Econnection\u003C\u002Fcode\u003E block to specify the connection details for the remote machine, such as the user name, host name, and private key.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003EUse the \u003Ccode\u003Einline\u003C\u002Fcode\u003E argument to specify a list of commands to execute on the remote machine.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003EUse the \u003Ccode\u003Einterpreter\u003C\u002Fcode\u003E argument to specify the command interpreter to use on the remote machine, such as \u003Ccode\u003E\u002Fbin\u002Fbash\u003C\u002Fcode\u003E or \u003Ccode\u003E\u002Fbin\u002Fsh\u003C\u002Fcode\u003E.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003EUse the \u003Ccode\u003Eworking_dir\u003C\u002Fcode\u003E argument to specify the working directory for the commands on the remote machine.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003EUse the \u003Ccode\u003Eenvironment\u003C\u002Fcode\u003E argument to specify environment variables for the commands on the remote machine.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003EUse the \u003Ccode\u003Etimeouts\u003C\u002Fcode\u003E argument to specify the maximum amount of time to wait for the commands to complete on the remote machine.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp dir=\"ltr\"\u003EBy following these best practices, you can ensure that your OpenTofu configurations are robust and reliable, and that you can effectively debug issues that may arise during the deployment process.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\"\u003E\u003Cstrong\u003EConclusion\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003EOpenTofu remote execution is a powerful feature that allows users to execute scripts or shell commands on remote machines as part of resource creation or deletion. However, it should be used with caution and only as a last resort. The main reason for this is that provisioners add considerable complexity and uncertainty to OpenTofu usage, making it difficult to model the actions of provisioners as part of a plan. Successful use of provisioners requires coordinating more details than OpenTofu usage usually requires, such as direct network access to servers, issuing OpenTofu credentials to log in, and ensuring that all necessary external software is installed.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EMoreover, OpenTofu provisioners can be slow and may cause performance issues, especially when dealing with large and highly connected configuration graphs or resources with very large numbers of instances. This can lead to long plan times, making it troublesome for large-scale deployments.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EAnother complexity of OpenTofu remote execution is that it lacks idempotence, meaning that it will only run \"file\", \"remote-exec\", or \"local-exec\" on resources once. If the commands in a \"remote-exec\" are changed or a file from the provisioner \"file\" is updated, OpenTofu will not re-run the provisioner automatically. This can be a drawback compared to other tools like Ansible, which can achieve the same tasks as OpenTofu but with idempotence.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EIn conclusion, while OpenTofu remote execution can be a useful feature, it should be used judiciously and with careful consideration of its complexities. It is recommended to use provisioners only when there is no other option and to explore alternative solutions that can provide more efficient and idempotent infrastructure provisioning.\u003C\u002Fp\u003E",tags:a,time_to_read:w,user_created:{id:f,first_name:g,last_name:h,email:k,password:b,location:a,title:l,description:a,tags:a,avatar:i,language:a,tfa_secret:a,status:c,role:j,token:a,last_access:m,last_page:n,provider:d,external_identifier:a,auth_data:a,email_notifications:e,appearance:a,theme_dark:a,theme_light:a,theme_light_overrides:a,theme_dark_overrides:a},user_updated:{id:f,first_name:g,last_name:h,email:k,password:b,location:a,title:l,description:a,tags:a,avatar:i,language:a,tfa_secret:a,status:c,role:j,token:a,last_access:m,last_page:n,provider:d,external_identifier:a,auth_data:a,email_notifications:e,appearance:a,theme_dark:a,theme_light:a,theme_light_overrides:a,theme_dark_overrides:a},image:{id:"0736c072-8ebd-43ea-b349-b2132a79ac68",storage:p,filename_disk:"0736c072-8ebd-43ea-b349-b2132a79ac68.png",filename_download:"opentofu (1).png",title:"Opentofu (1)",type:r,folder:q,uploaded_by:f,created_on:aQ,modified_by:a,modified_on:"2024-04-26T11:24:38.247Z",charset:a,filesize:"110917",width:J,height:J,duration:a,embed:a,description:a,location:a,tags:a,metadata:{},focal_point_x:a,focal_point_y:a,tus_id:a,tus_data:a,uploaded_on:aQ},authors:[{id:O,pe_blog_id:R,directus_users_id:{first_name:x,last_name:y,avatar:z}},{id:Z,pe_blog_id:R,directus_users_id:{first_name:g,last_name:h,avatar:i}}]},{id:S,status:o,sort:_,date_created:"2024-04-24T06:48:00.000Z",date_updated:"2024-04-24T06:48:52.716Z",slug:"continuous-delivery-using-git-ops-principles-with-flux-cd",title:aR,description:"\u003Cp\u003E\u003Cstrong id=\"docs-internal-guid-dde85188-7fff-b52a-04ab-f992f6d9977a\"\u003ELearn how to implement GitOps principles in your software delivery pipeline using FluxCD for continuous delivery.\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E",seo_title:aR,seo_description:"Learn how to implement GitOps principles in your software delivery pipeline using FluxCD for continuous delivery.",content:"\u003Cp dir=\"ltr\"\u003EContinuous Delivery (CD) automates the software release process, allowing teams to deliver features quickly and reliably. By combining CD practices with GitOps principles, developers can keep version control tools to manage their infrastructure and applications, ensuring consistent and predictable outcomes through automated pipelines.&nbsp;\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\"\u003EPrerequisites\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003EBefore proceeding, ensure the following requirements are met:\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003EA running Kubernetes cluster accessible to you. Refer to the \u003Ca href=\"https:\u002F\u002Fkubernetes.io\u002Fdocs\u002Fsetup\u002F\"\u003EKubernetes Docs\u003C\u002Fa\u003E for setup guidance.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003EAccess to the \u003Ca href=\"https:\u002F\u002Fgithub.com\u002FImprowised\u002Fflux-starter-kit\"\u003EStarter Kit \u003C\u002Fa\u003Erepository. (Refer this \u003Ca href=\"https:\u002F\u002Fgithub.com\u002FImprowised\u002Fflux-starter-kit?tab=readme-ov-file#deploy-apps-in-locally-follow-below-steps\"\u003Einstructions\u003C\u002Fa\u003E on how to run this repository)\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003EFamiliarity with Kubernetes interactions, including connecting to clusters using&nbsp;\u003Cspan style=\"background-color: #cccccc; border-radius: 3px; padding: 0px 4px; display: inline-block; color: green;\"\u003Ekubectl\u003C\u002Fspan\u003E.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch3 dir=\"ltr\"\u003E\u003Cstrong\u003EReconciling Cluster State with Git Repository\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Ca href=\"https:\u002F\u002Ffluxcd.io\u002F\" target=\"_blank\" rel=\"noopener\"\u003EFluxCD \u003C\u002Fa\u003Esynchronizes the state of your infrastructure using Git as the source of truth, following GitOps principles. This process, called reconciliation, ensures applications match a desired state declaratively defined somewhere, such as a \u003Ca href=\"https:\u002F\u002Fgit-scm.com\u002F\" target=\"_blank\" rel=\"noopener\"\u003EGit repository\u003C\u002Fa\u003E, \u003Ca href=\"https:\u002F\u002Fhelm.sh\u002Fdocs\u002Ftopics\u002Fchart_repository\u002F\" target=\"_blank\" rel=\"noopener\"\u003EHelm repository\u003C\u002Fa\u003E, or \u003Ca href=\"https:\u002F\u002Faws.amazon.com\u002Fs3\u002F\" target=\"_blank\" rel=\"noopener\"\u003ES3 bucket\u003C\u002Fa\u003E. FluxCD provides several ways to achieve reconciliation:\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003EHelm Controller defined in a Git repository or S3 bucket.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003EControllers containing necessary logic to fetch artifacts containing declarative state manifests and apply required changes to your cluster.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch3 dir=\"ltr\"\u003ESource CRD (Custom Resource Definition)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003EFluxCD treats sources as a way of fetching artifacts containing state configuration. For example, it can pull data from Git repositories, Helm repositories, S3 buckets, etc.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\"\u003E\u003Cstrong\u003EImplementing CD with FluxCD\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003EThe Starter Kit uses the Git repository source type and HelmReleases for maintaining application state. Each chapter of the Starter Kit uses Helm to perform application deployment, making HelmReleases a natural choice for managing application lifecycle via the standard package manager for Kubernetes.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\"\u003E\u003Cstrong\u003EManaging Helm Charts with FluxCD\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003EFluxCD handles Helm charts through dedicated custom resource definitions (CRDs), specifically the HelmRepository and HelmRelease CRDs. These resources allow FluxCD to manage Helm repositories and fetch charts from remote locations, similar to running \u003Cspan style=\"background-color: #cccccc; border-radius: 3px; padding: 0px 4px; display: inline-block; color: green;\"\u003Ehelm repo add \u003Cspan style=\"background-color: #cccccc; border-radius: 3px; padding: 0px 4px; display: inline-block; color: green;\"\u003E&lt;\u003C\u002Fspan\u003Ename\u003Cspan style=\"background-color: #cccccc; border-radius: 3px; padding: 0px 4px; display: inline-block; color: green;\"\u003E&gt;\u003C\u002Fspan\u003E &lt; url &gt;\u003C\u002Fspan\u003E&nbsp;and \u003Cspan style=\"background-color: #cccccc; border-radius: 3px; padding: 0px 4px; display: inline-block; color: green;\"\u003Ehelm repo update\u003C\u002Fspan\u003E.\u003Cbr\u003E\u003Cbr\u003E\u003C\u002Fp\u003E\n\u003Ch4 dir=\"ltr\"\u003ETypical Structure of a HelmRepository Manifest\u003C\u002Fh4\u003E\n\u003Cdiv class=\"code-block with-line-numbers\"\u003E\n\u003Cpre\u003E\u003Ccode\u003EapiVersion: source.toolkit.fluxcd.io\u002Fv1beta1\nkind: HelmRepository\nmetadata:\n  name: polymorphic-helm-source\n  namespace: flux-system\nspec:\n  interval: 10m0s\n  url: https:\u002F\u002Fimprowised.github.io\u002Fcharts\u002F\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Ch4 dir=\"ltr\"\u003ETypical Structure of a HelmRelease Manifest\u003C\u002Fh4\u003E\n\u003Cdiv class=\"code-block with-line-numbers\"\u003E\n\u003Cpre\u003E\u003Ccode\u003EapiVersion: helm.toolkit.fluxcd.io\u002Fv2beta1\nkind: HelmRelease\nmetadata:\n  name: nginx-app\n  namespace: flux-system\nspec:\n  chart:\n    spec:\n      chart: polymorphic-app\n      reconcileStrategy: ChartVersion\n      version: 1.3.0\n      sourceRef:\n        kind: HelmRepository\n        name: polymorphic-helm-source\n        namespace: flux-system\n  interval: 10m\u003C\u002Fcode\u003E\u003Ccode\u003E\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp dir=\"ltr\"\u003EEach Flux CD \u003Ca href=\"https:\u002F\u002Ffluxcd.io\u002Fflux\u002Fcomponents\u002Fhelm\u002Fhelmreleases\u002F\" target=\"_blank\" rel=\"noopener\"\u003EHelmRelease\u003C\u002Fa\u003E can override values via a values file or individually using key-value pairs.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\"\u003E\u003Cstrong\u003EOverriding Values with FluxCD\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003EFluxCD lets you override Helm values via two spec types:\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp\u003E\u003Cspan style=\"background-color: #cccccc; border-radius: 3px; padding: 0px 4px; display: inline-block; color: green;\"\u003E&lt; spec.values &gt;\u003C\u002Fspan\u003E&nbsp;: Allows overriding values inline as seen in a standard values.yaml file.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp\u003E\u003Cspan style=\"background-color: #cccccc; border-radius: 3px; padding: 0px 4px; display: inline-block; color: green;\"\u003E&lt; spec.valuesFrom &gt;\u003C\u002Fspan\u003E&nbsp;: Allows overriding values individually by using each key's fully qualified path from the values file.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp\u003E\u003Cstrong\u003ETypical usage of \u003Cspan style=\"background-color: #cccccc; border-radius: 3px; padding: 0px 4px; display: inline-block; color: green;\"\u003Espec.values\u003C\u002Fspan\u003E\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cdiv class=\"code-block with-line-numbers\"\u003E\n\u003Cpre\u003E\u003Ccode\u003E...\nspec:\n  values:\n    fullnameOverride: \"nginx\"\n    # service template\n    serviceTemplate:\n      autoscaling: false\n      minReplicaCount: 1\n      maxReplicaCount: 1\n      env: []\n      envFrom: []\n    services:\n      - name: nginx-app\n        image:\n          repository: nginx\n          tag: 1.25.4-alpine\n        minReplicaCount: 1\n        ports:\n          - name: http\n            containerPort: 80\n            protocol: TCP\n\n...\u003C\u002Fcode\u003E\u003Ccode\u003E\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Ch3 dir=\"ltr\"\u003EDeploying FluxCD to Your Kubernetes Cluster\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003EDeploying FluxCD involves using its CLI tool to execute commands on your Kubernetes cluster. To begin, you must first authenticate yourself to your cluster and obtain a personal access token. You can verify that everything is working correctly by performing some sanity checks:\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003Eflux check&nbsp;\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EThe output of the flux check command will indicate whether or not the configuration files are correctly deployed on your cluster. If there are no differences, you should see a message similar to the following:\u003C\u002Fp\u003E\n\u003Cdiv class=\"code-block with-line-numbers\"\u003E\n\u003Cpre\u003E\u003Ccode\u003EChecking sync status...\nWaiting for API server to become ready...\nSyncing now...\nSuccessfully synced 0 resources\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp dir=\"ltr\"\u003EIf there are differences, the command will provide details about which resources need to be updated to match the desired state. In such cases, you may need to manually update the configuration files in the Git repository to reflect the changes you want to make.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003Eflux get all\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cdiv class=\"code-block with-line-numbers\"\u003E\n\u003Cpre\u003E\u003Ccode\u003ENAME                                          \tREVISION       \tSUSPENDED\tREADY\tMESSAGE                                     \nhelmrepository\u002Factions-runner-controller      \tsha256:6469d242\tFalse    \tTrue \tstored artifact: revision 'sha256:6469d242'\t\nhelmrepository\u002Fbitnami                        \tsha256:293ea2e0\tFalse    \tTrue \tstored artifact: revision 'sha256:293ea2e0'\t\nhelmrepository\u002Fpolymorphic-helm-source                        \t              \tFalse    \tTrue \tHelm repository is Ready  \n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cdiv class=\"code-block with-line-numbers\"\u003E\n\u003Cpre\u003E\u003Ccode\u003ENAME                                                         \tREVISION\tSUSPENDED\tREADY\tMESSAGE                                                              \nhelmchart\u002Factions-runner-controller-actions-runner-controller\t0.21.0  \tFalse    \tTrue \tpulled 'actions-runner-controller' chart with version '0.21.0'      \t\nhelmchart\u002Fnginx-app                            \t1.3.0   \tFalse    \tTrue \tpulled 'polymorphic-app' chart with version '1.3.0'\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003C\u002Fdiv\u003E\n\u003Cdiv class=\"code-block with-line-numbers\"\u003E\n\u003Cp dir=\"ltr\"\u003EThis command displays information about the various components managed by FluxCD, allowing you to confirm that everything is configured correctly.\u003C\u002Fp\u003E\n\u003Cstrong id=\"docs-internal-guid-0994cfbe-7fff-57c2-77d2-452cf23657a6\"\u003E\u003C\u002Fstrong\u003E\u003C\u002Fdiv\u003E\n\u003Cdiv class=\"code-block with-line-numbers\"\u003E\n\u003Ch3 dir=\"ltr\"\u003E\u003Cstrong\u003EConfiguring FluxCD to Deploy Applications\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003ETo configure FluxCD to deploy applications, you need to create a HelmReleases resource that references the HelmRelease CRD. This HelmReleases resource will be responsible for deploying the application to your cluster.\u003C\u002Fp\u003E\n\u003Ch4 dir=\"ltr\"\u003ETypical Structure of a HelmRepository Manifest\u003C\u002Fh4\u003E\n\u003Cdiv class=\"code-block with-line-numbers\"\u003E\n\u003Cpre\u003E\u003Ccode\u003EapiVersion: kustomize.config.k8s.io\u002Fv1beta1\nkind: Kustomization\nmetadata:\n  name: my-app\n  namespace: flux-system\nspec:\n  resources:\n  - helmreleases\u002Fmy-app.yaml\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cstrong\u003EDeploying Applications with FluxCD\u003C\u002Fstrong\u003E\n\u003Cp dir=\"ltr\"\u003EOnce you have created the HelmReleases resource, you can deploy your application by applying the HelmReleases manifest to your cluster:\u003C\u002Fp\u003E\n\u003Cdiv class=\"code-block with-line-numbers\"\u003E\n\u003Cpre\u003E\u003Ccode\u003Ekubectl apply -f my-app.yaml\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp dir=\"ltr\"\u003EThis command deploys the application to your cluster, and FluxCD will automatically reconcile the application state with the desired state defined in the HelmRelease CRD.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\"\u003E\u003Cstrong\u003EMonitoring and Troubleshooting FluxCD\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003EFluxCD provides several tools for monitoring and troubleshooting your cluster. You can use the \u003Cspan style=\"background-color: #cccccc; border-radius: 3px; padding: 0px 4px; display: inline-block; color: green;\"\u003Eflux logs\u003C\u002Fspan\u003E command to view logs for all FluxCD components:\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003Eflux logs\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003ETo demonstrate the usage of Flux logs command, let's assume we have Flux installed in our cluster with custom resources named monitoring-configs in the namespace flux-system. We want to see the logs for this specific custom resource for the last minute. Here's how we can use the flux logs command to achieve this:\u003C\u002Fp\u003E\n\u003Cdiv class=\"code-block with-line-numbers\"\u003E\n\u003Cpre\u003E\u003Ccode\u003Eflux logs --kind=Helmrelease --name=nginx-app --namespace=flux-system\n2024-04-08T11:55:58.971Z info HelmRelease\u002Fnginx-app.flux-system - reconcilation finished in 110.570109ms, next run in 10m0s \n2024-04-08T12:05:59.167Z info HelmRelease\u002Fnginx-app.flux-system - reconcilation finished in 195.47602ms, next run in 10m0s \n2024-04-08T12:15:59.340Z info HelmRelease\u002Fnginx-app.flux-system - reconcilation finished in 170.867232ms, next run in 10m0s \n2024-04-08T12:25:59.451Z info HelmRelease\u002Fnginx-app.flux-system - reconcilation finished in 99.786584ms, next run in 10m0s\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cdiv class=\"code-block with-line-numbers\"\u003E\n\u003Cpre\u003E\u003Ccode\u003Eflux logs --kind=Helmrepository --name=polymorphic-helm-source --namespace=flux-system\n2024-03-29T10:55:35.203Z info HelmRepository\u002Fpolymorphic-helm-source.flux-system - Discarding event, no alerts found for the involved object \n2024-03-29T12:55:35.985Z info HelmRepository\u002Fpolymorphic-helm-source.flux-system - Discarding event, no alerts found for the involved object \n2024-04-04T10:56:11.562Z info HelmRepository\u002Fpolymorphic-helm-source.flux-system - Discarding event, no alerts found for the involved object \n2024-04-08T04:56:35.119Z info HelmRepository\u002Fpolymorphic-helm-source.flux-system - artifact up-to-date with remote revision: 'sha256:61d24125f529fff38a042f443e87ddd5ae2775a2af674e29926e0ca6e75e525b' \n2024-04-08T05:56:35.450Z info HelmRepository\u002Fpolymorphic-helm-source.flux-system - artifact up-to-date with remote revision: 'sha256:61d24125f529fff38a042f443e87ddd5ae2775a2af674e29926e0ca6e75e525b'\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp dir=\"ltr\"\u003EThis output shows us the logs related to our monitoring-configs custom resource in the flux-system namespace for the last minute (since 1m ago). It includes information about the status of the reconciliation process, such as when all dependencies were ready, when server-side apply was completed, and when the reconciliation finished. By analyzing these logs, we can gain insights into how our Flux components are operating and troubleshoot any issues that might arise.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong id=\"docs-internal-guid-56fe7310-7fff-c141-6c9c-f2cbd03382bd\"\u003Eflux get all\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cdiv class=\"code-block with-line-numbers\"\u003E\n\u003Cpre\u003E\u003Ccode\u003ENAME                                          \tREVISION       \tSUSPENDED\tREADY\tMESSAGE                                     \nhelmrepository\u002Factions-runner-controller      \tsha256:6469d242\tFalse    \tTrue \tstored artifact: revision 'sha256:6469d242'\t\nhelmrepository\u002Fbitnami                        \tsha256:293ea2e0\tFalse    \tTrue \tstored artifact: revision 'sha256:293ea2e0'\t\nhelmrepository\u002Fpolymorphic-helm-source                        \t              \tFalse    \tTrue \tHelm repository is Ready\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cdiv class=\"code-block with-line-numbers\"\u003E\n\u003Cpre\u003E\u003Ccode\u003ENAME                                                         \tREVISION\tSUSPENDED\tREADY\tMESSAGE                                                              \nhelmchart\u002Factions-runner-controller-actions-runner-controller\t0.21.0  \tFalse    \tTrue \tpulled 'actions-runner-controller' chart with version '0.21.0'      \t\nhelmchart\u002Fnginx-app                            \t1.3.0   \tFalse    \tTrue \tpulled 'polymorphic-app' chart with version '1.3.0'\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp dir=\"ltr\"\u003EThis command displays information about the various components managed by FluxCD, allowing you to confirm that everything is configured correctly.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\"\u003E\u003Cstrong\u003EConclusion\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003EBy following the steps outlined in this guide, you have successfully implemented Continuous Delivery using GitOps principles with FluxCD. You have learned how to manage Helm repositories and charts, reconcile your kubernetes cluster state with a Git repository, and use FluxCD to automate your infrastructure management and application deployment processes. With FluxCD installed and configured, you can now focus on delivering high-quality software while ensuring consistent and predictable outcomes through automated pipelines.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EWebhooks allow for immediate deployment of new versions when certain events occur. They work by sending an HTTP POST request to a specified URL whenever a specific event happens, triggering the deployment process.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EThe FluxCD Notification Controller is responsible for handling both incoming and outgoing events within the GitOps toolkit ecosystem. It serves two primary functions:\u003C\u002Fp\u003E\n\u003Col\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003E\u003Cstrong\u003EHandling incoming events:\u003C\u002Fstrong\u003E The controller receives events from external systems such as GitHub, GitLab, Bitbucket, Harbor, Jenkins, and others. These events are related to source changes that need to be notified to the GitOps Toolkit controllers.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003E\u003Cstrong\u003EDispatching outgoing events:\u003C\u002Fstrong\u003E The Notification Controller also handles events emitted by the GitOps Toolkit controllers (source, kustomize, helm) and dispatches them to external systems like Slack, Microsoft Teams, Discord, and others based on event severity and involved objects. This ensures that the appropriate teams are notified about the status of their GitOps pipelines.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003C\u002Fol\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EHere is an example of notification controller:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003ESlack notification of release successful:\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cimg src=\"https:\u002F\u002Fdata.improwised.com\u002Fassets\u002Fc3f7da29-482e-4c42-a835-b7a63c4a3740?width=638&amp;height=629\" alt=\"Helm Controller Notification\"\u003E\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003ESlack notification of error:\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cimg src=\"https:\u002F\u002Fdata.improwised.com\u002Fassets\u002F14bf74ce-4511-4b3e-879c-59edc6faa5e6?width=658&amp;height=378\" alt=\"Fluxcd Error\"\u003E\u003C\u002Fp\u003E\n\u003C\u002Fdiv\u003E\n\u003C\u002Fdiv\u003E\n\u003C\u002Fdiv\u003E\n\u003C\u002Fdiv\u003E\n\u003C\u002Fdiv\u003E\n\u003C\u002Fdiv\u003E\n\u003C\u002Fdiv\u003E\n\u003C\u002Fdiv\u003E\n\u003C\u002Fdiv\u003E\n\u003C\u002Fdiv\u003E\n\u003C\u002Fdiv\u003E\n\u003Cstrong\u003E \u003C\u002Fstrong\u003E\u003C\u002Fdiv\u003E",tags:a,time_to_read:"6 minutes",user_created:{id:f,first_name:g,last_name:h,email:k,password:b,location:a,title:l,description:a,tags:a,avatar:i,language:a,tfa_secret:a,status:c,role:j,token:a,last_access:m,last_page:n,provider:d,external_identifier:a,auth_data:a,email_notifications:e,appearance:a,theme_dark:a,theme_light:a,theme_light_overrides:a,theme_dark_overrides:a},user_updated:{id:f,first_name:g,last_name:h,email:k,password:b,location:a,title:l,description:a,tags:a,avatar:i,language:a,tfa_secret:a,status:c,role:j,token:a,last_access:m,last_page:n,provider:d,external_identifier:a,auth_data:a,email_notifications:e,appearance:a,theme_dark:a,theme_light:a,theme_light_overrides:a,theme_dark_overrides:a},image:{id:"e3afc165-f038-4861-8824-66f9d983ec75",storage:p,filename_disk:"e3afc165-f038-4861-8824-66f9d983ec75.png",filename_download:"flux.png",title:"Flux",type:r,folder:q,uploaded_by:f,created_on:aS,modified_by:a,modified_on:"2024-04-12T10:25:27.469Z",charset:a,filesize:"135463",width:J,height:J,duration:a,embed:a,description:a,location:a,tags:a,metadata:{},focal_point_x:a,focal_point_y:a,tus_id:a,tus_data:a,uploaded_on:aS},authors:[{id:$,pe_blog_id:S,directus_users_id:{first_name:x,last_name:y,avatar:z}},{id:P,pe_blog_id:S,directus_users_id:{first_name:g,last_name:h,avatar:i}}]},{id:aa,status:o,sort:ab,date_created:"2024-04-04T06:24:20.635Z",date_updated:"2024-06-10T07:37:51.233Z",slug:"configurations-for-kubernetes-network-policies",title:aT,description:"\u003Cp dir=\"ltr\"\u003ENetwork Policies (NPs) in Kubernetes provide a granular security mechanism for controlling inbound and outbound&nbsp; traffic to pods. While basic network policies offer essential functionality, complex deployments often necessitate more sophisticated configurations. This blog post delves into advanced Network Policy features, enabling you to create robust and secure communication channels within your Kubernetes clusters.\u003C\u002Fp\u003E\n\u003Cp\u003E&nbsp;\u003C\u002Fp\u003E",seo_title:aT,seo_description:"Implement granular network access controls, manage egress traffic, and leverage network policy features for robust security within your Kubernetes clusters",content:"\u003Ch3 dir=\"ltr\"\u003E\u003Cstrong\u003ETesting Setup\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003EIn this testing setup, we have a Kubernetes cluster running on the lightweight Kubernetes distribution called \u003Ca href=\"https:\u002F\u002Franchermanager.docs.rancher.com\u002Fhow-to-guides\u002Fnew-user-guides\u002Fkubernetes-cluster-setup\u002Fk3s-for-rancher\" target=\"_blank\" rel=\"noopener\"\u003ERancher K3s\u003C\u002Fa\u003E (version v1.29.3+k3s1). This version of K3s provides a minimalistic approach to deploying and managing a Kubernetes cluster with a focus on simplicity and resource efficiency. It supports both ARM64 and x86 architectures, making it suitable for various hardware platforms.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EThe Container Network Interface \u003Ca href=\"https:\u002F\u002Fkubernetes.io\u002Fdocs\u002Fconcepts\u002Fextend-kubernetes\u002Fcompute-storage-net\u002Fnetwork-plugins\u002F\" target=\"_blank\" rel=\"noopener\"\u003E(CNI) plugin\u003C\u002Fa\u003E being used in this setup is \u003Ca href=\"https:\u002F\u002Fwww.tigera.io\u002Fproject-calico\u002F\" target=\"_blank\" rel=\"noopener\"\u003ECalico\u003C\u002Fa\u003E (version v3.27.3), which is a popular open-source networking solution for Kubernetes clusters. Calico offers features like network policy management, IP address management, and BGP routing. By integrating Calico with K3s, we can leverage its advanced networking capabilities while maintaining the lightweight nature of K3s.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\"\u003ENetwork policies support in different CNI\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Ca href=\"https:\u002F\u002Fkubernetes.io\u002Fdocs\u002Fconcepts\u002Fservices-networking\u002Fnetwork-policies\u002F\" target=\"_blank\" rel=\"noopener\"\u003ENetwork policies\u003C\u002Fa\u003E are supported in various types of Container Network Interface (CNI) plugins, including self-hosted, Amazon EKS, Google Cloud Platform (GCP), and others. Here's a breakdown of the support for network policies in each type:\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003ESelf-Hosted: \u003C\u002Fstrong\u003ESelf-hosted CNI plugin support for network policies depends on the specific plugin being used. It is essential to check the documentation of the chosen CNI plugin to ensure it supports network policies.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EAmazon EKS:\u003C\u002Fstrong\u003E In Amazon EKS, the \u003Ca href=\"https:\u002F\u002Fdocs.aws.amazon.com\u002Feks\u002Flatest\u002Fuserguide\u002Fmanaging-vpc-cni.html\" target=\"_blank\" rel=\"noopener\"\u003EAmazon VPC CNI plugin\u003C\u002Fa\u003E supports network policies starting from version 1.14 or later. This integration allows users to control all in-cluster communication, including IP addresses, ports, and connections between pods.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EGoogle Cloud Platform (GCP): \u003C\u002Fstrong\u003EGoogle Cloud Platform does not explicitly mention support for network policies in their CNI plugin documentation. However, since Kubernetes supports network policies natively, it can be assumed that \u003Ca href=\"https:\u002F\u002Fwww.googlecloudcommunity.com\u002Fgc\u002FGoogle-Kubernetes-Engine-GKE\u002FWhat-CNI-plugin-is-used-in-GKE-and-are-there-alternatives\u002Fm-p\u002F671169\" target=\"_blank\" rel=\"noopener\"\u003EGCP's CNI plugin\u003C\u002Fa\u003E would also support network policies indirectly.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EOthers: \u003C\u002Fstrong\u003EFor other CNI plugins like \u003Ca href=\"https:\u002F\u002Fdocs.tigera.io\u002Fcalico\u002Flatest\u002Fabout\u002F\" target=\"_blank\" rel=\"noopener\"\u003ECalico\u003C\u002Fa\u003E, \u003Ca href=\"https:\u002F\u002Fgcore.com\u002Flearning\u002Fconfigure-kubernetes-network-with-flannel\u002F\" target=\"_blank\" rel=\"noopener\"\u003EFlannel\u003C\u002Fa\u003E, \u003Ca href=\"https:\u002F\u002Fubuntu.com\u002Fkubernetes\u002Fdocs\u002Fcni-canal\" target=\"_blank\" rel=\"noopener\"\u003ECanal\u003C\u002Fa\u003E, and \u003Ca href=\"https:\u002F\u002Fkubernetes.io\u002Fdocs\u002Ftasks\u002Fadminister-cluster\u002Fnetwork-policy-provider\u002Fweave-network-policy\u002F\" target=\"_blank\" rel=\"noopener\"\u003EWeave Net,\u003C\u002Fa\u003E it is crucial to consult the respective documentation to understand their support for network policies. Each CNI plugin may have different approaches to implementing network policies, so understanding the specific implementation details is important.\u003C\u002Fp\u003E\n\u003Ch2 dir=\"ltr\"\u003EAdvanced Network Policy Capabilities\u003C\u002Fh2\u003E\n\u003Cp dir=\"ltr\"\u003ENetwork Policies (NPs) in Kubernetes provide a granular security mechanism for controlling inbound and outbound&nbsp; traffic to pods. While basic network policies offer essential functionality, complex deployments often necessitate more sophisticated configurations. This blog post delves into advanced Network Policy features, enabling you to create robust and secure communication channels within your Kubernetes clusters.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EHaving grasped the fundamentals of NetworkPolicy resources, we can explore advanced configurations to achieve finer-grained control over network traffic. Here's a breakdown of key features:\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003E\u003Cstrong\u003EPod Selectors with Multiple Expressions:\u003C\u002Fstrong\u003E Network Policy selectors can encompass multiple expressions to precisely target specific pods or groups of pods. This allows for more granular control over which pods are subject to the policy's rules.\u003C\u002Fp\u003E\n\u003Cdiv class=\"code-block with-line-numbers\" data-language=\"yaml\"\u003E\n\u003Cpre\u003E\u003Ccode spellcheck=\"false\"\u003EYAML\napiVersion: networking.k8s.io\u002Fv1\nkind: NetworkPolicy\nmetadata:\n&nbsp;&nbsp;name: allow-db-access-on-service-port\nspec:\n&nbsp;&nbsp;podSelector:\n&nbsp;&nbsp;&nbsp;&nbsp;matchExpressions:\n&nbsp;&nbsp;&nbsp;&nbsp;- key: app\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;operator: In\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;values:\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- webserver\n&nbsp;&nbsp;ingress:\n&nbsp;&nbsp;- from:\n&nbsp;&nbsp;&nbsp;&nbsp;- podSelector: {}\n&nbsp;&nbsp;&nbsp;&nbsp;ports:\n&nbsp;&nbsp;&nbsp;&nbsp;- protocol: TCP\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;port: 80\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp dir=\"ltr\"\u003EIn the example above, the Network Policy only applies to pods with labels app: database and tier: backend.\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003E\u003Cstrong\u003ENamespace Selectors: \u003C\u002Fstrong\u003ENetwork Policies can be scoped to specific namespaces, restricting traffic flow within those namespaces.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cdiv class=\"code-block with-line-numbers\" data-language=\"yaml\"\u003E\n\u003Cpre\u003E\u003Ccode spellcheck=\"false\"\u003EYAML\napiVersion: networking.k8s.io\u002Fv1\nkind: NetworkPolicy\nmetadata:\n&nbsp;&nbsp;name: restrict-web-ingress\nspec:\n&nbsp;&nbsp;podSelector: {}\n&nbsp;&nbsp;ingress:\n&nbsp;&nbsp;- from:\n&nbsp;&nbsp;&nbsp;&nbsp;- namespaceSelector:\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;matchLabels:\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;kubernetes.io\u002Fmetadata.name: frontend\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp dir=\"ltr\"\u003EHere, the Network Policy allows ingress traffic only from pods within the frontend namespace.\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003E\u003Cstrong\u003EIngress from Specific Ports: \u003C\u002Fstrong\u003ENetwork Policies can restrict traffic by specifying allowed \u003Ca href=\"https:\u002F\u002Fkubernetes.io\u002Fdocs\u002Fconcepts\u002Fservices-networking\u002Fingress\u002F\" target=\"_blank\" rel=\"noopener\"\u003Eingress ports\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cdiv class=\"code-block with-line-numbers\" data-language=\"yaml\"\u003E\n\u003Cpre\u003E\u003Ccode\u003EYAML\napiVersion: networking.k8s.io\u002Fv1\nkind: NetworkPolicy\nmetadata:\n  name: allow-db-access-on-service-port\nspec:\n  podSelector:\n    matchExpressions:\n    - key: app\n      operator: In\n      values:\n      - webserver\n  ingress:\n  - from:\n    - podSelector: {}\n    ports:\n    - protocol: TCP\n      port: 80\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp dir=\"ltr\"\u003EThis policy allows only TCP traffic on port 80 (HTTP) to reach pods with the label app: webserver.\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003E\u003Cstrong\u003EEgress to Specific IPs\u002FCIDRs:\u003C\u002Fstrong\u003E Similar to ingress, Network Policies can control outbound traffic by specifying allowed egress destinations using IP addresses or CIDR blocks.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cdiv class=\"code-block with-line-numbers\" data-language=\"yaml\"\u003E\n\u003Cpre\u003E\u003Ccode\u003EYAML\napiVersion: networking.k8s.io\u002Fv1\nkind: NetworkPolicy\nmetadata:\n  name: restrict-db-egress\nspec:\n  podSelector:\n    matchExpressions:\n    - key: app\n      operator: In\n      values:\n      - database\n  egress:\n  - to:\n    - podSelector: {}\n  - to:\n    - ipBlock:\n        cidr: 10.0.0.0\u002F16\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp dir=\"ltr\"\u003EThis policy allows the database pods to communicate with any pod within the cluster and restricts outbound traffic to the IP range 10.0.0.0\u002F16.\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003E\u003Cstrong\u003EProtocol Specificity: \u003C\u002Fstrong\u003ENetwork Policies can define allowed protocols (TCP, UDP, SCTP) for both ingress and egress traffic, offering granular control over the types of network communication permitted.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cdiv class=\"code-block with-line-numbers\" data-language=\"yaml\"\u003E\n\u003Cpre\u003E\u003Ccode spellcheck=\"false\"\u003EYAML\napiVersion: networking.k8s.io\u002Fv1\nkind: NetworkPolicy\nmetadata:\n  name: allow-dns-udp\nspec:\n  podSelector:\n    matchExpressions:\n    - key: app\n      operator: In\n      values:\n      - webserver\n  ingress:\n  - from:\n    - podSelector: {}\n    ports:\n    - protocol: UDP\n      port: 53\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp dir=\"ltr\"\u003EThe policy above allows only UDP traffic on port 53 (DNS) to reach the webserver pods.\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003E\u003Cstrong\u003EApplication Ports: \u003C\u002Fstrong\u003ENetwork Policies can reference pod ports defined within a Service resource using the applicationPorts field. This simplifies policy creation and avoids hardcoding port numbers.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cdiv class=\"code-block with-line-numbers\" data-language=\"yaml\"\u003E\n\u003Cpre\u003E\u003Ccode spellcheck=\"false\"\u003EYAML\napiVersion: networking.k8s.io\u002Fv1\nkind: NetworkPolicy\nmetadata:\n  name: allow-http-ingress\nspec:\n  podSelector:\n    matchExpressions:\n    - key: app\n      operator: In\n      values:\n      - webserver\n  ingress:\n  - from:\n    - podSelector: {}\n    ports:\n    - protocol: TCP\n      port: 80\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Ch3 dir=\"ltr\"\u003E\u003Cstrong\u003EAdvanced Configurations\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cul\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003E\u003Cstrong\u003ECombining Ingress and Egress Rules:\u003C\u002Fstrong\u003E Network Policies allow combining ingress and egress rules within a single policy to define comprehensive communication restrictions for a set of pods.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cdiv class=\"code-block with-line-numbers\" data-language=\"yaml\"\u003E\n\u003Cpre\u003E\u003Ccode spellcheck=\"false\"\u003EYAML\napiVersion: networking.k8s.io\u002Fv1\nkind: NetworkPolicy\nmetadata:\n name: restrict-db-communication\nspec:\n podSelector:\n   matchExpressions:\n   - key: app\n     operator: In\n     values:\n     - database\n ingress:\n - from:\n   - podSelector:\n       matchExpressions:\n       - key: app\n         operator: In\n         values:\n         - webserver\n egress:\n - to:\n   - podSelector: {}\n - to:\n   - ipBlock:\n       cidr: 192.168.0.0\u002F16\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp dir=\"ltr\"\u003EThis policy restricts the database pods to receive traffic only from webserver pods within the same cluster and allows them to communicate with any pod within the 192.168.0.0\u002F16 CIDR range.\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003E\u003Cstrong\u003EUsing NetworkPolicy API Groups: \u003C\u002Fstrong\u003ENetwork Policies support multiple API groups, offering additional functionalities.&nbsp; The networking.k8s.io\u002Fv1 API group is the most commonly used, while newer versions might offer extended features.\u003Cbr\u003E\u003Cbr\u003E\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003E\u003Cstrong\u003ENetworkPolicy Resources in Workloads: \u003C\u002Fstrong\u003ENetwork Policy resources can be embedded within deployments or pod specifications, allowing for tighter integration with specific workloads. This approach is generally discouraged as it reduces reusability and maintainability of Network Policies.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch3 dir=\"ltr\"\u003E&nbsp;\u003C\u002Fh3\u003E\n\u003Ch3 dir=\"ltr\"\u003E\u003Cstrong\u003EAdvanced Use Cases and Best Practices\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003EHaving explored advanced configurations, let's delve into some practical use cases for Network Policies in complex deployments:\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003E\u003Cem\u003EMicrosegmentation: \u003C\u002Fem\u003ENetwork Policies can be used to create micro segments within a cluster, isolating communication between specific services or groups of pods. This enhances security by limiting the attack surface and preventing lateral movement of threats within the cluster.\u003Cbr\u003E\u003Cbr\u003E\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003E\u003Cem\u003EDenying All Traffic by Default:\u003C\u002Fem\u003E&nbsp; A common best practice is to implement a \"deny-all-by-default\" approach using Network Policies. This policy initially blocks all traffic, and subsequent Network Policies explicitly allow necessary communication channels. This ensures a more secure baseline and minimizes the risk of unintended traffic flow.\u003Cbr\u003E\u003Cbr\u003E\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003E\u003Cem\u003ELimiting Egress Traffic: \u003C\u002Fem\u003ENetwork Policies can be used to restrict outbound traffic from pods to specific IP addresses or domains. This helps prevent data exfiltration and enforces communication with authorized external services only.\u003Cbr\u003E\u003Cbr\u003E\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch3 dir=\"ltr\"\u003EConclusion\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003ENetwork Policies offer a powerful mechanism for securing communication channels within Kubernetes clusters. By using advanced configurations and best practices, you can create robust security policies that isolate workloads, restrict unnecessary traffic, and enhance the overall security posture of your Kubernetes deployments. Remember to carefully design your Network Policies to balance security requirements with application functionality and avoid overly restrictive policies that hinder communication needed for your applications to function.\u003C\u002Fp\u003E\n\u003C\u002Fdiv\u003E\n\u003C\u002Fdiv\u003E\n\u003C\u002Fdiv\u003E\n\u003C\u002Fdiv\u003E\n\u003C\u002Fdiv\u003E\n\u003C\u002Fdiv\u003E\n\u003C\u002Fdiv\u003E\n\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E",tags:a,time_to_read:av,user_created:{id:f,first_name:g,last_name:h,email:k,password:b,location:a,title:l,description:a,tags:a,avatar:i,language:a,tfa_secret:a,status:c,role:j,token:a,last_access:m,last_page:n,provider:d,external_identifier:a,auth_data:a,email_notifications:e,appearance:a,theme_dark:a,theme_light:a,theme_light_overrides:a,theme_dark_overrides:a},user_updated:{id:f,first_name:g,last_name:h,email:k,password:b,location:a,title:l,description:a,tags:a,avatar:i,language:a,tfa_secret:a,status:c,role:j,token:a,last_access:m,last_page:n,provider:d,external_identifier:a,auth_data:a,email_notifications:e,appearance:a,theme_dark:a,theme_light:a,theme_light_overrides:a,theme_dark_overrides:a},image:{id:"b962570e-21d3-4298-b6c9-eea9aefd4ace",storage:p,filename_disk:"b962570e-21d3-4298-b6c9-eea9aefd4ace.png",filename_download:"ad_config-removebg-preview.png",title:"Ad Config Removebg Preview",type:r,folder:q,uploaded_by:f,created_on:aU,modified_by:a,modified_on:"2024-04-04T06:35:46.137Z",charset:a,filesize:"209951",width:I,height:I,duration:a,embed:a,description:a,location:a,tags:a,metadata:{},focal_point_x:a,focal_point_y:a,tus_id:a,tus_data:a,uploaded_on:aU},authors:[{id:ac,pe_blog_id:aa,directus_users_id:{first_name:g,last_name:h,avatar:i}},{id:ad,pe_blog_id:aa,directus_users_id:{first_name:x,last_name:y,avatar:z}}]},{id:_,status:o,sort:ae,date_created:"2024-02-20T13:26:34.921Z",date_updated:"2024-04-16T06:16:16.291Z",slug:"pros-and-cons-of-open-source-tools-in-idp",title:"Exploring the Pros and Cons of Leveraging Open Source Tools in Internal Developer Platforms (IDP)",description:"\u003Cp data-pm-slice=\"1 1 []\"\u003EOrganizations, in their pursuit of operational excellence, crave streamlined processes. Internal Developer Platforms (IDP) emerge as the powerhouse solution, offering a unified space for developers to seamlessly collaborate, deploy, and manage applications. Dive into this blog to uncover the strategic advantages and potential pitfalls of incorporating open-source tools.\u003C\u002Fp\u003E",seo_title:"Pros and Cons of Leveraging Open Source Tools in IDP",seo_description:"Explore the technical strengths & challenges of using open-source tools in your Internal Developer Platform (IDP). Analyze cost savings, customization, and potential security risks. ",content:"\u003Ch3 style=\"font-size: 22px;\"\u003E\u003Cspan class=\"heading-content\"\u003EBrief Introduction to Internal Developer Platforms (IDP)&nbsp;\u003C\u002Fspan\u003E\u003C\u002Fh3\u003E\n\u003Cp\u003ERevolutionize your software development! Organizations, in their pursuit of operational excellence, crave streamlined processes. Internal Developer Platforms (IDP) emerge as the powerhouse solution, offering a unified space for developers to seamlessly collaborate, deploy, and manage applications. Dive into this blog to uncover the strategic advantages and potential pitfalls of incorporating open-source tools.\u003C\u002Fp\u003E\n\u003Ch3 style=\"font-size: 22px;\"\u003E\u003Cspan class=\"heading-content\"\u003ESignificance of Integrating Open Source Tools into IDP for Modern Development Workflows\u003C\u002Fspan\u003E\u003C\u002Fh3\u003E\n\u003Cp\u003EOpen-source tools play a crucial role in the effectiveness of IDPs. By incorporating open-source solutions, organizations can harness the power of community-driven innovation, ensuring a dynamic and flexible development environment. This section will delve into the advantages and challenges associated with leveraging open-source tools within \u003Ca href=\"https:\u002F\u002Fplatformengineers.io\u002Fblog\u002Frole-of-golden-path-in-internal-developer-platforms\u002F\" target=\"_blank\" rel=\"noopener\"\u003EInternal Developer Platforms.\u003C\u002Fa\u003E\u003C\u002Fp\u003E\n\u003Ch2 style=\"font-size: 30px;\"\u003E\u003Cspan class=\"heading-content\"\u003EOpen Source Tools in Internal Developer Platforms\u003C\u002Fspan\u003E\u003C\u002Fh2\u003E\n\u003Ch3 style=\"font-size: 20px;\"\u003E\u003Cstrong\u003E\u003Cspan class=\"heading-content\"\u003EDefinition and Characteristics\u003C\u002Fspan\u003E\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Ch4 style=\"font-size: 16px;\"\u003E1. Overview of Popular Open Source Tools for IDP\u003C\u002Fh4\u003E\n\u003Cp\u003EIn IDPs, various open-source tools have gained prominence. \u003Cstrong\u003E\u003Cem\u003E\u003Ca href=\"https:\u002F\u002Fwww.terraform.io\u002F\" target=\"_blank\" rel=\"noopener\"\u003ETerraform\u003C\u002Fa\u003E, \u003Ca href=\"https:\u002F\u002Fgithub.com\u002F\" target=\"_blank\" rel=\"noopener\"\u003EGitHub\u003C\u002Fa\u003E, \u003Ca href=\"https:\u002F\u002Fabout.gitlab.com\u002F\" target=\"_blank\" rel=\"noopener\"\u003EGitHub Actions\u002FGitLab CI\u003C\u002Fa\u003E, \u003Ca href=\"https:\u002F\u002Ffluxcd.io\u002F\" target=\"_blank\" rel=\"noopener\"\u003EFluxCD\u003C\u002Fa\u003E, \u003Ca href=\"https:\u002F\u002Fprometheus.io\u002F\" target=\"_blank\" rel=\"noopener\"\u003EPrometheus\u003C\u002Fa\u003E, \u003Ca href=\"https:\u002F\u002Fgrafana.com\u002F\" target=\"_blank\" rel=\"noopener\"\u003EGrafana\u003C\u002Fa\u003E, \u003Ca href=\"https:\u002F\u002Fgrafana.com\u002Foss\u002Floki\u002F\" target=\"_blank\" rel=\"noopener\"\u003ELoki\u003C\u002Fa\u003E,\u003C\u002Fem\u003E\u003C\u002Fstrong\u003E and \u003Cstrong\u003E\u003Cem\u003E\u003Ca href=\"https:\u002F\u002Fkubernetes.io\u002F\" target=\"_blank\" rel=\"noopener\"\u003EKubernetes\u003C\u002Fa\u003E\u003C\u002Fem\u003E \u003C\u002Fstrong\u003Estand out as leading choices, each providing distinct features tailored to various facets of the development lifecycle. Renowned for their transparency and accessibility, these tools have become indispensable linchpins, shaping and optimizing the landscape of contemporary software development.\u003C\u002Fp\u003E\n\u003Ch4 style=\"font-size: 16px;\"\u003E2. Key Features and Adaptability\u003C\u002Fh4\u003E\n\u003Cp\u003EThe versatility of open-source tools lies in their adaptability to diverse development scenarios. From version control (Git) to continuous integration (GitLab CI) and container orchestration (\u003Ca href=\"https:\u002F\u002Fplatformengineers.io\u002Fservices\u002Fkubernetes-consulting-services\u002F\" target=\"_blank\" rel=\"noopener\"\u003EKubernetes\u003C\u002Fa\u003E), these tools seamlessly integrate into IDPs, providing a cohesive and efficient development ecosystem.\u003C\u002Fp\u003E\n\u003Ch3 style=\"font-size: 20px;\"\u003E\u003Cstrong\u003E\u003Cspan class=\"heading-content\"\u003EAdvantages\u003C\u002Fspan\u003E\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Ch4 style=\"font-size: 16px;\"\u003E1. Code Auditability for Security\u003C\u002Fh4\u003E\n\u003Cp\u003EOne of the paramount advantages of incorporating open-source tools is the enhanced code auditability they offer. With the source code being accessible to all, development teams can scrutinize and validate the security measures implemented, mitigating potential vulnerabilities and ensuring a robust and secure application.\u003C\u002Fp\u003E\n\u003Ch4 style=\"font-size: 16px;\"\u003E2. Enhanced Flexibility Through Potential Customization\u003C\u002Fh4\u003E\n\u003Cp\u003EOpen-source tools empower organizations with the freedom to customize and tailor solutions according to specific needs. This flexibility is particularly advantageous in IDPs, allowing teams to adapt tools to their unique workflows and requirements.\u003C\u002Fp\u003E\n\u003Ch4 style=\"font-size: 16px;\"\u003E3. Community-Driven Innovation and Continuous Improvement\u003C\u002Fh4\u003E\n\u003Cp\u003EThe collaborative nature of open-source communities fuels innovation. Leveraging open-source tools in IDPs means tapping into a vast pool of developers and experts contributing ideas, fixes, and improvements. This fosters continuous enhancement, ensuring that tools remain cutting-edge and aligned with industry best practices.\u003C\u002Fp\u003E\n\u003Ch4 style=\"font-size: 16px;\"\u003E4. Potentially Reduced Total Cost of Ownership\u003C\u002Fh4\u003E\n\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Fplatformengineers.io\u002Fassessment\u002F\" target=\"_blank\" rel=\"noopener\"\u003ECost-effectiveness\u003C\u002Fa\u003E is a significant benefit of open-source tools. Many of these tools are available free of charge, eliminating licensing fees. However, the real cost savings come from community support, which can help organizations troubleshoot issues and implement solutions without hefty consulting fees.\u003C\u002Fp\u003E\n\u003Ch3 style=\"font-size: 20px;\"\u003E\u003Cstrong\u003E\u003Cspan class=\"heading-content\"\u003EChallenges\u003C\u002Fspan\u003E\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Ch4 style=\"font-size: 16px;\"\u003E1. Potential Lack of Enterprise Support for Some Tools\u003C\u002Fh4\u003E\n\u003Cp\u003EWhile \u003Ca href=\"https:\u002F\u002Fplatformengineers.io\u002Fservices\u002Finfrastructure-maintenance-and-support\u002F\" target=\"_blank\" rel=\"noopener\"\u003Ecommunity support\u003C\u002Fa\u003E is robust for many open-source tools, some may lack comprehensive enterprise support. In mission-critical situations, organizations may find themselves grappling with limited assistance, necessitating in-house expertise or alternative solutions.\u003C\u002Fp\u003E\n\u003Ch4 style=\"font-size: 16px;\"\u003E2. Security Concerns and the Need for Thorough Vetting\u003C\u002Fh4\u003E\n\u003Cp\u003ESecurity remains a paramount concern when integrating open-source tools. The decentralized nature of development in open-source communities requires organizations to conduct thorough vetting processes to ensure that the tools meet stringent security standards and compliance requirements.\u003C\u002Fp\u003E\n\u003Ch2 style=\"font-size: 30px;\"\u003E\u003Cspan class=\"heading-content\"\u003EEvaluating the Fit for Your Organization\u003C\u002Fspan\u003E\u003C\u002Fh2\u003E\n\u003Ch3 style=\"font-size: 20px;\"\u003E\u003Cstrong\u003EAssessing Organizational Needs and Goals\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp\u003EBefore diving into open-source integration, organizations must meticulously assess their needs and goals. Understanding specific requirements and desired outcomes ensures that the selected open-source tools align with the organization's overarching objectives.\u003C\u002Fp\u003E\n\u003Ch3 style=\"font-size: 20px;\"\u003E\u003Cstrong\u003EConsidering Team Expertise and Community Support\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp\u003EThe success of open-source tool integration heavily relies on the expertise of the development team. Assessing the team's familiarity with chosen tools and the availability of robust community support is crucial for addressing challenges and optimizing tool usage.\u003C\u002Fp\u003E\n\u003Ch3 style=\"font-size: 20px;\"\u003E\u003Cstrong\u003EBalancing the Advantages and Disadvantages for Your Specific Context\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp\u003EAchieving a balanced approach is essential. While the advantages of open-source tools are evident, organizations must weigh them against potential disadvantages. Striking the right balance ensures a seamless integration that enhances development workflows without compromising on security or support.\u003C\u002Fp\u003E\n\u003Ch2 style=\"font-size: 30px;\"\u003E\u003Cspan class=\"heading-content\"\u003ECommunity Collaboration and Building a Culture of Contribution\u003C\u002Fspan\u003E\u003C\u002Fh2\u003E\n\u003Ch3 style=\"font-size: 20px;\"\u003E\u003Cstrong\u003EFostering Internal Collaboration Through Open Source Initiatives\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp\u003EEmbracing open-source tools opens the door to a world of collaboration. Organizations can foster internal collaboration by encouraging developers to share knowledge, collaborate on projects, and contribute to the wider open-source community.\u003C\u002Fp\u003E\n\u003Ch3 style=\"font-size: 20px;\"\u003E\u003Cstrong\u003EEncouraging Developers to Contribute Back to the Community\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp\u003EThe essence of open source lies in reciprocity. Encouraging developers to contribute back to the community not only strengthens the ecosystem but also nurtures a culture of giving and receiving. This reciprocal relationship can lead to mutual benefits and collective growth.\u003C\u002Fp\u003E\n\u003Ch3 style=\"font-size: 20px;\"\u003E\u003Cstrong\u003EEstablishing Best Practices for Responsible Open Source Use\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp\u003EWhile collaboration is vital, it should be tempered with responsibility. Organizations must establish best practices to govern the use of open-source tools, ensuring compliance, security, and ethical contributions. This involves creating guidelines, conducting training, and actively monitoring open-source usage.\u003C\u002Fp\u003E\n\u003Ch2 style=\"font-size: 30px;\"\u003E\u003Cspan class=\"heading-content\"\u003EFuture Trends and Innovations\u003C\u002Fspan\u003E\u003C\u002Fh2\u003E\n\u003Ch3 style=\"font-size: 20px;\"\u003E\u003Cstrong\u003EEmerging Trends in Open Source IDP Solutions\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp\u003EThe landscape of IDPs is dynamic, and open-source solutions continue to evolve. Exploring emerging trends such as AI-driven automation, enhanced developer experience, and deeper integration with cloud-native technologies provides insights into the future of open source in development platforms.\u003C\u002Fp\u003E\n\u003Ch3 style=\"font-size: 20px;\"\u003E\u003Cstrong\u003EThe Role of Open Source in Shaping the Future of Development Platforms\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp\u003EAs technology advances, the role of open source in shaping the future of development platforms becomes increasingly pivotal. Collaboration, innovation, and community-driven development are set to play an even more significant role, in influencing how organizations build, deploy, and manage applications.\u003C\u002Fp\u003E\n\u003Ch3\u003E\u003Cstrong\u003E\u003Cspan class=\"heading-content\"\u003EConclusion\u003C\u002Fspan\u003E\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp\u003EIn conclusion, the integration of open-source tools into Internal Developer \u003Ca href=\"https:\u002F\u002Fplatformengineers.io\u002Fservices\u002Fplatform-engineering\u002F\" target=\"_blank\" rel=\"noopener\"\u003EPlatforms\u003C\u002Fa\u003E brings forth a myriad of opportunities and challenges. Organizations must consider factors such as security, community support, and customization to make informed decisions that align with their development goals.\u003C\u002Fp\u003E\n\u003Cp\u003EFor organizations navigating the open source seas, the key lies in strategic decision-making. Embrace open source tools that align with organizational goals, invest in team training, and actively participate in the open source community. This approach ensures a symbiotic relationship that propels both internal development and the broader open-source ecosystem.\u003C\u002Fp\u003E",tags:["Internal Development Platform",aw,T,"Open-Source"],time_to_read:aV,user_created:{id:A,first_name:t,last_name:u,email:K,password:b,location:a,title:a,description:a,tags:a,avatar:v,language:B,tfa_secret:a,status:c,role:j,token:a,last_access:L,last_page:M,provider:d,external_identifier:a,auth_data:a,email_notifications:e,appearance:a,theme_dark:a,theme_light:a,theme_light_overrides:a,theme_dark_overrides:a},user_updated:{id:s,first_name:C,last_name:a,email:D,password:b,location:a,title:a,description:a,tags:a,avatar:E,language:a,tfa_secret:a,status:c,role:F,token:a,last_access:G,last_page:H,provider:d,external_identifier:a,auth_data:a,email_notifications:e,appearance:a,theme_dark:a,theme_light:a,theme_light_overrides:a,theme_dark_overrides:a},image:{id:"2b05b0ee-dcdf-4d76-b3d9-def3315d280d",storage:p,filename_disk:"2b05b0ee-dcdf-4d76-b3d9-def3315d280d.png",filename_download:"opensource pros and cons.png",title:"Opensource Pros and Cons",type:r,folder:q,uploaded_by:A,created_on:aW,modified_by:a,modified_on:"2024-02-20T15:36:24.233Z",charset:a,filesize:"217781",width:N,height:N,duration:a,embed:a,description:a,location:a,tags:a,metadata:{},focal_point_x:a,focal_point_y:a,tus_id:a,tus_data:a,uploaded_on:aW},authors:[{id:af,pe_blog_id:_,directus_users_id:{first_name:t,last_name:u,avatar:v}}]},{id:ab,status:o,sort:ag,date_created:"2024-01-29T12:26:07.989Z",date_updated:"2024-04-16T06:16:38.578Z",slug:"building-vs-buying-unveiling-the-advantages-and-pitfalls-of-customized-vs-hosted-internal-developer-platforms-idp",title:"Building vs. Buying : Unveiling the Advantages and Pitfalls of Customized vs. Hosted Internal Developer Platforms (IDP)",description:"\u003Cp dir=\"ltr\"\u003EIn the heart of modern software development lies a hidden battlefield: the struggle for developer efficiency and innovation. One of the key players in this pursuit is the Internal Developer Platform (IDP), a powerful tool that streamlines development workflows and enhances collaboration. However, organizations face a critical decision when choosing between a Customized Internal Developer Platform and a Hosted IDP Solution. We delve into the intricacies of both options, exploring their advantages and disadvantages to assist readers in making an informed decision tailored to their organization's needs.\u003C\u002Fp\u003E",seo_title:"Build vs Buy Your IDP? Strengths & Weaknesses",seo_description:"Build or Buy? Demystifying the advantages and pitfalls of customized vs. hosted IDPs. Find the perfect fit for your team's agility and security needs!",content:"\u003Cp\u003EIn the heart of modern software development lies a hidden battlefield: the struggle for developer efficiency and innovation. One of the key players in this pursuit is the Internal Developer Platform (IDP), a powerful tool that streamlines development workflows and enhances collaboration. However, organizations face a critical decision when choosing between a Customized Internal Developer Platform and a Hosted IDP Solution. We delve into the intricacies of both options, exploring their advantages and disadvantages to assist readers in making an informed decision tailored to their organization's needs.\u003C\u002Fp\u003E\n\u003Ch2 style=\"font-size: 21px;\"\u003E\u003Cspan class=\"heading-content\"\u003E\u003Cstrong\u003EI. Customized Internal Developer Platforms:\u003C\u002Fstrong\u003E\u003C\u002Fspan\u003E\u003C\u002Fh2\u003E\n\u003Ch3 style=\"font-size: 18px;\"\u003E\u003Cstrong\u003EA. Definition and Characteristics:\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp\u003EInternal Developer Platforms tailored to specific organizational needs have gained prominence as businesses seek to optimize their development processes. These platforms are designed in-house, with customization at the forefront. From streamlined \u003Ca href=\"https:\u002F\u002Fplatformengineers.io\u002Fservices\u002Fci-cd-implementation\u002F\" target=\"_blank\" rel=\"noopener\"\u003Edeployment pipelines\u003C\u002Fa\u003E to integrations with existing tools, the possibilities are vast.\u003C\u002Fp\u003E\n\u003Ch3 style=\"font-size: 18px;\"\u003E\u003Cstrong\u003EB. Advantages:\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp\u003E\u003Cstrong\u003ETailored to Specific Organizational Needs:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003ECustomized IDPs can be finely tuned to align with the unique requirements and workflows of an organization.\u003C\u002Fli\u003E\n\u003Cli\u003EThis ensures that the platform seamlessly integrates into existing processes, reducing friction and enhancing productivity.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp\u003E\u003Cstrong\u003EEnhanced Integration with Existing Systems:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003EIntegrating the IDP with other tools and systems becomes more straightforward, fostering a cohesive development environment.\u003C\u002Fli\u003E\n\u003Cli\u003EThis high level of integration leads to better data flow, collaboration, and overall efficiency.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp\u003E\u003Cstrong\u003EIncreased Flexibility and Control:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003ECustomization allows for flexibility in adapting to changing requirements, ensuring the platform remains relevant as the organization evolves.\u003C\u002Fli\u003E\n\u003Cli\u003EControl is achieved by tailoring the platform to have a finer granularity of oversight, allowing for precise adjustments and optimizations based on evolving needs and technical considerations.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch3 style=\"font-size: 18px;\"\u003E\u003Cstrong\u003EC. Challenges:\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp\u003E\u003Cstrong\u003EHigher Initial Development and Maintenance Costs: \u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003EBuilding a customized IDP requires significant upfront investment in terms of development resources, time, and costs.\u003C\u002Fli\u003E\n\u003Cli\u003EOngoing maintenance and updates contribute to the total cost of ownership, making it essential to weigh the benefits against the expenses.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp\u003E\u003Cstrong\u003ETime-Consuming Implementation Process: \u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003EDeveloping a bespoke IDP takes time, potentially delaying the benefits it brings to the development process.\u003C\u002Fli\u003E\n\u003Cli\u003EThe time investment may be a critical factor for organizations with pressing deadlines or those looking for rapid deployment.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp\u003E\u003Cstrong\u003EContinuous Adaptation to Evolving Requirements:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003EThe dynamic nature of the tech industry means that organizational requirements can evolve rapidly.\u003C\u002Fli\u003E\n\u003Cli\u003EKeeping a customized IDP aligned with these changes demands ongoing efforts and resources.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch2 style=\"font-size: 21px;\"\u003E\u003Cspan class=\"heading-content\"\u003E\u003Cstrong\u003EII. Hosted IDP Platforms:\u003C\u002Fstrong\u003E\u003C\u002Fspan\u003E\u003C\u002Fh2\u003E\n\u003Ch3 style=\"font-size: 18px;\"\u003E\u003Cstrong\u003EA. Definition and Features:\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp\u003EHosted IDP platforms, on the other hand, are third-party solutions that provide a ready-made environment for development teams. These platforms are designed to offer quick setup and a range of features out of the box.\u003C\u002Fp\u003E\n\u003Ch3 style=\"font-size: 18px;\"\u003E\u003Cstrong\u003EB. Advantages:\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp\u003E\u003Cstrong\u003ERapid Deployment and Setup:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003EHosted IDP solutions are designed for quick deployment, enabling development teams to get started swiftly.\u003C\u002Fli\u003E\n\u003Cli\u003EThe setup process is streamlined, reducing the time and resources required to implement the platform.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp\u003E\u003Cstrong\u003EReduced Upfront Costs:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003EUnlike customized IDPs, hosted solutions often come with lower initial costs as they eliminate the need for extensive in-house development efforts.\u003C\u002Fli\u003E\n\u003Cli\u003EThis can be particularly attractive for smaller organizations or those with budget constraints.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch3 style=\"font-size: 18px;\"\u003E\u003Cstrong\u003EC. Disadvantages:\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp\u003E\u003Cstrong\u003ELimited Customization Options:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003EHosted IDP platforms may lack the level of customization needed to align seamlessly with specific organizational workflows.\u003C\u002Fli\u003E\n\u003Cli\u003EThis limitation can hinder the platform's ability to adapt to unique development processes.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp\u003E\u003Cstrong\u003EPotential Integration Challenges: \u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003EIntegrating a hosted IDP with existing tools may present challenges due to predefined integrations and limited flexibility.\u003C\u002Fli\u003E\n\u003Cli\u003EThis could result in a less cohesive development environment and reduced efficiency.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp\u003E\u003Cstrong\u003EDependency on Third-Party Providers:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003ERelying on a hosted solution implies entrusting the third-party provider with the platform's availability, security, and performance. However, it is essential to acknowledge the inherent risk associated with these dependencies.\u003C\u002Fli\u003E\n\u003Cli\u003EThese third-party platforms may undergo changes or modifications at any time, necessitating a proactive approach on our part to adapt and align our systems accordingly.\u003C\u002Fli\u003E\n\u003Cli\u003EThus, ongoing evaluation and vigilant monitoring of these dependencies are crucial to promptly identify and address potential risks, ensuring a resilient and adaptable organizational infrastructure.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch2 style=\"font-size: 21px;\"\u003E\u003Cspan class=\"heading-content\"\u003E\u003Cstrong\u003EIII. Comparative Analysis:\u003C\u002Fstrong\u003E\u003C\u002Fspan\u003E\u003C\u002Fh2\u003E\n\u003Ch3 style=\"font-size: 18px;\"\u003E\u003Cstrong\u003EA. Performance and Efficiency:\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp\u003E\u003Cstrong\u003ESpeed of Deployment and Iteration:\u003C\u002Fstrong\u003E Customized IDPs may have a longer initial setup time, but they offer faster iteration cycles once deployed. Hosted solutions excel in quick deployment but may lag in adapting to evolving needs.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EResource Utilization and Optimization:\u003C\u002Fstrong\u003E Customized IDPs can be fine-tuned for optimal resource utilization, catering to specific performance requirements. Hosted solutions may prioritize general optimization, potentially resulting in suboptimal resource utilization for specific use cases.\u003C\u002Fp\u003E\n\u003Ch3 style=\"font-size: 18px;\"\u003E\u003Cstrong\u003EB. Cost Considerations:\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp\u003E\u003Cstrong\u003EInitial Setup Costs:\u003C\u002Fstrong\u003E Customized IDPs generally have higher upfront costs due to development efforts. Hosted solutions provide a cost-effective entry point but may have long-term subscription costs.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003ELong-Term Maintenance Expenses:\u003C\u002Fstrong\u003E Customized IDPs require ongoing \u003Ca href=\"https:\u002F\u002Fplatformengineers.io\u002Fservices\u002Finfrastructure-maintenance-and-support\u002F\" target=\"_blank\" rel=\"noopener\"\u003Emaintenance\u003C\u002Fa\u003E, which can contribute to long-term costs. Hosted solutions transfer maintenance responsibilities to the provider, but subscription fees may accumulate over time.\u003C\u002Fp\u003E\n\u003Ch3 style=\"font-size: 18px;\"\u003E\u003Cstrong\u003EC. Flexibility and Adaptability:\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp\u003E\u003Cstrong\u003EAccommodating Changes in Development Practices:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003ECustomized IDPs excel in adapting to evolving development practices and changing organizational needs.\u003C\u002Fli\u003E\n\u003Cli\u003EHosted solutions may struggle to accommodate unique workflows without extensive customization options.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp\u003E\u003Cstrong\u003EScaling with Organizational Growth:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003ECustomized IDPs can scale with the organization, evolving alongside growing requirements and user bases.\u003C\u002Fli\u003E\n\u003Cli\u003EHosted solutions may offer scalability features, but customization limitations can hinder seamless scaling.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch2 style=\"font-size: 21px;\"\u003E\u003Cspan class=\"heading-content\"\u003E\u003Cstrong\u003EIV. Best Practices for Choosing an IDP Solution:\u003C\u002Fstrong\u003E\u003C\u002Fspan\u003E\u003C\u002Fh2\u003E\n\u003Ch3 style=\"font-size: 18px;\"\u003E\u003Cstrong\u003EA. Evaluating Organizational Needs and Goals:\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cul\u003E\n\u003Cli\u003E\n\u003Cp\u003EConduct a thorough analysis of your organization's development workflows, goals, and specific requirements.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli\u003E\n\u003Cp\u003EConsider factors such as project complexity, team size, and scalability needs.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch3 style=\"font-size: 18px;\"\u003E\u003Cstrong\u003EB. Considering Development Team Feedback:\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cul\u003E\n\u003Cli\u003E\n\u003Cp\u003ESolicit feedback from the development team to understand their preferences and pain points.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli\u003E\n\u003Cp\u003EInvolving the end-users in the decision-making process ensures that the chosen IDP aligns with their workflow and enhances productivity.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch3 style=\"font-size: 18px;\"\u003E\u003Cstrong\u003EC. Balancing Customization and Simplicity:\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cul\u003E\n\u003Cli\u003E\n\u003Cp\u003EStriking the right balance between customization and simplicity is crucial. While a highly customized IDP may align perfectly with specific needs, it could introduce complexity.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli\u003E\n\u003Cp\u003EConsider the learning curve for the development team and the ease with which the platform can be adopted across the organization.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch3 style=\"font-size: 18px;\"\u003E\u003Cstrong\u003ED. Scalability and Future-Proofing:\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cul\u003E\n\u003Cli\u003E\n\u003Cp\u003EAssess the scalability features of the IDP solution and its ability to accommodate future growth.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli\u003E\n\u003Cp\u003EEnsure that the chosen platform is adaptable to emerging technologies and can seamlessly integrate with evolving development practices.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch3 style=\"font-size: 18px;\"\u003E\u003Cstrong\u003EE. Security and Compliance:\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cul\u003E\n\u003Cli\u003E\n\u003Cp\u003EPrioritize security features to safeguard sensitive data and ensure compliance with industry regulations.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli\u003E\n\u003Cp\u003EEvaluate the hosting provider's security measures for hosted solutions and implement additional measures for customized platforms.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch2 style=\"font-size: 21px;\"\u003E\u003Cspan class=\"heading-content\"\u003E\u003Cstrong\u003EV. Conclusion:\u003C\u002Fstrong\u003E \u003C\u002Fspan\u003E\u003C\u002Fh2\u003E\n\u003Cp\u003EAs organizations navigate the complex landscape of Internal Developer Platforms, the decision between a Customized IDP and a Hosted Solution carries significant implications. Customized platforms offer tailored solutions that align closely with organizational needs, providing flexibility and scalability but come with higher development and maintenance costs. Hosted solutions, while cost-effective and quick to deploy, may lack the level of customization needed for unique workflows.\u003C\u002Fp\u003E\n\u003Cp\u003EIn the end, the choice between these two options depends on the specific context of each organization. Best practices involve a careful consideration of organizational needs, feedback from the development team, and a balance between customization and simplicity. Scalability, security, community support, pilot testing, and a thorough cost-benefit analysis should all play a role in the decision-making process.\u003C\u002Fp\u003E",tags:["Internal Development Portal",T,"Cloud Infrastructure"],time_to_read:av,user_created:{id:A,first_name:t,last_name:u,email:K,password:b,location:a,title:a,description:a,tags:a,avatar:v,language:B,tfa_secret:a,status:c,role:j,token:a,last_access:L,last_page:M,provider:d,external_identifier:a,auth_data:a,email_notifications:e,appearance:a,theme_dark:a,theme_light:a,theme_light_overrides:a,theme_dark_overrides:a},user_updated:{id:s,first_name:C,last_name:a,email:D,password:b,location:a,title:a,description:a,tags:a,avatar:E,language:a,tfa_secret:a,status:c,role:F,token:a,last_access:G,last_page:H,provider:d,external_identifier:a,auth_data:a,email_notifications:e,appearance:a,theme_dark:a,theme_light:a,theme_light_overrides:a,theme_dark_overrides:a},image:{id:"a8653dea-0248-4787-8d52-063c8a6d850f",storage:p,filename_disk:"a8653dea-0248-4787-8d52-063c8a6d850f.webp",filename_download:"Customized vs hosted IDP blog (1).webp",title:"Customized Vs Hosted Idp Blog (1)",type:W,folder:q,uploaded_by:s,created_on:aX,modified_by:a,modified_on:"2024-02-09T10:32:41.748Z",charset:a,filesize:"90590",width:aY,height:aY,duration:a,embed:a,description:a,location:a,tags:a,metadata:{},focal_point_x:a,focal_point_y:a,tus_id:a,tus_data:a,uploaded_on:aX},authors:[{id:ag,pe_blog_id:ab,directus_users_id:{first_name:t,last_name:u,avatar:v}}]},{id:ae,status:o,sort:aZ,date_created:"2024-01-15T09:15:56.371Z",date_updated:"2024-04-16T06:38:04.539Z",slug:"product-method-for-building-platforms",title:"Employing the Product Method for Building Platforms",description:"\u003Cp\u003ELeveraging the \u003Cstrong\u003Eproduct platform architecture\u003C\u002Fstrong\u003E to \u003Cstrong\u003Estreamline platform development\u003C\u002Fstrong\u003E through \u003Cstrong\u003Emodularization, standardization, and shared components\u003C\u002Fstrong\u003E.\u003C\u002Fp\u003E",seo_title:"Demystify platform development with the Product Method",seo_description:"Reduce time-to-market and development costs while optimizing platform flexibility via the product method. ",content:"\u003Ch2 dir=\"ltr\" style=\"font-size: 22px;\"\u003E\u003Cstrong\u003EEmploying the Product Method for Building Platforms\u003C\u002Fstrong\u003E\u003C\u002Fh2\u003E\n\u003Cp dir=\"ltr\"\u003EBuilding platforms has become a cornerstone of technological advancement. These platforms serve as the bedrock for innovation, connectivity, and the delivery of services. In this context, the approach taken in developing these platforms holds immense significance. One approach that stands out in its efficacy and adaptability is the Product Method.&nbsp;\u003C\u002Fp\u003E\n\u003Ch3 style=\"font-size: 18px;\"\u003E\u003Cstrong\u003EOverview of Building Platforms\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003EBefore delving into the intricacies of the Product Method, it's crucial to understand the broader landscape of \u003Ca href=\"https:\u002F\u002Fplatformengineers.io\u002Fservices\u002Fplatform-engineering\u002F\" target=\"_blank\" rel=\"noopener\"\u003E\u003Cspan style=\"text-decoration: underline;\"\u003Ebuilding platforms\u003C\u002Fspan\u003E\u003C\u002Fa\u003E. Platforms, in essence, act as enablers, providing a framework for various services, applications, or ecosystems to coexist and interact. They are pivotal in shaping the digital experiences of developers and businesses alike.\u003C\u002Fp\u003E\n\u003Ch2 dir=\"ltr\" style=\"font-size: 20px;\"\u003E\u003Cstrong\u003EUnderstanding the Product Approach\u003C\u002Fstrong\u003E\u003C\u002Fh2\u003E\n\u003Cp dir=\"ltr\"\u003EThe product approach represents a distinct philosophical shift in various domains, ranging from marketing and management to education and writing. It advocates for prioritizing the final product or service as the focal point of strategy and decision-making, rather than focusing on individual components or processes.&nbsp;\u003C\u002Fp\u003E\n\u003Ch3 style=\"font-size: 18px;\"\u003E\u003Cstrong\u003EKey Characteristics of the Product Approach\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EUser-Centricity\u003C\u002Fstrong\u003E\u003Cbr\u003E\u003Cbr\u003EThe product approach elevates the users to the center of the equation, in this case the users are developers. Every aspect of the development, marketing, and delivery process revolves around understanding and catering to developer needs, frustrations, and aspirations. This entails extensive developer research, iterative development cycles, and constant feedback loops to ensure the product remains relevant and impactful.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EData-Driven Optimization\u003C\u002Fstrong\u003E\u003Cbr\u003E\u003Cbr\u003EThe product approach leverages data and analytics extensively to inform decision-making and iteratively optimize the product. Using Developer behavior data as user data, market trends, and performance metrics are constantly monitored and analyzed to refine the product, adjust features, and prioritize development efforts.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EContinuous Improvement\u003C\u002Fstrong\u003E\u003Cbr\u003E\u003Cbr\u003EEmbracing the product approach signifies a commitment to ongoing evolution and improvement. The product is never considered \"finished\" but rather undergoes continual refinement based on developer feedback, emerging technologies, and changing market dynamics.\u003C\u002Fp\u003E\n\u003Ch3 style=\"font-size: 18px;\"\u003E\u003Cstrong\u003EDifferentiating from Traditional Methods\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003ETraditionally, many strategies focused on internal factors like production optimization, \u003Ca href=\"https:\u002F\u002Fplatformengineers.io\u002Fassessment\" target=\"_blank\" rel=\"noopener\"\u003Ecost reduction\u003C\u002Fa\u003E, or feature-driven development. This often led to products that lacked market fit and failed to cater to developer needs effectively. The product approach offers a refreshing counterpoint by placing the developer at the heart of the process, leading to more relevant, successful, and ultimately valuable offerings.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EBy prioritizing developer needs, integrating developer feedback, and optimizing for holistic value creation, the product approach can empower businesses and organizations to deliver truly impactful products and services that stand out in today's competitive landscape.\u003C\u002Fp\u003E\n\u003Ch2 dir=\"ltr\" style=\"font-size: 20px;\"\u003E\u003Cstrong\u003EAdvantages of Adopting the Product Approach\u003C\u002Fstrong\u003E\u003C\u002Fh2\u003E\n\u003Cp dir=\"ltr\"\u003EA product-centric approach has gained significant traction in recent years, offering numerous benefits for organizations across various industries. This approach prioritizes delivering tangible products that solve developer needs over internal processes or abstract deliverables. By shifting towards this mindset, organizations can unlock several key advantages:\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\" style=\"font-size: 18px;\"\u003E\u003Cstrong\u003EFaster Development Cycles\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cul\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003E\u003Cstrong\u003EAgile Methodologies: \u003C\u002Fstrong\u003EThe product approach often embraces agile methodologies like Scrum or Kanban. These iterative and incremental approaches break down development into smaller, manageable chunks (sprints or features). This allows for faster feedback loops, enabling teams to quickly adapt to changing needs and deliver value sooner.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003E\u003Cstrong\u003EPrioritization and Focus: \u003C\u002Fstrong\u003EBy concentrating on specific product goals and developer stories, teams can prioritize tasks and eliminate inefficiencies. This laser focus keeps developers on track, minimizing context switching and reducing unnecessary work, ultimately leading to faster development cycles.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch3 dir=\"ltr\" style=\"font-size: 18px;\"\u003E\u003Cstrong\u003EEnhanced Development Experience\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cul\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003E\u003Cstrong\u003EImproved Ownership and Autonomy:\u003C\u002Fstrong\u003E The product approach empowers development teams with greater ownership of their work. Teams are responsible for the entire product lifecycle, from conception to delivery, fostering a sense of accountability and pride. This ownership motivates teams to take initiative and experiment, leading to a more engaging and fulfilling development experience.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003E\u003Cstrong\u003ECross-functional Collaboration:\u003C\u002Fstrong\u003E Product-centric teams require close collaboration between various departments, including developers, designers, product managers, and marketing personnel. This collaborative environment fosters knowledge sharing, breaks down silos, and leads to a more holistic understanding of developer needs. The synergy created by diverse perspectives enhances the development experience and results in better products.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch2 dir=\"ltr\" style=\"font-size: 20px;\"\u003E\u003Cstrong\u003EKey Components of the Product Approach\u003C\u002Fstrong\u003E\u003C\u002Fh2\u003E\n\u003Cp dir=\"ltr\"\u003EDeveloping successful products in today's dynamic landscapes requires a robust and nuanced approach. This paper delves into three critical components that form the bedrock of a powerful product strategy: developer-centric design, iterative development, and continuous feedback loops.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\" style=\"font-size: 18px;\"\u003E\u003Cstrong\u003EDeveloper-Centric Design\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cul\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003E\u003Cstrong\u003EAPI-First Mindset: \u003C\u002Fstrong\u003EEmbracing an API-first approach prioritizes building intuitive and well-documented APIs as the foundation for the product. This fosters faster and smoother \u003Cspan style=\"text-decoration: underline;\"\u003E\u003Ca href=\"https:\u002F\u002Fplatformengineers.io\u002Fblog\u002Frole-of-golden-path-in-internal-developer-platforms\" target=\"_blank\" rel=\"noopener\"\u003Eintegration for developers\u003C\u002Fa\u003E,\u003C\u002Fspan\u003E enabling wider adoption and accelerated innovation.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003E\u003Cstrong\u003EDeveloper Empathy: \u003C\u002Fstrong\u003EUnderstanding the needs, challenges, and preferences of developers is key. Tools like developer portals, comprehensive SDKs, and active community engagement contribute to a positive developer experience.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003E\u003Cstrong\u003EDesign for Maintainability:\u003C\u002Fstrong\u003E Prioritizing modularity, code clarity, and robust testing frameworks ensures \u003Cspan style=\"text-decoration: underline;\"\u003E\u003Ca href=\"https:\u002F\u002Fplatformengineers.io\u002Fservices\u002Finfrastructure-maintenance-and-support\" target=\"_blank\" rel=\"noopener\"\u003Elong-term maintainability\u003C\u002Fa\u003E\u003C\u002Fspan\u003E and facilitates future development cycles.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch3 dir=\"ltr\" style=\"font-size: 18px;\"\u003E\u003Cstrong\u003EIterative Development\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cul\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003E\u003Cstrong\u003EMinimum Viable Products (MVPs): \u003C\u002Fstrong\u003EFocusing on launching core functionalities through MVPs enables early developer testing and validation, minimizing resource consumption while maximizing learning.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003E\u003Cstrong\u003EContinuous Integration and \u003Cspan style=\"text-decoration: underline;\"\u003E\u003Ca href=\"https:\u002F\u002Fplatformengineers.io\u002Fservices\u002Fci-cd-implementation\" target=\"_blank\" rel=\"noopener\"\u003EContinuous Delivery (CI\u002FCD)\u003C\u002Fa\u003E\u003C\u002Fspan\u003E:\u003C\u002Fstrong\u003E Automating build, testing, and deployment processes minimizes friction and fosters rapid iteration cycles.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch3 dir=\"ltr\" style=\"font-size: 18px;\"\u003E\u003Cstrong\u003EContinuous Feedback Loops\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cul\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003E\u003Cstrong\u003ETelemetric Data and Analytics: \u003C\u002Fstrong\u003ELeveraging robust data gathering and analysis tools provides valuable insights into developer behavior and product performance. This informs data-driven decision-making and prioritizes features that deliver tangible value.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\" aria-level=\"1\"\u003E\n\u003Cp dir=\"ltr\" role=\"presentation\"\u003E\u003Cstrong\u003EOpen Communication and Collaboration:\u003C\u002Fstrong\u003E Maintaining open communication channels with both internal and external stakeholders fosters active feedback loops and promotes continuous product improvement.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp dir=\"ltr\"\u003EBy integrating these three components, organizations can establish a product approach that fosters agility, adaptability, and developer-centricity. Developer-centric design attracts and empowers talent, iterative development ensures continuous progress and learning, and continuous feedback loops guarantee ongoing product refinement and evolution.&nbsp;\u003C\u002Fp\u003E\n\u003Ch2 dir=\"ltr\" style=\"font-size: 20px;\"\u003E\u003Cstrong\u003EBuilding a Platform with a Product Mindset\u003C\u002Fstrong\u003E\u003C\u002Fh2\u003E\n\u003Cp dir=\"ltr\"\u003EPlatforms are the engines driving innovation and collaboration. But simply constructing a technological framework isn't enough. To thrive, platforms must be built with a product mindset - an approach that prioritizes developer needs, embraces continuous improvement, and fosters a vibrant ecosystem.&nbsp;\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\" style=\"font-size: 18px;\"\u003E\u003Cstrong\u003EDesigning for Usability and Adoption\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EFrictionless On-boarding\u003C\u002Fstrong\u003E\u003Cbr\u003EStreamline the initial developer journey. Offer clear documentation, tutorials, and helpful resources to equip developers with the knowledge and confidence to leverage the platform's full potential.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003ECustomization and Personalization\u003C\u002Fstrong\u003E\u003Cbr\u003E\u003Ca href=\"https:\u002F\u002Fplatformengineers.io\u002Fblog\u002Finternal-developer-platform\" target=\"_blank\" rel=\"noopener\"\u003EEmpower developers\u003C\u002Fa\u003E to tailor the platform experience to their preferences and working styles. Consider offering configurable dashboards, customizable workflows, and integration with external tools.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\" style=\"font-size: 18px;\"\u003E\u003Cstrong\u003EBuilding for Continuous Improvement\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EData-Driven Decision Making\u003C\u002Fstrong\u003E\u003Cbr\u003ETrack platform usage metrics and gather developer feedback through surveys, interviews, and A\u002FB testing. Analyze these data points to identify pain points, \u003Ca href=\"https:\u002F\u002Fplatformengineers.io\u002Fservices\u002Fperformance-optimization\" target=\"_blank\" rel=\"noopener\"\u003Eopportunities for optimization\u003C\u002Fa\u003E, and potential new features.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EExperimentation and Innovation\u003C\u002Fstrong\u003E\u003Cbr\u003ECultivate a culture of experimentation and encourage trying new ideas. Prototype features, gather feedback, and iterate quickly to discover what resonates with developers and drives platform value.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\" style=\"font-size: 18px;\"\u003E\u003Cstrong\u003ECultivating a Platform Ecosystem\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EOpen SDKs: \u003C\u002Fstrong\u003EProvide developers with well-documented APIs and Software Development Kits (SDKs) to facilitate third-party integrations and extensions. This empowers developers to extend the platform's functionality and tailor it to their specific needs.\u003Cbr\u003E\u003Cbr\u003E\u003Cstrong\u003ECommunity Building: \u003C\u002Fstrong\u003EFoster an active and engaged community around the platform. Offer forums, meetups, and other interactive spaces for developers to share best practices, troubleshoot issues, and collaborate on projects.\u003Cbr\u003E\u003Cbr\u003E\u003Cstrong\u003EPartnering with Strategic Players: \u003C\u002Fstrong\u003ECollaborate with complementary businesses and technologies to broaden the platform's reach and value proposition. Strategic partnerships can unlock new use cases, attract new developers, and solidify the platform's position within the ecosystem.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\" style=\"font-size: 18px;\"\u003E\u003Cstrong\u003EConclusion\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003EBy embracing these principles, you can build a platform that not only functions but thrives. You'll create a tool that developers love to use, constantly evolves to meet their needs, and attracts a vibrant community that fuels its continued success. Remember, in the world of platforms, it's not enough to just build - it's about cultivating a thriving ecosystem that delivers lasting value to its developers.\u003C\u002Fp\u003E",tags:[T,"product method"],time_to_read:"6 min",user_created:{id:f,first_name:g,last_name:h,email:k,password:b,location:a,title:l,description:a,tags:a,avatar:i,language:a,tfa_secret:a,status:c,role:j,token:a,last_access:m,last_page:n,provider:d,external_identifier:a,auth_data:a,email_notifications:e,appearance:a,theme_dark:a,theme_light:a,theme_light_overrides:a,theme_dark_overrides:a},user_updated:{id:s,first_name:C,last_name:a,email:D,password:b,location:a,title:a,description:a,tags:a,avatar:E,language:a,tfa_secret:a,status:c,role:F,token:a,last_access:G,last_page:H,provider:d,external_identifier:a,auth_data:a,email_notifications:e,appearance:a,theme_dark:a,theme_light:a,theme_light_overrides:a,theme_dark_overrides:a},image:{id:"515ffe7a-5658-4bbd-a49d-459366019c72",storage:p,filename_disk:"515ffe7a-5658-4bbd-a49d-459366019c72.png",filename_download:"Untitled design.png",title:"Untitled Design",type:r,folder:q,uploaded_by:s,created_on:a_,modified_by:a,modified_on:"2024-02-14T10:13:56.750Z",charset:a,filesize:"166555",width:a$,height:a$,duration:a,embed:a,description:a,location:a,tags:a,metadata:{},focal_point_x:a,focal_point_y:a,tus_id:a,tus_data:a,uploaded_on:a_},authors:[{id:Q,pe_blog_id:ae,directus_users_id:{first_name:g,last_name:h,avatar:i}}]},{id:ax,status:o,sort:af,date_created:"2024-01-05T14:38:40.412Z",date_updated:"2024-04-16T06:37:34.657Z",slug:"strategies-for-proactively-maintaining-resilient-infrastructure",title:"Strategies for Proactively Maintaining Resilient Infrastructure",description:"\u003Cp\u003EAt the core of a robust and resilient infrastructure lies an often-overlooked yet fundamental principle: the power of a well-crafted design. In our philosophy, preventing avoidable challenges takes precedence over solving problems post-occurrence.\u003C\u002Fp\u003E",seo_title:"Strategies for Maintaining Resilient Infrastructure",seo_description:"At the core of a robust and resilient infrastructure lies an often-overlooked yet fundamental principle: the power of a well-crafted design. ",content:"\u003Cp dir=\"ltr\"\u003EAt the core of a robust and resilient infrastructure lies an often-overlooked yet fundamental principle: the power of a well-crafted design. In our philosophy, preventing avoidable challenges takes precedence over solving problems post-occurrence.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EYet, in the realm of \u003Ca href=\"https:\u002F\u002Fplatformengineers.io\u002Fservices\u002Fplatform-engineering\" target=\"_blank\" rel=\"noopener\"\u003Eplatform engineering\u003C\u002Fa\u003E, ensuring the robustness and continuity of systems in the face of disasters is a multifaceted challenge. Disruptions caused by unforeseen circumstances necessitate advanced disaster recovery strategies to minimize downtime and data loss.\u003C\u002Fp\u003E\n\u003Ch2 dir=\"ltr\" style=\"font-size: 20px;\"\u003E\u003Cstrong\u003EUnderstanding Disaster Recovery in Platform Engineering:\u003C\u002Fstrong\u003E\u003C\u002Fh2\u003E\n\u003Cp dir=\"ltr\"\u003EDisaster recovery encompasses a comprehensive approach that integrates a variety of technical tools, methodologies, and proactive measures. It aims to expedite the recovery of systems swiftly and effectively in the event of disruptions or failures.\u003C\u002Fp\u003E\n\u003Ch2 dir=\"ltr\" style=\"font-size: 20px;\"\u003E\u003Cstrong\u003ESome of the Potential Building Blocks of Resilient Infrastructure:\u003C\u002Fstrong\u003E\u003C\u002Fh2\u003E\n\u003Ch3 dir=\"ltr\" style=\"font-size: 18px;\"\u003E\u003Cstrong\u003EEliminate single points of failure:&nbsp;\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003EEliminating or at least reducing single points of failure is a fundamental goal in constructing a \u003Ca href=\"https:\u002F\u002Fplatformengineers.io\u002Fassessment\" target=\"_blank\" rel=\"noopener\"\u003Eresilient infrastructure\u003C\u002Fa\u003E. We meticulously identify and address vulnerable components that, if disrupted, could compromise the entire system. Strategies encompass the implementation of redundant hardware, software, and networking setups like load balancers, clustering techniques, and failover mechanisms. By distributing workloads and minimizing dependency on any one element, these proactive measures ensure system stability even if a component fails, safeguarding the continuous functionality of the infrastructure.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\" style=\"font-size: 18px;\"\u003E\u003Cstrong\u003EStateless Architecture Design:&nbsp;\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003EEmbracing stateless architecture design allows systems to function without storing client session state information. This design principle facilitates easier scalability, fault tolerance, and resilience as it enables any instance to handle requests, thereby reducing dependencies on specific server instances and minimizing the impact of potential failures.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\" style=\"font-size: 18px;\"\u003E\u003Cstrong\u003ERedundancy Across Availability Zones and Data:\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003ERedundancy across availability zones and data is fundamental in crafting a resilient infrastructure. We strategically plan resources across multiple zones or data centers, utilizing Infrastructure as Code (IaC) to replicate components in case of regional outages or hardware failures. Implementing active-active and active-passive redundancy strategies ensures continuous operations during disruptions, distributing traffic across multiple locations and providing failover capabilities. These meticulous strategies significantly enhance system reliability, minimize downtime, and maintain continuous operations, ultimately fortifying the infrastructure's robustness and resilience.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\" style=\"font-size: 18px;\"\u003E\u003Cstrong\u003EContinuous Data Protection:&nbsp;\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003EContinuous data protection forms a crucial aspect of resilient infrastructure, encompassing regular backups and real-time data integrity verification. This proactive approach minimizes data loss and reduces recovery time objectives, ensuring detailed recovery points. By upholding data integrity, this strategy significantly enhances the infrastructure's resilience against disruptions and fortifies its ability to maintain \u003Ca href=\"https:\u002F\u002Fplatformengineers.io\u002Fservices\u002Fci-cd-implementation\" target=\"_blank\" rel=\"noopener\"\u003Econtinuous operations.&nbsp;\u003C\u002Fa\u003E\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\" style=\"font-size: 18px;\"\u003E\u003Cstrong\u003EImmutable Infrastructure:&nbsp;\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003EImmutable infrastructure serves as a fundamental pillar of resilient systems and helps in avoiding a class of problems involving the deployment of unchanging systems post-deployment. This approach streamlines rollbacks, maintains consistency, and enhances security by preventing configuration drift. By simplifying updates and ensuring uniformity, this strategy fortifies the infrastructure's resilience against potential configuration inconsistencies.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\" style=\"font-size: 18px;\"\u003E\u003Cstrong\u003EAutomated Orchestration and Configuration Management:&nbsp;\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003EWe heavily use automated orchestration and configuration management, coupled with Infrastructure as Code (IaC) for building resilient infrastructure. These practices involve automated procedures for efficiently managing and deploying infrastructure components through code. This automation ensures consistency, minimizes errors, and enhances system adaptability and resilience by streamlining operations, reducing manual intervention, and allowing for rapid infrastructure reprovisioning after failures.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\" style=\"font-size: 18px;\"\u003E\u003Cstrong\u003EChaos Engineering:&nbsp;&nbsp;\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003EChaos engineering intentionally induces failures in systems to preemptively identify weaknesses, fortify resilience, and enhance infrastructure reliability. It's crucial for proactively strengthening systems, uncovering vulnerabilities before they escalate, and fortifying fault tolerance and overall resilience. This tool assesses your system's resilience but does not directly contribute to enhancing the system's overall resilience.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\" style=\"font-size: 18px;\"\u003E\u003Cstrong\u003ETechnical Strategies for Effective Disaster Recovery:&nbsp;\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003EDisaster recovery strategies form a critical backbone of resilient infrastructure. Technical strategies for effective disaster recovery encompass comprehensive plans to swiftly recover systems and data in the event of disruptions. These strategies emphasize \u003Ca href=\"https:\u002F\u002Fplatformengineers.io\u002Fservices\u002Fkubernetes-consulting-services\" target=\"_blank\" rel=\"noopener\"\u003Ebackup restoration\u003C\u002Fa\u003E, failover processes, and system reconfiguration, enabling rapid recovery and minimizing downtime. We proactively build resilient systems by incorporating automated orchestration, configuration management, Infrastructure as Code (IaC), and microservices architecture. Additionally, we conduct regular and adaptive disaster recovery testing, incorporating various scenarios and failure simulations to identify weaknesses in the infrastructure continually. This continual improvement approach ensures that disaster recovery plans remain effective and updated, enhancing the infrastructure's readiness to handle unforeseen disruptions while ensuring minimal impact on operations.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\" style=\"font-size: 18px;\"\u003E\u003Cstrong\u003EMicroservices Architecture:&nbsp;\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003EMicroservices architecture plays a critical role for us in building resilient infrastructure by breaking down applications into smaller, independent services. This approach enhances fault isolation, scalability, and overall system resilience by allowing individual service operation, minimizing the impact of failures, and enabling swift updates and deployments.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\" style=\"font-size: 18px;\"\u003E\u003Cstrong\u003EDistributed Logging and Monitoring:&nbsp;\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003EDistributed logging and monitoring form integral components of resilient infrastructure, gathering data from diverse sources to monitor system health and performance. Distributing these systems across multiple sources fortifies resilience against potential monitoring system failures, ensuring comprehensive oversight and \u003Ca href=\"https:\u002F\u002Fplatformengineers.io\u002Fservices\u002Finfrastructure-maintenance-and-support\" target=\"_blank\" rel=\"noopener\"\u003Eproactive management\u003C\u002Fa\u003E for optimal system functionality.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\" style=\"font-size: 18px;\"\u003E\u003Cstrong\u003EConclusion:\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003EIn conclusion, the construction of a robust and resilient infrastructure is not merely a one-time achievement but an ongoing commitment to fortify systems against potential disruptions.&nbsp;\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EWe employ a diverse toolkit of proactive measures and innovative strategies, placing significant emphasis on meticulous design, disaster recovery planning, and the adoption of cutting-edge technologies.&nbsp;\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EBy prioritizing prevention over reaction, these approaches ensure system stability, reduce downtime and bolster overall resilience. The amalgamation of microservices architecture, distributed monitoring, active redundancy strategies, and continual adaptive testing serves as a testament to the multifaceted approach embraced by engineers.&nbsp;\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EThrough these collective efforts, the infrastructure is not just safeguarded against challenges but poised to adapt, evolve, and thrive even in the face of unforeseen circumstances, reinforcing its resilience and readiness for the future\u003C\u002Fp\u003E",tags:["Resilent Cloud Infrastructure",T],time_to_read:av,user_created:{id:ba,first_name:ah,last_name:ai,email:bb,password:b,location:a,title:a,description:a,tags:a,avatar:aj,language:B,tfa_secret:a,status:c,role:j,token:a,last_access:bc,last_page:bd,provider:d,external_identifier:a,auth_data:a,email_notifications:e,appearance:a,theme_dark:a,theme_light:a,theme_light_overrides:a,theme_dark_overrides:a},user_updated:{id:s,first_name:C,last_name:a,email:D,password:b,location:a,title:a,description:a,tags:a,avatar:E,language:a,tfa_secret:a,status:c,role:F,token:a,last_access:G,last_page:H,provider:d,external_identifier:a,auth_data:a,email_notifications:e,appearance:a,theme_dark:a,theme_light:a,theme_light_overrides:a,theme_dark_overrides:a},image:{id:"23cd1138-a9ab-4007-beac-00f0330d2d08",storage:p,filename_disk:"23cd1138-a9ab-4007-beac-00f0330d2d08.webp",filename_download:"Team_analyzes_cloud_server_data (2).webp",title:"Team Analyzes Cloud Server Data (2)",type:W,folder:q,uploaded_by:s,created_on:be,modified_by:a,modified_on:"2024-02-09T11:57:18.992Z",charset:a,filesize:"47178",width:1875,height:1562,duration:a,embed:a,description:a,location:a,tags:a,metadata:{},focal_point_x:a,focal_point_y:a,tus_id:a,tus_data:a,uploaded_on:be},authors:[{id:R,pe_blog_id:ax,directus_users_id:{first_name:ah,last_name:ai,avatar:aj}}]},{id:ag,status:o,sort:8,date_created:"2024-01-03T12:22:58.022Z",date_updated:"2024-04-16T06:37:14.586Z",slug:"what-is-platform-engineering",title:"What is Platform Engineering?",description:"\u003Cp dir=\"ltr\"\u003EPlatform engineering is the practice of designing, building, and maintaining internal developer platforms (IDPs). These platforms act as self-service toolsets and workflows, empowering developers to build, deploy, and manage applications faster and more efficiently.\u003C\u002Fp\u003E",seo_title:"What is platform engineering ? ",seo_description:"Platform engineering is the practice of designing, building, and maintaining internal developer platforms (IDPs).",content:"\u003Ch2 dir=\"ltr\" style=\"font-size: 22px;\"\u003E\u003Cstrong\u003EWhat is Platform Engineering?\u003C\u002Fstrong\u003E\u003Cbr\u003E\u003Cbr\u003E\u003Cimg src=\"https:\u002F\u002Fdata.improwised.com\u002Fassets\u002F6c976dac-c9d5-4ad7-8c47-e2c44a67151e?width=2240&amp;height=1260\" alt=\"Platform Engineering\"\u003E\u003C\u002Fh2\u003E\n\u003Cp dir=\"ltr\"\u003EPlatform engineering is the practice of designing, building, and maintaining internal developer platforms (IDPs). These platforms act as self-service toolsets and workflows, empowering developers to build, deploy, and manage applications faster and more efficiently.\u003C\u002Fp\u003E\n\u003Ch2 dir=\"ltr\" style=\"font-size: 20px;\"\u003E\u003Cstrong\u003EEvolution of a Discipline:\u003C\u002Fstrong\u003E\u003C\u002Fh2\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cbr\u003EPlatform engineering arose from the need to overcome the limitations of traditional software development, characterized by slow cycles, siloed teams, and cumbersome infrastructure. The rise of cloud computing, microservices architectures, and DevOps principles paved the way for a more agile and automated approach.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\" style=\"font-size: 18px;\"\u003E\u003Cstrong\u003ECore Components:\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003EInfrastructure Management: Platform engineers oversee the underlying infrastructure, ensuring scalability, reliability, and security. This includes managing servers, storage, networks, and cloud resources.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EApplication Development: They design and implement tools and workflows that streamline the development process. This includes CI\u002FCD pipelines, \u003Ca href=\"https:\u002F\u002Fplatformengineers.io\u002Fservices\u002Fcontainerization\u002F\" target=\"_blank\" rel=\"noopener\"\u003Econtainerization technologies\u003C\u002Fa\u003E, and developer portals.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003ESecurity &amp; Governance: Security is paramount, and platform engineers implement robust security measures like access controls, encryption, and vulnerability management. They also define governance policies to ensure responsible platform usage.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\" style=\"font-size: 18px;\"\u003E\u003Cstrong\u003EImpact and Benefits:\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EFaster Development Cycles:\u003C\u002Fstrong\u003E IDPs automate repetitive tasks and provide pre-built components, accelerating development and deployment.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EIncreased Developer Productivity: \u003C\u002Fstrong\u003EDevelopers spend less time on infrastructure and toil, focusing on core development tasks.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EImproved Scalability and Flexibility: \u003C\u002Fstrong\u003EPlatforms can easily scale to meet changing needs and adapt to new technologies.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EReduced Costs: \u003C\u002Fstrong\u003EAutomation and efficient resource utilization lead to lower operational costs.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EEnhanced Collaboration: \u003C\u002Fstrong\u003EIDPs facilitate collaboration between developers, operations teams, and other stakeholders.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\" style=\"font-size: 18px;\"\u003E\u003Cstrong\u003EChallenges and Best Practices:\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EScalability and Flexibility: \u003C\u002Fstrong\u003EPlatforms must be able to handle growing workloads and adapt to new requirements. Modularity and automation are key.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003ESecurity and Governance:\u003C\u002Fstrong\u003E Robust security measures and clear governance policies are essential to protect the platform and its data.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EAutomation: \u003C\u002Fstrong\u003EAutomating repetitive tasks frees developers and improves efficiency, but careful planning and testing are crucial.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EContinuous Improvement: \u003C\u002Fstrong\u003EPlatform engineering is an ongoing process, requiring continuous monitoring, feedback, and iteration.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003ESkills and Roles: \u003C\u002Fstrong\u003EPlatform engineers need a blend of technical expertise and soft skills. They should be proficient in cloud platforms, programming languages, system architecture, and automation tools. Strong communication, collaboration, and problem-solving skills are also essential.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\" style=\"font-size: 18px;\"\u003E\u003Cstrong\u003EFuture Trends:\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EThe future of platform engineering is bright, with exciting trends like:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EAI-powered Platforms: \u003C\u002Fstrong\u003EAI will play a bigger role in automating tasks, optimizing resource usage, and providing intelligent insights.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EAdvanced Containerization: \u003C\u002Fstrong\u003EContainerization technologies will evolve, offering greater flexibility and portability for applications.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003ECloud Innovations: \u003C\u002Fstrong\u003ECloud providers will continue to innovate, offering new services and capabilities that platform engineers can leverage to build even more powerful platforms.\u003C\u002Fp\u003E\n\u003Ch2 dir=\"ltr\" style=\"font-size: 20px;\"\u003E\u003Cstrong\u003EConclusion:\u003C\u002Fstrong\u003E\u003C\u002Fh2\u003E\n\u003Cp dir=\"ltr\"\u003EPlatform engineering is a rapidly evolving field with the potential to revolutionize software development. By embracing automation, agility, and collaboration, \u003Ca href=\"https:\u002F\u002Fplatformengineers.io\u002F\" target=\"_blank\" rel=\"noopener\"\u003Eplatform engineers\u003C\u002Fa\u003E can empower developers and build the future-proof platforms that businesses need to thrive in the digital age.\u003C\u002Fp\u003E\n\u003Ch2 dir=\"ltr\" style=\"font-size: 20px;\"\u003E\u003Cstrong\u003EFAQs\u003C\u002Fstrong\u003E\u003C\u002Fh2\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EQ. What is the role of automation in platform engineering?\u003C\u002Fstrong\u003E\u003Cbr\u003EA. Automation in platform engineering streamlines repetitive tasks, enhancing efficiency and minimizing errors in deployment and monitoring processes.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EQ. How does platform engineering contribute to business growth?\u003C\u002Fstrong\u003E\u003Cbr\u003EA. Platform engineering accelerates development cycles, fosters innovation, and optimizes resource utilization, resulting in accelerated business growth.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EQ. What challenges do platform engineers face in their roles?\u003C\u002Fstrong\u003E\u003Cbr\u003EA. Platform engineers often grapple with challenges such as ensuring system reliability, managing diverse technologies, and addressing security concerns.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EQ. Can small enterprises benefit from platform engineering?\u003C\u002Fstrong\u003E\u003Cbr\u003EA. Yes, the scalability of platform engineering makes it viable for small enterprises, offering cost-effective solutions and fostering innovation.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EQ. What are the future trends in platform engineering?\u003C\u002Fstrong\u003E\u003Cbr\u003EA. Future trends in platform engineering include the integration of artificial intelligence, advancements in containerization, and continuous innovations in cloud technologies\u003C\u002Fp\u003E",tags:a,time_to_read:"3 mins",user_created:{id:f,first_name:g,last_name:h,email:k,password:b,location:a,title:l,description:a,tags:a,avatar:i,language:a,tfa_secret:a,status:c,role:j,token:a,last_access:m,last_page:n,provider:d,external_identifier:a,auth_data:a,email_notifications:e,appearance:a,theme_dark:a,theme_light:a,theme_light_overrides:a,theme_dark_overrides:a},user_updated:{id:s,first_name:C,last_name:a,email:D,password:b,location:a,title:a,description:a,tags:a,avatar:E,language:a,tfa_secret:a,status:c,role:F,token:a,last_access:G,last_page:H,provider:d,external_identifier:a,auth_data:a,email_notifications:e,appearance:a,theme_dark:a,theme_light:a,theme_light_overrides:a,theme_dark_overrides:a},image:{id:"fabfce7f-6380-4132-b6f1-8232a386e3cc",storage:p,filename_disk:"fabfce7f-6380-4132-b6f1-8232a386e3cc.png",filename_download:"Untitled design (3) (1).png",title:"Untitled Design (3) (1)",type:r,folder:q,uploaded_by:s,created_on:bf,modified_by:a,modified_on:"2024-02-14T10:16:18.864Z",charset:a,filesize:"28570",width:I,height:I,duration:a,embed:a,description:a,location:a,tags:a,metadata:{},focal_point_x:a,focal_point_y:a,tus_id:a,tus_data:a,uploaded_on:bf},authors:[{id:aM,pe_blog_id:ag,directus_users_id:{first_name:g,last_name:h,avatar:i}}]},{id:af,status:o,sort:7,date_created:"2023-12-01T10:31:00.000Z",date_updated:"2024-04-16T06:36:31.559Z",slug:"unlocking-success-the-power-of-key-performance-indicators-kpis-for-platform-engineering-teams",title:"Power of Key Performance Indicators for Platform Engineers",description:"\u003Cp\u003EUnlock the power of Key Performance Indicators (KPIs) for platform engineering teams! Dive into the world of measurable metrics shaping efficient digital platforms. Learn how KPIs drive performance, enhance user experiences, and steer strategies for success in this comprehensive guide. From uptime and scalability to incident response and customer satisfaction, uncover the insights and formulas that empower platform engineering teams to excel. Explore actionable strategies, benefits, and the transformative impact of KPI-driven approaches in optimizing digital ecosystems.\u003C\u002Fp\u003E",seo_title:"Platform Engineering with Powerful KPIs",seo_description:"Unleash the full potential of your platform engineering team! This guide vital KPIs for optimizing performance, boosting reliability, and scaling with confidence.",content:"\u003Cp\u003EPlatform engineering forms the backbone of today's businesses, making digital services reliable and scalable.&nbsp;To steer this ship well, having clear performance indicators is key. Think of Key Performance Indicators (KPIs) as your team's friendly guide, helping focus on what the business truly needs.&nbsp;They're like signposts, showing how well your systems are doing and pointing out ways to make them even better.&nbsp;\u003C\u002Fp\u003E\n\u003Cp\u003EIn this blog, we'll explore how KPIs play a crucial role in optimizing your strategies for \u003Ca href=\"https:\u002F\u002Fplatformengineers.io\u002Fabout-us\u002F\" target=\"_blank\" rel=\"noopener\"\u003Eplatform engineering teams\u003C\u002Fa\u003E. Join us as we explore how KPIs wield their influence in crafting a more efficient and successful methodology for platform engineering teams.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cspan class=\"heading-content\" style=\"font-size: 20px;\"\u003E\u003Cstrong\u003EWhat Are KPIs?\u003C\u002Fstrong\u003E\u003C\u002Fspan\u003E\u003C\u002Fp\u003E\n\u003Cp\u003EKey Performance Indicators (KPIs) are measurable metrics that help organizations gauge their performance against specific goals and objectives.&nbsp;\u003C\u002Fp\u003E\n\u003Cp\u003EIn the context of platform engineering, KPIs can provide valuable insights into the health, efficiency, and effectiveness of your engineering efforts. By tracking and analyzing KPIs, you can make data-driven decisions to improve your platform's performance and functionality.\u003C\u002Fp\u003E\n\u003Ch2\u003E\u003Cspan class=\"heading-content\" style=\"font-size: 20px;\"\u003E\u003Cstrong\u003EChoosing the Right KPIs:\u003C\u002Fstrong\u003E\u003C\u002Fspan\u003E\u003C\u002Fh2\u003E\n\u003Cp\u003E\u003Cspan class=\"heading-content\"\u003ESelecting the right KPIs is crucial for a successful strategy of platform engineering teams. \u003C\u002Fspan\u003EHere are some essential KPIs that can help you optimize your efforts:\u003C\u002Fp\u003E\n\u003Ch3\u003E\u003Cspan class=\"heading-content\" style=\"font-size: 18px;\"\u003E\u003Cstrong\u003E1) Uptime and Availability:\u003C\u002Fstrong\u003E\u003C\u002Fspan\u003E\u003C\u002Fh3\u003E\n\u003Cp\u003E\u003Cstrong\u003EImportance:\u003C\u002Fstrong\u003E The availability of the platform is crucial as downtime can lead to revenue loss and a poor user experience. High uptime ensures that the platform is accessible to users when they need it.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003ECalculating Uptime:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Ccode class=\"inline\" spellcheck=\"false\"\u003E24 hours\u002Fday x 365 days\u002Fyear = 8,760 hours\u002Fyear.\u003C\u002Fcode\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Ccode class=\"inline\" spellcheck=\"false\"\u003ENumber of hours your network is up and running per year &divide; 8,760 hours x 100 = Yearly uptime percentage.\u003C\u002Fcode\u003E\u003C\u002Fp\u003E\n\u003Cp\u003EUptime signals a system's binary state of operation, whereas Availability delves deeper, encompassing \u003Ca href=\"https:\u002F\u002Fplatformengineers.io\u002Fservices\u002Fperformance-optimization\u002F\" target=\"_blank\" rel=\"noopener\"\u003Eperformance and functionality\u003C\u002Fa\u003E. It acknowledges that a system, while operational, may run below par, impacting user service.&nbsp;\u003C\u002Fp\u003E\n\u003Cp\u003EThus, evaluating both uptime and availability offers a comprehensive insight into a system's operational capacity and user satisfaction.\u003C\u002Fp\u003E\n\u003Ch3\u003E\u003Cspan class=\"heading-content\" style=\"font-size: 18px;\"\u003E\u003Cstrong\u003E2) Error Rate:\u003C\u002Fstrong\u003E\u003C\u002Fspan\u003E\u003C\u002Fh3\u003E\n\u003Cp\u003E\u003Cstrong\u003EImportance:\u003C\u002Fstrong\u003E Monitoring error rates helps identify issues and bugs within the platform. A high error rate indicates potential problems that need to be addressed to maintain reliability.\u003C\u002Fp\u003E\n\u003Cp\u003EThe Error Rate is a crucial Key Performance Indicator (KPI) that measures the frequency of errors or mistakes made in a specific process or system. To calculate the Error Rate for a platform engineering team, you can use the following formula:\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Ccode class=\"inline\" spellcheck=\"false\"\u003EError Rate = (Number of Errors\u002F Total Number of Actions)&nbsp;&times;100\u003C\u002Fcode\u003E\u003C\u002Fp\u003E\n\u003Cp\u003EWhere:\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003E\n\u003Cp\u003ENumber of Errors: This represents the total count of errors encountered within a given period.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli\u003E\n\u003Cp\u003ETotal Number of Actions: This refers to the total count of actions or transactions performed within the same period.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp\u003EThe result is typically multiplied by 100 to express the Error Rate as a percentage, providing a clearer representation of the error frequency in relation to the total actions taken.\u003C\u002Fp\u003E\n\u003Cp\u003EFor instance, let's say the platform engineering team \u003Ca href=\"https:\u002F\u002Fplatformengineers.io\u002Fcase-studies\" target=\"_blank\" rel=\"noopener\"\u003Eperformed 10,000 actions\u003C\u002Fa\u003E within a month and encountered 100 errors during that time. Using the formula:\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Ccode class=\"inline\" spellcheck=\"false\"\u003EError Rate = (100\u002F10000) &times; 100\u003C\u002Fcode\u003E\u003C\u002Fp\u003E\n\u003Cp\u003EError Rate=1%\u003C\u002Fp\u003E\n\u003Cp\u003EThis means that the Error Rate for that period is 1%, indicating that 1% of the actions performed resulted in errors. Adjust the time frame and actions\u002Ferrors count according to the specific period you're evaluating to calculate the Error Rate accurately for your platform engineers.\u003C\u002Fp\u003E\n\u003Ch2 style=\"font-size: 20px;\"\u003E\u003Cstrong\u003EWhat are actions and errors?\u003C\u002Fstrong\u003E\u003C\u002Fh2\u003E\n\u003Cp\u003E\u003Cstrong\u003EActions:\u003C\u002Fstrong\u003E Any meaningful user interaction or system operation within the platform, such as clicking buttons, processing transactions, or executing commands.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EErrors:\u003C\u002Fstrong\u003E Unexpected or undesired outcomes resulting from actions, encompassing system malfunctions, failed transactions, error messages, or inaccurate data output.\u003C\u002Fp\u003E\n\u003Ch3 style=\"font-size: 18px;\"\u003E\u003Cstrong\u003EWhy is it important for the platform engineering team?\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp\u003EMeasuring the error rate involves tallying the count of errors against the total number of actions within a specified timeframe.&nbsp;\u003C\u002Fp\u003E\n\u003Cp\u003EThis clarity ensures a precise evaluation of the platform's performance, aiding engineers in identifying and addressing issues effectively.\u003C\u002Fp\u003E\n\u003Ch3\u003E\u003Cspan class=\"heading-content\" style=\"font-size: 18px;\"\u003E\u003Cstrong\u003E3) Scalability Metrics:\u003C\u002Fstrong\u003E\u003C\u002Fspan\u003E\u003C\u002Fh3\u003E\n\u003Cp\u003E\u003Cstrong\u003EImportance:\u003C\u002Fstrong\u003E Tracking the platform's ability to scale horizontally or vertically is crucial as it ensures the system can handle increased load without performance degradation. Scalability is vital for growth and accommodating more users.\u003C\u002Fp\u003E\n\u003Ch3 style=\"font-size: 18px;\"\u003E\u003Cstrong\u003EThe Scalability Index formula is:&nbsp;\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp\u003E\u003Ccode class=\"inline\" spellcheck=\"false\"\u003EScalability Index =(Performance of Larger System\u002FPerformance of Smaller System) &times;100%\u003C\u002Fcode\u003E&nbsp; &nbsp; &nbsp;\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EPerformance of Larger System:\u003C\u002Fstrong\u003E Refers to the system's performance when it's scaled up or expanded, handling higher loads or resources.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EPerformance of Smaller System:\u003C\u002Fstrong\u003E Denotes the system's performance in its initial or smaller state, typically handling lower loads or resources.\u003C\u002Fp\u003E\n\u003Cp\u003EEvaluating performance often encompasses factors like response time (in milliseconds), throughput (in transactions per second), resource utilization (in percentage), or any specific metric relevant to the platform's functionality.\u003C\u002Fp\u003E\n\u003Cp\u003EFor instance, measuring the system's response time under increasing loads, say from 100 to 1000 concurrent users, allows engineers to assess how quickly the system processes requests as user numbers rise.&nbsp;\u003C\u002Fp\u003E\n\u003Cp\u003EThese metrics aid in understanding performance changes as the system transitions from its smaller state to a larger, more heavily loaded environment.\u003C\u002Fp\u003E\n\u003Ch3 style=\"font-size: 18px;\"\u003E\u003Cstrong\u003ESuggested Approach:\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp\u003EFor instance, in measuring the Scalability Index, consider using response time as a performance metric. Measure the response time of critical operations or user interactions under varying loads, comparing performance between the smaller and larger system configurations.\u003C\u002Fp\u003E\n\u003Ch3 style=\"font-size: 18px;\"\u003EScalability Index Formula using Response Time:\u003C\u002Fh3\u003E\n\u003Cp\u003E\u003Ccode class=\"inline\" spellcheck=\"false\"\u003E= (Response Time of Larger System\u002FResponse Time of Smaller System) x100%&nbsp;\u003C\u002Fcode\u003E\u003C\u002Fp\u003E\n\u003Cp\u003EBy assessing how response times scale as the system expands, this approach offers insights into how well the system handles increased loads or resources.&nbsp;\u003C\u002Fp\u003E\n\u003Cp\u003EThese metrics aid in understanding performance changes as the system transitions from its smaller state to a larger, more heavily loaded environment.\u003C\u002Fp\u003E\n\u003Cp\u003EEngineers can adapt this method by replacing \"response time\" with other relevant performance metrics to comprehensively evaluate scalability based on the platform's unique requirements and functionalities.\u003C\u002Fp\u003E\n\u003Ch3\u003E\u003Cspan class=\"heading-content\" style=\"font-size: 18px;\"\u003E\u003Cstrong\u003E4) Throughput:\u003C\u002Fstrong\u003E\u003C\u002Fspan\u003E\u003C\u002Fh3\u003E\n\u003Cp\u003E\u003Cstrong\u003EImportance: \u003C\u002Fstrong\u003EThroughput, quantifying successful requests or transaction volumes within a timeframe, is critical for evaluating system efficiency and capacity. It guides capacity planning, ensuring the platform can handle expected user loads, and showcasing its scalability and performance.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Ccode class=\"inline\" spellcheck=\"false\"\u003EFormula: Throughput = Total processed requests or transactions\u002F\u003C\u002Fcode\u003E\u003Ccode class=\"inline\" spellcheck=\"false\"\u003ETime period\u003C\u002Fcode\u003E\u003C\u002Fp\u003E\n\u003Ch3\u003E\u003Cspan class=\"heading-content\" style=\"font-size: 18px;\"\u003E\u003Cstrong\u003E5) Resource Utilization:\u003C\u002Fstrong\u003E\u003C\u002Fspan\u003E\u003C\u002Fh3\u003E\n\u003Cp\u003E\u003Cstrong\u003EImportance\u003C\u002Fstrong\u003E: Monitoring resource usage helps optimize infrastructure, ensuring efficient resource allocation, and cost-effectiveness, and preventing resource bottlenecks.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Ccode class=\"inline\" spellcheck=\"false\"\u003EUtilization Rate = (Resource Used\u002FTotal Available Resource) x 100%\u003C\u002Fcode\u003E&nbsp; &nbsp;&nbsp;\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EExamples of Resources Used can be:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003ECPU:\u003C\u002Fstrong\u003E This refers to the amount of processing power utilized by the system.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003ERAM (Memory):\u003C\u002Fstrong\u003E It indicates the extent of memory resources in use by the system.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EDisk Space:\u003C\u002Fstrong\u003E Represents the amount of storage space occupied by data, applications, or system files.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003ENetwork Bandwidth:\u003C\u002Fstrong\u003E Denotes the level of data transfer or network usage.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EExamples of Total Available Resources can be:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003ECPU:\u003C\u002Fstrong\u003E The total processing capacity of the system's CPU.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003ERAM (Memory):\u003C\u002Fstrong\u003E The total amount of available memory in the system.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EDisk Space:\u003C\u002Fstrong\u003E Total storage capacity or free disk space.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003ENetwork Bandwidth:\u003C\u002Fstrong\u003E The maximum data transfer capacity or available network bandwidth.\u003C\u002Fp\u003E\n\u003Ch3 style=\"font-size: 18px;\"\u003E\u003Cstrong\u003EUtilization Rate Calculation:\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp\u003EUtilization Rate calculates the percentage of resources utilized concerning their total available capacity. It's computed for each resource by dividing the used amount by the total available amount and multiplying by 100.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EFor example, for CPU utilization rate:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Ccode class=\"inline\" spellcheck=\"false\"\u003ECPU Utilization Rate = (CPU Used\u002FTotal CPU Capacity) x 100&nbsp;\u003C\u002Fcode\u003E\u003C\u002Fp\u003E\n\u003Cp\u003ESimilarly, this formula applies to other resources like RAM, disk space, or network bandwidth to determine their utilization rates.\u003C\u002Fp\u003E\n\u003Cp\u003EWhen assessing resource utilization across different components like CPU, memory (RAM), disk space, and network bandwidth, it's crucial to understand acceptable utilization rates.&nbsp;\u003C\u002Fp\u003E\n\u003Cp\u003EHere's guidance on evaluating utilization rates for various resources:\u003C\u002Fp\u003E\n\u003Ch3 style=\"font-size: 18px;\"\u003E\u003Cstrong\u003EAcceptable Utilization Rates for Different Resources:\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Col\u003E\n\u003Cli\u003E\u003Cstrong\u003ECPU Utilization Rate:\u003C\u002Fstrong\u003E\u003C\u002Fli\u003E\n\u003C\u002Fol\u003E\n\u003Cp\u003E\u003Cstrong\u003EGuideline: \u003C\u002Fstrong\u003EGenerally, average CPU utilization rates around 50% are considered optimum. Beyond this, sustained high usage may indicate the need for additional processing power to maintain optimal performance.\u003C\u002Fp\u003E\n\u003Col start=\"2\"\u003E\n\u003Cli\u003E\n\u003Cp\u003E\u003Cstrong\u003ERAM (Memory) Utilization Rate:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003C\u002Fol\u003E\n\u003Cp\u003E\u003Cstrong\u003EGuideline:\u003C\u002Fstrong\u003E Memory utilization rates up to 70% are often acceptable. Higher utilization might impact system responsiveness or lead to swapping, affecting performance.\u003C\u002Fp\u003E\n\u003Cp\u003EFor workloads that are not volatile in terms of memory use 70% - 80% memory utilization rates can be often acceptable. For volatile workloads maximum provisioning should be done and of course actions to reduce volatility should be taken by the team.\u003C\u002Fp\u003E\n\u003Col start=\"3\"\u003E\n\u003Cli\u003E\n\u003Cp\u003E\u003Cstrong\u003EDisk Space Utilization Rate:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003C\u002Fol\u003E\n\u003Cp\u003E\u003Cstrong\u003EGuideline\u003C\u002Fstrong\u003E: Aim to maintain disk space utilization below 50%-80% to prevent performance degradation or potential data loss due to insufficient space. For this utmost care should be taken and when the disk space utilization reaches 70%- 80% new disk space should be created immediately.&nbsp;\u003C\u002Fp\u003E\n\u003Col start=\"4\"\u003E\n\u003Cli\u003E\n\u003Cp\u003E\u003Cstrong\u003ENetwork Bandwidth Utilization Rate:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003C\u002Fol\u003E\n\u003Cp\u003E\u003Cstrong\u003EGuideline:\u003C\u002Fstrong\u003E Network utilization rates vary based on the network's capacity and usage patterns. Consistently full utilization of the network's bandwidth should be avoided to ensure adequate bandwidth for smooth data transfer and prevents congestion.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EImportance of Monitoring Utilization Rates:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp\u003ERegularly monitoring these utilization rates allows engineers to:\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003E\n\u003Cp\u003EIdentify potential bottlenecks or resource constraints.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli\u003E\n\u003Cp\u003EProactively allocate resources or scale infrastructure as needed.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli\u003E\n\u003Cp\u003EMaintain system performance within baselines.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli\u003E\n\u003Cp\u003EPrevent service disruptions due to resource exhaustion.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp\u003EHaving different resources with unique utilization rates can be overwhelming to calculate and consolidate. It can be equally difficult to see all resources from a single prism.&nbsp;\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003ESo, here are a few suggestions to overcome this:-\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EDashboard Display:\u003C\u002Fstrong\u003E Create a centralized view to show CPU, RAM, disk space, and network metrics together.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EResource Patterns Analysis: \u003C\u002Fstrong\u003EStudy usage trends to find connections between different resources.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EConsistent Metrics:\u003C\u002Fstrong\u003E Normalize metrics for easy comparison across resources.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EAlert Systems:\u003C\u002Fstrong\u003E Set up alerts for resource usage thresholds to take proactive measures.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EOptimization Strategies:\u003C\u002Fstrong\u003E Develop plans based on combined resource analysis for efficient system management.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EReporting Tools:\u003C\u002Fstrong\u003E Generate comprehensive reports merging diverse resource data for informed decision-making.\u003C\u002Fp\u003E\n\u003Cp\u003EThis approach offers a streamlined view of system health, reveals resource correlations, and enables proactive resource management for optimal system performance.\u003C\u002Fp\u003E\n\u003Ch3 style=\"font-size: 18px;\"\u003E\u003Cstrong\u003EAdjustments Based on Specific Needs:\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp\u003EResource utilization thresholds may vary based on the platform's unique requirements, application characteristics, and industry standards.&nbsp;\u003C\u002Fp\u003E\n\u003Cp\u003ETherefore, adjusting these thresholds to suit specific operational demands ensures an effective and comprehensive understanding of resource utilization.\u003C\u002Fp\u003E\n\u003Ch3\u003E\u003Cspan class=\"heading-content\" style=\"font-size: 18px;\"\u003E\u003Cstrong\u003E6) Incident Response and Resolution Time:\u003C\u002Fstrong\u003E\u003C\u002Fspan\u003E\u003C\u002Fh3\u003E\n\u003Cp\u003E\u003Cstrong\u003EIncident Response Time:-&nbsp;\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EImportance:\u003C\u002Fstrong\u003E Incident Response Time measures the duration taken to acknowledge and start addressing an incident after it occurs. A shorter response time indicates a more proactive and efficient response, reducing the impact of incidents on system availability and user experience.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EFormula:\u003C\u002Fstrong\u003E&nbsp;\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Ccode class=\"inline\" spellcheck=\"false\"\u003EIncident Response Time = Time of Incident Acknowledgement - Time of Incident Occurrence\u003C\u002Fcode\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EIncident Resolution Time:-&nbsp;\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EImportance:\u003C\u002Fstrong\u003E Incident Resolution Time measures the duration taken to resolve an incident fully. A shorter resolution time indicates a faster restoration of service, minimizing downtime, and ensuring a quicker return to normal system functionality.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EFormula:\u003C\u002Fstrong\u003E&nbsp;\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Ccode class=\"inline\" spellcheck=\"false\"\u003EIncident Resolution Time = Time of Incident Resolution - Time of Incident Occurrence\u003C\u002Fcode\u003E\u003C\u002Fp\u003E\n\u003Ch3\u003E\u003Cspan class=\"heading-content\" style=\"font-size: 18px;\"\u003E\u003Cstrong\u003E7) Security Metrics:\u003C\u002Fstrong\u003E\u003C\u002Fspan\u003E\u003C\u002Fh3\u003E\n\u003Cp\u003E\u003Cstrong\u003EImportance:\u003C\u002Fstrong\u003E Security metrics track the system's resilience against cyber threats and compliance with standards. It helps engineers identify vulnerabilities and maintain data integrity and user trust.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EExamples of Security Metrics:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003ENumber of Security Incidents:\u003C\u002Fstrong\u003E Quantifies the total count of security breaches, incidents, or unauthorized access attempts within a defined period. Each instance of unauthorized access or breach is documented, aiding in understanding the frequency and severity of security issues.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EFormula:\u003C\u002Fstrong\u003E Count the total incidents detected within a specific period.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EExample\u003C\u002Fstrong\u003E: In a month, the system encountered 15 unauthorized access attempts.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EVulnerability Rate:\u003C\u002Fstrong\u003E Measures the total count of identified vulnerabilities or weaknesses within the system's infrastructure, applications, or networks. It \u003Ca href=\"https:\u002F\u002Fplatformengineers.io\u002Fassessment\" target=\"_blank\" rel=\"noopener\"\u003Eassesses the system's\u003C\u002Fa\u003E susceptibility to potential threats and highlights areas requiring immediate attention or patching.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EFormula:\u003C\u002Fstrong\u003E Divide the number of identified vulnerabilities by the total systems or applications assessed.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EExample:\u003C\u002Fstrong\u003E 20 vulnerabilities found in an assessment of 100 systems.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003ECompliance Adherence Rate:\u003C\u002Fstrong\u003E Evaluates the system's compliance with industry-specific or regulatory standards such as GDPR, HIPAA, ISO, etc. It quantifies the system's alignment with required security protocols, policies, and guidelines.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EFormula:\u003C\u002Fstrong\u003E Divide compliant systems by the total number of systems assessed, multiplied by 100 for a percentage.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EExample:\u003C\u002Fstrong\u003E 80 out of 100 systems meet GDPR standards.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EPatch Management Effectiveness:\u003C\u002Fstrong\u003E Assesses the efficiency and timeliness of applying security patches or updates to address known vulnerabilities. It tracks the system's vulnerability exposure duration and the speed of implementing necessary security fixes.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EFormula:\u003C\u002Fstrong\u003E Calculate the time taken to apply patches after their release.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EExample:\u003C\u002Fstrong\u003E On average, patches are applied within 3 days of release.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003ETime to Detect and Time to Respond:\u003C\u002Fstrong\u003E Measures the duration taken to detect security incidents or breaches and the subsequent response time to mitigate or resolve these issues. A shorter detection and response time indicates a more proactive and efficient security posture.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EFormula:\u003C\u002Fstrong\u003E Measure the time taken from incident detection to resolution.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EExample:\u003C\u002Fstrong\u003E Incidents are detected within 1 hour and resolved in 4 hours.\u003C\u002Fp\u003E\n\u003Cp\u003EThese metrics collectively provide a comprehensive overview of a system's security posture, aiding engineers in identifying potential weaknesses, implementing necessary safeguards, and ensuring ongoing compliance with security standards and regulations.&nbsp;\u003C\u002Fp\u003E\n\u003Cp\u003EAdjust and prioritize these metrics based on the system's specific security needs and potential threats it faces.\u003C\u002Fp\u003E\n\u003Ch3\u003E\u003Cspan class=\"heading-content\" style=\"font-size: 18px;\"\u003E\u003Cstrong\u003E8) Deployment Frequency and Lead Time:\u003C\u002Fstrong\u003E\u003C\u002Fspan\u003E\u003C\u002Fh3\u003E\n\u003Cp\u003E\u003Cstrong\u003EImportance:\u003C\u002Fstrong\u003E These metrics indicate the team's agility and efficiency in delivering new features or updates. Faster deployment frequencies and shorter lead times contribute to faster innovation and adaptability.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EFormulas:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003E\n\u003Cp\u003EDeployment Frequency = Number of Deployments \u002F Time Period\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli\u003E\n\u003Cp\u003ELead Time = Time from Idea to Deployment\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch3\u003E\u003Cspan class=\"heading-content\" style=\"font-size: 18px;\"\u003E\u003Cstrong\u003E9) Developer Satisfaction:\u003C\u002Fstrong\u003E\u003C\u002Fspan\u003E\u003C\u002Fh3\u003E\n\u003Cp\u003E\u003Cstrong\u003EImportance:\u003C\u002Fstrong\u003E Gathering user feedback and measuring developer satisfaction can help gauge the platform's developer-friendliness and identify areas for improvement.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EFormulas:\u003C\u002Fstrong\u003E Net Promoter Score (NPS) = Percentage of Promoters - Percentage of Detractors\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cspan class=\"heading-content\"\u003E\u003Cstrong\u003E10) Capacity Planning and Forecasting:\u003C\u002Fstrong\u003E\u003C\u002Fspan\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EImportance:\u003C\u002Fstrong\u003E Predicting future resource needs and capacity requirements is essential for cost-effectively scaling the platform. This can be a place where there are chances of over-provisioning which can result in a dent in our pockets for no reason.\u003C\u002Fp\u003E\n\u003Cp\u003E\"Current Usage\" and \"Forecasted Growth\" can encompass various resources and cannot typically be represented by a single number.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003ECapacity Needed:\u003C\u002Fstrong\u003E The anticipated resources required in the future to support the platform's demand.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003ECurrent Usage:\u003C\u002Fstrong\u003E Represents the present utilization of various resources like CPU, memory, storage, network bandwidth, etc.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EForecasted Growth:\u003C\u002Fstrong\u003E Refers to the expected percentage increase in demand or workload across these resources over a specific period.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EConsiderations for Calculation:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp\u003EInstead of a single number for \"Current Usage,\" it involves assessing multiple resource parameters and their respective utilization levels.&nbsp;\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EFor instance:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003E\n\u003Cp\u003ECPU usage percentage\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli\u003E\n\u003Cp\u003EMemory utilization in GB\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli\u003E\n\u003Cp\u003EStorage space used in TB\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli\u003E\n\u003Cp\u003ENetwork bandwidth in Mbps\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp\u003ESimilarly, \"Forecasted Growth\" estimates the expected percentage increase across these resources based on business projections, user demand trends, or application usage forecasts.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003ECapacity Needed Formula:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Ccode class=\"inline\" spellcheck=\"false\"\u003ECapacity Needed = (Current Usage\u002F Forecasted Growth) x 100\u003C\u002Fcode\u003E\u003C\u002Fp\u003E\n\u003Cp\u003EThis formula estimates the required resources for future scalability based on the present resource utilization and anticipated growth.&nbsp;\u003C\u002Fp\u003E\n\u003Cp\u003EHowever, actual resource planning involves analyzing various resource parameters individually and projecting their respective growth for accurate capacity planning.&nbsp;\u003C\u002Fp\u003E\n\u003Cp\u003EAdjust the parameters according to specific resource types and anticipated growth rates to derive more precise estimations.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\u003C\u002Fp\u003E\n\u003Ch2\u003E\u003Cspan class=\"heading-content\" style=\"font-size: 20px;\"\u003E\u003Cstrong\u003EImplementing KPI&rsquo;s Strategy for Your Platform Engineering Team:\u003C\u002Fstrong\u003E\u003C\u002Fspan\u003E\u003C\u002Fh2\u003E\n\u003Cp\u003E\u003Cstrong\u003EDefine Clear Objectives:\u003C\u002Fstrong\u003E Begin by setting clear and specific goals for your platform engineering team. What do you want to achieve? Your KPIs should align with these objectives.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003ECollect and Analyze Data:\u003C\u002Fstrong\u003E Implement tools and processes to collect relevant data for your chosen KPIs. Use data analytics and visualization tools to gain insights from the data.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003ERegular Monitoring:\u003C\u002Fstrong\u003E Establish a routine for monitoring KPIs. Regularly check and update your KPI data to ensure you have a real-time understanding of your platform's performance.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EBenchmarking: \u003C\u002Fstrong\u003ECompare your KPIs with industry standards and competitors to understand where you stand and identify areas for improvement.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EActionable Insights:\u003C\u002Fstrong\u003E Use the insights gained from KPI analysis to make informed decisions and optimize your platform engineering team. If you notice a KPI is consistently falling short of its target, take corrective actions.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EContinuous Improvement:\u003C\u002Fstrong\u003E Your platform engineering strategy should be an ongoing process. Continuously review and refine your KPIs as your platform evolves and new challenges emerge.\u003C\u002Fp\u003E\n\u003Ch2\u003E\u003Cspan class=\"heading-content\" style=\"font-size: 20px;\"\u003E\u003Cstrong\u003EBenefits of KPI-driven Platform Engineering\u003C\u002Fstrong\u003E\u003C\u002Fspan\u003E\u003C\u002Fh2\u003E\n\u003Cp\u003E\u003Cstrong\u003EData-Driven Decision Making\u003C\u002Fstrong\u003E: KPIs provide objective data, enabling informed decisions rather than relying on assumptions or intuition.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EProactive Issue Identification:\u003C\u002Fstrong\u003E Monitoring KPIs can help in the early detection of potential issues or bottlenecks, allowing for timely interventions.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EPerformance Optimization: \u003C\u002Fstrong\u003ESince this is an emerging role\u003Cstrong\u003E,\u003C\u002Fstrong\u003E clear KPIs can help platform engineers direct their efforts towards areas that need improvement, optimizing the platform's overall performance.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EEnhanced User Experience:\u003C\u002Fstrong\u003E Prioritizing KPIs related to user satisfaction directly translates into a better user experience and retention.\u003C\u002Fp\u003E\n\u003Ch3\u003E\u003Cspan class=\"heading-content\" style=\"font-size: 18px;\"\u003E\u003Cstrong\u003EConclusion:\u003C\u002Fstrong\u003E\u003C\u002Fspan\u003E\u003C\u002Fh3\u003E\n\u003Cp\u003EIn the realm of platform engineering, Key Performance Indicators (KPIs) are our guiding stars, steering us toward efficiency, reliability, and scalability.\u003C\u002Fp\u003E\n\u003Cp\u003EHarnessing KPI insights empowers teams to make informed decisions, drive continuous improvement, and align strategies with organizational goals.\u003C\u002Fp\u003E\n\u003Cp\u003EAs we navigate the complex terrains of technological innovation, the strategic utilization of KPIs by platform engineering teams remains not just a tool, but a guiding principle, ensuring that every step taken is purposeful, measured, and in pursuit of excellence.&nbsp;\u003C\u002Fp\u003E\n\u003Cp\u003EWith KPIs as our guiding light, the journey toward optimizing the platform engineering team becomes an ongoing commitment to elevate performance, enhance user experiences, and pave the way for a more agile and successful future.\u003C\u002Fp\u003E",tags:["Platform Engineering Team","Key Performance Indicators (KPI)","Data Driven Decisions","Performance Metrics"],time_to_read:"11 minutes",user_created:{id:ba,first_name:ah,last_name:ai,email:bb,password:b,location:a,title:a,description:a,tags:a,avatar:aj,language:B,tfa_secret:a,status:c,role:j,token:a,last_access:bc,last_page:bd,provider:d,external_identifier:a,auth_data:a,email_notifications:e,appearance:a,theme_dark:a,theme_light:a,theme_light_overrides:a,theme_dark_overrides:a},user_updated:{id:s,first_name:C,last_name:a,email:D,password:b,location:a,title:a,description:a,tags:a,avatar:E,language:a,tfa_secret:a,status:c,role:F,token:a,last_access:G,last_page:H,provider:d,external_identifier:a,auth_data:a,email_notifications:e,appearance:a,theme_dark:a,theme_light:a,theme_light_overrides:a,theme_dark_overrides:a},image:{id:"037faa8b-fc0c-4585-83ed-648ead7ef34a",storage:p,filename_disk:"037faa8b-fc0c-4585-83ed-648ead7ef34a.jpg",filename_download:"platform-engineers-blogs.jpg",title:"Platform Engineers",type:"image\u002Fjpeg",folder:q,uploaded_by:f,created_on:bg,modified_by:f,modified_on:"2023-12-28T12:56:15.152Z",charset:a,filesize:"83471",width:ak,height:ak,duration:a,embed:a,description:a,location:a,tags:a,metadata:{},focal_point_x:a,focal_point_y:a,tus_id:a,tus_data:a,uploaded_on:bg},authors:[{id:aa,pe_blog_id:af,directus_users_id:{first_name:ah,last_name:ai,avatar:aj}}]},{id:ay,status:o,sort:ay,date_created:"2023-11-08T14:32:00.000Z",date_updated:"2024-05-22T11:31:00.658Z",slug:"s6-overlay-quickstart",title:bh,description:"\u003Cp data-pm-slice=\"1 1 []\"\u003ES6-overlay is designed for docker containers. But, It is hard to understand from the \u003Cu\u003E\u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fjust-containers\u002Fs6-overlay\u002Fblob\u002Fmaster\u002FREADME.md\" target=\"_self\"\u003EReadme\u003C\u002Fa\u003E\u003C\u002Fu\u003E of the \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fjust-containers\u002Fs6-overlay\" target=\"_self\"\u003Erepository\u003C\u002Fa\u003E. So, I thought that let&rsquo;s write a quick start guide for it.\u003C\u002Fp\u003E\n\u003Ch2\u003E&nbsp;\u003C\u002Fh2\u003E",seo_title:"Orchestrate Container Processes with s6-overlay",seo_description:"S6-Overlay offers powerful features like dependency control, graceful shutdown, and log management, making it a game-changer for containerized applications.",content:"\u003Ch2 dir=\"ltr\" style=\"font-size: 22px;\"\u003E\u003Cstrong\u003EWhat is S6-Overlay?\u003C\u002Fstrong\u003E\u003C\u002Fh2\u003E\n\u003Cp dir=\"ltr\"\u003ES6-overlay is a container-focused process manager that offers end-to-end management of the container's lifecycle, from initialization to graceful shutdown. Its innovative design and feature set make it a compelling alternative to other process managers, such as supervisord. But what sets it apart from the rest? Let's explore its key features.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\"\u003E\u003Cstrong\u003EKey Features of S6-Overlay\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EVersatile Process Management:\u003C\u002Fstrong\u003E S6-overlay efficiently handles both one-time tasks and long-running processes, making it versatile for \u003Ca href=\"https:\u002F\u002Fplatformengineers.io\u002Fservices\u002Fcontainerization\" target=\"_blank\" rel=\"noopener\"\u003Econtainerized tasks\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EDependency Control:\u003C\u002Fstrong\u003E Establish dependencies between processes to ensure orderly execution in complex application stacks.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003ESequence Management: \u003C\u002Fstrong\u003EControl the start and stop sequence of processes, streamlining container operations.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EEnvironment Variable Templating:\u003C\u002Fstrong\u003E Easily customize process behavior with environment variables, adapting to different environments.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EEasy Integration:\u003C\u002Fstrong\u003E Seamlessly integrate S6-overlay into Docker images with a straightforward installation process.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003ELog Management:\u003C\u002Fstrong\u003E Built-in log rotation simplifies log file management within container environments.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EGraceful Shutdown:\u003C\u002Fstrong\u003E Ensure data integrity with graceful process shutdown and the ability to execute custom scripts before container shutdown.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EMulti-Arch Support:\u003C\u002Fstrong\u003E S6-overlay accommodates the diverse landscape of container \u003Ca href=\"https:\u002F\u002Fplatformengineers.io\u002Fservices\u002Finfrastructure-maintenance-and-support\" target=\"_blank\" rel=\"noopener\"\u003Eplatforms with support \u003C\u002Fa\u003Efor multi-architecture container images.\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\"\u003E\u003Cstrong\u003EDirectory Structure of S6-Overlay\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003ES6-overlay's directory structure sets it apart from other process managers, providing flexibility for process management in container environments. The base directory, \u002Fetc\u002Fs6-overlay\u002Fs6-rc, serves as the starting point. To manage processes effectively, create a directory for each process you wish to run. Within these directories, several key files play essential roles:\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EType File:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EDefine the nature of your process by writing either \"oneshot\" or \"longrun.\" Use \"oneshot\" for tasks that run as initializations before primary processes. For continuous daemon processes supervised by S6, choose \"longrun.\"\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EUp File:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EIn this file, specify the file path of your script. This path guides S6 to run the script. If left blank, S6 will consider the \"run\" file as your script.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003ERun File:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EHere, you write the script or process that your container executes. If necessary, you can utilize environment variables and the \u003Ca href=\"https:\u002F\u002Fskarnet.org\u002Fsoftware\u002Fexecline\u002Fexeclineb.html\"\u003Eexeclineb\u003C\u002Fa\u003E command for added flexibility.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EDependencies Directory:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EThe dependencies.d directory plays a crucial role when your process relies on another. To establish dependencies, create empty files named after the processes your task depends on. Multiple files can be created when a process depends on more than one other process.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EThis directory structure simplifies the organization and execution of processes within your containerized environment, enhancing control and flexibility.\u003C\u002Fp\u003E\n\u003Cp\u003EFinally, it will look like below\u003C\u002Fp\u003E\n\u003Cdiv class=\"code-block with-line-numbers\" data-language=\"javascript\"\u003E\n\u003Cpre\u003E\u003Ccode spellcheck=\"false\"\u003Es6-overlay\n s6-rc.d\n     init-nginx\n    &nbsp;&nbsp;  dependencies.d\n    &nbsp;&nbsp; &nbsp;&nbsp;  base\n    &nbsp;&nbsp;  run\n    &nbsp;&nbsp;  type\n    &nbsp;&nbsp;  up\n     init-php82-fpm\n    &nbsp;&nbsp;  dependencies.d\n    &nbsp;&nbsp; &nbsp;&nbsp;  init-nginx\n    &nbsp;&nbsp;  run\n    &nbsp;&nbsp;  type\n    &nbsp;&nbsp;  up\n     init-usermod\n    &nbsp;&nbsp;  dependencies.d\n    &nbsp;&nbsp; &nbsp;&nbsp;  init-nginx\n    &nbsp;&nbsp;  run\n    &nbsp;&nbsp;  type\n    &nbsp;&nbsp;  up\n     svc-nginx\n    &nbsp;&nbsp;  dependencies.d\n    &nbsp;&nbsp; &nbsp;&nbsp;  init-nginx\n    &nbsp;&nbsp; &nbsp;&nbsp;  svc-php82-fpm\n    &nbsp;&nbsp;  run\n    &nbsp;&nbsp;  type\n     svc-php82-fpm\n    &nbsp;&nbsp;  dependencies.d\n    &nbsp;&nbsp;  run\n    &nbsp;&nbsp;  type\n     svc-stdout\n    &nbsp;&nbsp;  dependencies.d\n    &nbsp;&nbsp; &nbsp;&nbsp;  svc-nginx\n    &nbsp;&nbsp; &nbsp;&nbsp;  svc-php82-fpm\n    &nbsp;&nbsp;  run\n    &nbsp;&nbsp;  type\n     user\n         contents.d\n             init-nginx\n             init-php82-fpm\n             init-usermod\n             svc-nginx\n             svc-php82-fpm\n             svc-stdout\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003C\u002Fdiv\u003E\n\u003Cp dir=\"ltr\"\u003E\u003Cstrong\u003EInstalling S6\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003ES6-overlay is distributed in several tar files, each with a \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fjust-containers\u002Fs6-overlay?tab=readme-ov-file#installation\" target=\"_self\"\u003Edetailed description\u003C\u002Fa\u003E. The choice of which tar file to install depends on your specific requirements:\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003ETo utilize scripts with environment variables, you should install the symlinks tar files.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EIf you are running daemons that cannot log to stderr to take advantage of the s6 logging infrastructure, and rely on the old syslog() mechanism, consider installing the syslog-overlay-noarch tar file.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EHere is a sample Dockerfile code snippet to install S6-overlay in your Docker image,\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong id=\"docs-internal-guid-304629f9-7fff-4b58-9703-30876d7ce914\"\u003E\u003C\u002Fstrong\u003EYou can find the full image \u003Ca href=\"https:\u002F\u002Fgithub.com\u002FImprowised\u002Fdocker-php-base\u002Ftree\u002Fs6-php82\" target=\"_self\"\u003Ehere\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\n\u003Cdiv class=\"code-block with-line-numbers\" data-language=\"bash\"\u003E\n\u003Cpre\u003E\u003Ccode spellcheck=\"false\"\u003EFROM alpine:3.18\n\nARG S6_OVERLAY_VERSION=\"3.1.5.0\"\nARG S6_OVERLAY_ARCH=\"x86_64\"\n\n# add s6 overlay\nADD https:\u002F\u002Fgithub.com\u002Fjust-containers\u002Fs6-overlay\u002Freleases\u002Fdownload\u002Fv${S6_OVERLAY_VERSION}\u002Fs6-overlay-noarch.tar.xz \u002Ftmp\nRUN tar -C \u002F -Jxpf \u002Ftmp\u002Fs6-overlay-noarch.tar.xz\nADD https:\u002F\u002Fgithub.com\u002Fjust-containers\u002Fs6-overlay\u002Freleases\u002Fdownload\u002Fv${S6_OVERLAY_VERSION}\u002Fs6-overlay-${S6_OVERLAY_ARCH}.tar.xz \u002Ftmp\nRUN tar -C \u002F -Jxpf \u002Ftmp\u002Fs6-overlay-${S6_OVERLAY_ARCH}.tar.xz\n\n# add s6 optional symlinks\nADD https:\u002F\u002Fgithub.com\u002Fjust-containers\u002Fs6-overlay\u002Freleases\u002Fdownload\u002Fv${S6_OVERLAY_VERSION}\u002Fs6-overlay-symlinks-noarch.tar.xz \u002Ftmp\nRUN tar -C \u002F -Jxpf \u002Ftmp\u002Fs6-overlay-symlinks-noarch.tar.xz\nADD https:\u002F\u002Fgithub.com\u002Fjust-containers\u002Fs6-overlay\u002Freleases\u002Fdownload\u002Fv${S6_OVERLAY_VERSION}\u002Fs6-overlay-symlinks-arch.tar.xz \u002Ftmp\nRUN tar -C \u002F -Jxpf \u002Ftmp\u002Fs6-overlay-symlinks-arch.tar.xz\nADD https:\u002F\u002Fgithub.com\u002Fjust-containers\u002Fs6-overlay\u002Freleases\u002Fdownload\u002Fv${S6_OVERLAY_VERSION}\u002Fsyslogd-overlay-noarch.tar.xz \u002Ftmp\nRUN tar -C \u002F -Jxpf \u002Ftmp\u002Fsyslogd-overlay-noarch.tar.xz\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003C\u002Fdiv\u003E\n\u003Cp dir=\"ltr\"\u003EIn a production environment, when running PHP applications, it becomes essential to employ PHP-FPM. This is due to PHP's single-threaded nature, which can limit its ability to efficiently handle heavy traffic while conserving memory and CPU resources. In such scenarios, S6-overlay proves to be an ideal choice for managing these applications. In this example, we will demonstrate how to set up PHP-FPM alongside \u003Ca href=\"https:\u002F\u002Fwww.nginx.com\u002F\" target=\"_blank\" rel=\"noopener\"\u003ENginx\u003C\u002Fa\u003E using S6-overlay.&nbsp;\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\"\u003E\u003Cstrong\u003EInstalling PHP and Nginx\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003ETo install PHP and Nginx, you can use a package manager. In our case, we are utilizing an Alpine base image, making it convenient to use the apk package manager.\u003C\u002Fp\u003E\n\u003Cdiv class=\"code-block with-line-numbers\" data-language=\"bash\"\u003E\n\u003Cpre\u003E\u003Ccode spellcheck=\"false\"\u003E# Install OS Dependencies\nRUN set -ex \\\n  &amp;&amp; apk add --no-cache --virtual .build-deps \\\n    autoconf automake build-base python3 gmp-dev \\\n    curl \\\n    tar \\\n  &amp;&amp; apk add --no-cache --virtual .run-deps \\\n    nodejs npm \\\n    # PHP and extensions\n    php82 php82-bcmath php82-ctype php82-curl php82-dom php82-exif php82-fileinfo \\\n    php82-fpm php82-gd php82-gmp php82-iconv php82-intl php82-mbstring \\\n    php82-mysqlnd php82-mysqli php82-opcache php82-openssl php82-pcntl php82-pecl-apcu php82-pdo php82-pdo_mysql \\\n    php82-phar php82-posix php82-session php82-simplexml php82-sockets php82-sqlite3 php82-tidy \\\n    php82-tokenizer php82-xml php82-xmlreader php82-xmlwriter php82-zip php82-pecl-xdebug php82-pecl-redis php82-soap php82-sodium php82-pdo_sqlite php82-pdo_pgsql php82-pgsql \\\n    # Other dependencies\n    mariadb-client sudo shadow \\\n    # Miscellaneous packages\n    bash ca-certificates dialog git libjpeg libpng-dev openssh-client vim wget shadow \\\n    # Nginx\n    nginx \\\n    # Create directories\n  &amp;&amp; mkdir -p \u002Fetc\u002Fnginx \\\n    &amp;&amp; mkdir -p \u002Frun\u002Fnginx \\\n    &amp;&amp; mkdir -p \u002Fetc\u002Fnginx\u002Fsites-available \\\n    &amp;&amp; mkdir -p \u002Fetc\u002Fnginx\u002Fsites-enabled \\\n    &amp;&amp; rm -Rf \u002Fvar\u002Fwww\u002F* \\\n    &amp;&amp; rm -Rf \u002Fetc\u002Fnginx\u002Fnginx.conf \\\n  # Composer\n  &amp;&amp; wget https:\u002F\u002Fcomposer.github.io\u002Finstaller.sig -O - -q | tr -d '\\n' &gt; installer.sig \\\n    &amp;&amp; php82 -r \"copy('https:\u002F\u002Fgetcomposer.org\u002Finstaller', 'composer-setup.php');\" \\\n    &amp;&amp; php82 -r \"if (hash_file('SHA384', 'composer-setup.php') === file_get_contents('installer.sig')) { echo 'Installer verified'; } else { echo 'Installer corrupt'; unlink('composer-setup.php'); } echo PHP_EOL;\" \\\n    &amp;&amp; php82 composer-setup.php --install-dir=\u002Fusr\u002Fbin --filename=composer \\\n    &amp;&amp; php82 -r \"unlink('composer-setup.php'); unlink('installer.sig');\" \\\n  # Cleanup\n  &amp;&amp; apk del .build-deps\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003C\u002Fdiv\u003E\n\u003Cp dir=\"ltr\"\u003EHere we also installed Nodejs and npm as we are using this image for the Laravel app in which frontend files will be built using vite.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003ENow we need PHP-FPM and Nginx config.&nbsp; Generally, we kept in base image repo in the following directory structure\u003C\u002Fp\u003E\n\u003Cdiv class=\"code-block with-line-numbers\" data-language=\"javascript\"\u003E\n\u003Cpre\u003E\u003Ccode spellcheck=\"false\"\u003E rootfs\n     etc\n         nginx\n        &nbsp;&nbsp;  nginx.conf\n        &nbsp;&nbsp;  sites-available\n        &nbsp;&nbsp; &nbsp;&nbsp;  default.conf\n        &nbsp;&nbsp;  sites-enabled\n         php82\n        &nbsp;&nbsp;  conf.d\n        &nbsp;&nbsp;  php-fpm.conf\n        &nbsp;&nbsp;  php-fpm.d\n        &nbsp;&nbsp; &nbsp;&nbsp;  www.conf\n        &nbsp;&nbsp;  php.ini\n         s6-overlay\n        &nbsp;&nbsp;  s6-rc.d\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003C\u002Fdiv\u003E\n\u003Cp\u003ESo we added that using the below line in the Dockerfile\u003C\u002Fp\u003E\n\u003Cdiv class=\"code-block with-line-numbers\" data-language=\"javascript\"\u003E\n\u003Cpre\u003E\u003Ccode spellcheck=\"false\"\u003EADD rootfs \u002F\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003C\u002Fdiv\u003E\n\u003Ch3 dir=\"ltr\"\u003ESetting Users &amp; Permissions in Your Image\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003EBy default, Docker runs containers with root privileges. While this is suitable for local development, where \u003Ca href=\"https:\u002F\u002Fplatformengineers.io\u002Fblog\u002Finternal-developer-platform\" target=\"_blank\" rel=\"noopener\"\u003Edevelopers\u003C\u002Fa\u003E may need to install new packages or edit configs, it is not recommended for production environments. Allowing anyone with access to the container to add packages or change permissions can pose security risks.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003ETo address this, it's advisable to run your processes as non-root users. To achieve this, we utilize the following script to set the User ID (UID) and Group ID (GID) of the \"nginx\" user and group. This script is included in the S6-overlay, ensuring that it runs as a one-shot task before the start of PHP-FPM and Nginx.\u003C\u002Fp\u003E\n\u003Cdiv class=\"code-block with-line-numbers\" data-language=\"javascript\"\u003E\n\u003Cpre\u003E\u003Ccode spellcheck=\"false\"\u003E#!\u002Fusr\u002Fbin\u002Fwith-contenv bash\n# shellcheck shell=bash\n\nUID=${UID:-911}\nGID=${GID:-911}\n\nusermod -u \"$UID\" nginx &amp;&amp; groupmod -g \"$GID\" nginx\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003C\u002Fdiv\u003E\n\u003Cp\u003EIn that script, you can use the environment variables that you passed to your container.&nbsp;i.e\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Ccode class=\"inline\" spellcheck=\"false\"\u003Edocker run --name s6-app -e UID -e GID -p 8888:80 -v .:\u002Fvar\u002Fwww\u003C\u002Fcode\u003E\u003C\u002Fp\u003E\n\u003Ch3 dir=\"ltr\"\u003EProcess Sequences\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003EWhen you are running multiple processes, it becomes essential to ensure they run in a specific sequence. S6-overlay simplifies the process of defining and controlling the sequence in which these processes operate.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EFor instance, if your Nginx process depends on an initialization process for Nginx, it's straightforward to establish the sequence, ensuring that the necessary processes start in the correct order.&nbsp;\u003C\u002Fp\u003E\n\u003Cdiv class=\"code-block with-line-numbers\" data-language=\"javascript\"\u003E\n\u003Cpre\u003E\u003Ccode spellcheck=\"false\"\u003Ecd svc-nginx\nmkdir dependencies.d\ntouch init-nginx svc-php82-fpm\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003C\u002Fdiv\u003E\n\u003Cp\u003ENow, \u003Ccode class=\"inline\" spellcheck=\"false\"\u003Esvc-nginx\u003C\u002Fcode\u003E will start only after \u003Ccode class=\"inline\" spellcheck=\"false\"\u003Einit-nginx\u003C\u002Fcode\u003E will be completed(as it is oneshot) and \u003Ccode class=\"inline\" spellcheck=\"false\"\u003Esvc-php82-fpm\u003C\u002Fcode\u003E will be in a running state.\u003C\u002Fp\u003E\n\u003Ch3\u003E\u003Cspan class=\"heading-content\"\u003EGraceful Shutdown\u003C\u002Fspan\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003EA graceful shutdown is crucial for preserving the proper state of data and responses when a container exits. S6-overlay simplifies this process by supporting the use of variables like S6_SERVICES_GRACETIME and S6_KILL_GRACETIME to set a graceful exit time frame. Additionally, S6-overlay allows you to customize the exit code that the container returns upon termination. You can achieve this by creating a \"finish\" script in your process directory.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EFor instance, in cases where you are running a non-critical process, you can use the finish script to specify a return status of 0. The inclusion of a finish script is optional but valuable when you need to modify the exit code or execute specific cleanup tasks during the container's shutdown.\u003C\u002Fp\u003E\n\u003Cdiv class=\"code-block with-line-numbers\" data-language=\"bash\"\u003E\n\u003Cpre\u003E\u003Ccode spellcheck=\"false\"\u003E#!\u002Fbin\u002Fsh\n\necho \"0\" &gt; \u002Frun\u002Fs6-linux-init-container-results\u002Fexitcode\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003C\u002Fdiv\u003E\n\u003Ch3\u003E\u003Cspan class=\"heading-content\"\u003EExtending Image\u003C\u002Fspan\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003ES6-overlay's directory-based structure offers a straightforward method for extending this image from another base image. You can effortlessly create multiple images by adding additional processes without overwriting existing configurations in S6-overlay.\u003C\u002Fp\u003E\n\u003Cdiv class=\"code-block with-line-numbers\" data-language=\"javascript\"\u003E\n\u003Cpre\u003E\u003Ccode spellcheck=\"false\"\u003ECOPY svc-crond \u002Fetc\u002Fs6-overlay\u002Fs6-rc.d\u002Fsvc-crond\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003C\u002Fdiv\u003E\n\u003Ch3\u003E\u003Cspan class=\"heading-content\"\u003ESummary\u003C\u002Fspan\u003E\u003C\u002Fh3\u003E\n\u003Cp dir=\"ltr\"\u003EFor many years, we relied on Supervisor in our production environments. However, it became evident that the Supervisor could not address all the challenges we encountered.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EThis is where S6-overlay emerged as a modern alternative, offering a comprehensive solution to our process management needs.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003ES6-overlay's versatility and ease of use make it a perfect choice, particularly for container images that require the execution of multiple processes.&nbsp;\u003C\u002Fp\u003E",tags:a,time_to_read:aV,user_created:{id:"c7ce7ff3-6577-4e14-9e95-71b2dd8d67c4",first_name:ao,last_name:ap,email:"munir@improwised.com",password:b,location:a,title:a,description:a,tags:a,avatar:aq,language:a,tfa_secret:a,status:c,role:j,token:a,last_access:"2023-11-08T11:54:20.706Z",last_page:"\u002Fcontent\u002Fpe_blog\u002F6",provider:d,external_identifier:a,auth_data:a,email_notifications:e,appearance:a,theme_dark:a,theme_light:a,theme_light_overrides:a,theme_dark_overrides:a},user_updated:{id:f,first_name:g,last_name:h,email:k,password:b,location:a,title:l,description:a,tags:a,avatar:i,language:a,tfa_secret:a,status:c,role:j,token:a,last_access:m,last_page:n,provider:d,external_identifier:a,auth_data:a,email_notifications:e,appearance:a,theme_dark:a,theme_light:a,theme_light_overrides:a,theme_dark_overrides:a},image:{id:"8c141490-592d-44e9-af15-f62f90bf32d1",storage:p,filename_disk:"8c141490-592d-44e9-af15-f62f90bf32d1.png",filename_download:"S6 Image-modified-min.png",title:bh,type:r,folder:q,uploaded_by:A,created_on:bi,modified_by:f,modified_on:"2023-12-28T12:55:24.377Z",charset:a,filesize:"320237",width:ak,height:ak,duration:a,embed:a,description:a,location:a,tags:a,metadata:{},focal_point_x:a,focal_point_y:a,tus_id:a,tus_data:a,uploaded_on:bi},authors:[{id:S,pe_blog_id:ay,directus_users_id:{first_name:ao,last_name:ap,avatar:aq}}]},{id:az,status:o,sort:az,date_created:"2023-09-20T11:19:44.294Z",date_updated:"2024-04-16T06:36:15.305Z",slug:"role-of-golden-path-in-internal-developer-platforms",title:"Understanding the Role of Golden Path in Internal Developer Platforms",description:"\u003Cp dir=\"ltr\"\u003EPlatform engineering is an evolving discipline that underscores the importance of providing developers with consistent, scalable, and effective tools to foster innovation. In the context of DevOps, platform engineering bridges the gap between software development and IT operations, offering seamless integration and streamlined processes. A pivotal instrument in this nexus is the Internal Developer Platform (IDP).\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EInternal Developer Platforms are platforms designed within organizations to automate the environments, tools, and workflows that their developers use. It's like a &ldquo;PaaS&rdquo; but built and tailored for a specific company&rsquo;s needs. Within these platforms, a pivotal component that promises standardization and efficiency is the \"Golden Path\".\u003C\u002Fp\u003E",seo_title:"Internal Dev Platform Complexity with Golden Paths",seo_description:"Reduce friction in your development pipeline. Learn how Golden Paths can simplify your IDP, leading to faster deployments & increased code quality.",content:"\u003Cp dir=\"ltr\"\u003EPlatform engineering is an evolving discipline that underscores the importance of providing developers with consistent, scalable, and effective tools to foster innovation. In the context of DevOps, platform engineering bridges the gap between software development and IT operations, offering seamless integration and streamlined processes. A pivotal instrument in this nexus is the Internal Developer Platform (IDP).\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EInternal Developer Platforms are platforms designed within organizations to automate the environments, tools, and workflows that their developers use. It's like a &ldquo;PaaS&rdquo; but built and tailored for a specific company&rsquo;s needs. Within these platforms, a pivotal component that promises standardization and efficiency is the \u003Cstrong\u003E\"Golden Path\"\u003C\u002Fstrong\u003E.\u003C\u002Fp\u003E\n\u003Ch2 dir=\"ltr\" style=\"font-size: 22px;\"\u003E\u003Cstrong\u003EWhat are Golden Paths?\u003C\u002Fstrong\u003E\u003C\u002Fh2\u003E\n\u003Cp\u003EGolden paths are predefined, recommended workflows and \u003Ca href=\"https:\u002F\u002Fplatformengineers.io\u002Fblog\u002Fevolving-from-software-to-platform-engineering\" target=\"_blank\" rel=\"noopener\"\u003Eenvironments for developers\u003C\u002Fa\u003E. Instead of letting developers figure out every process from scratch, golden paths provide a blueprint for how certain tasks should ideally be executed.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EThese paths ensure that there's consistency in how different developers approach similar problems, thus enhancing efficiency and overall code quality. Furthermore, for startups and enterprises, golden paths are instrumental in streamlining onboarding, making collaboration more intuitive, and promoting knowledge sharing across teams.\u003C\u002Fp\u003E\n\u003Ch2 dir=\"ltr\" style=\"font-size: 20px;\"\u003E\u003Cstrong\u003EBenefits of Golden Path in Internal Developer Platforms\u003C\u002Fstrong\u003E\u003C\u002Fh2\u003E\n\u003Cul\u003E\n\u003Cli dir=\"ltr\"\u003EEnhanced Productivity and Efficiency: Standardized workflows reduce decision fatigue and ensure that developers are working optimally.\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\"\u003EReduced Errors and Rework: Consistent practices mean fewer mistakes and less rework, saving both time and resources.\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\"\u003EAccelerated Development Timelines: With clear guidelines in place, the development process can move faster, leading to quicker time-to-market.\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\"\u003EImproved Collaboration: With everyone on the same page, collaboration becomes seamless and more effective.\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\"\u003EEasier Onboarding: New team members can quickly understand and integrate into existing workflows.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch2 dir=\"ltr\" style=\"font-size: 20px;\"\u003E\u003Cstrong\u003EDesigning Effective Golden Path\u003C\u002Fstrong\u003E\u003C\u002Fh2\u003E\n\u003Ch3 dir=\"ltr\" style=\"font-size: 18px;\"\u003EThe creation of an effective golden path involves:\u003C\u002Fh3\u003E\n\u003Cul\u003E\n\u003Cli dir=\"ltr\"\u003EUnderstanding Developer Personas: Different developers have different needs. Identify these personas to tailor golden path effectively.\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\"\u003EUser Research: Gather feedback from developers to understand pain points and areas of improvement.\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\"\u003EChoosing Tools and Technologies: The tools selected should align with the organization's objectives and the developers' preferences.\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\"\u003EClear Documentation: Each golden path should be documented meticulously, ensuring clarity.\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\"\u003ERegular Updates: The tech landscape is always evolving. Golden path should be reviewed and updated periodically.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch2 dir=\"ltr\" style=\"font-size: 20px;\"\u003E\u003Cstrong\u003EImplementing Golden Path in Internal Developer Platforms\u003C\u002Fstrong\u003E\u003C\u002Fh2\u003E\n\u003Ch3 dir=\"ltr\" style=\"font-size: 18px;\"\u003EIntegration is key. This involves:\u003C\u002Fh3\u003E\n\u003Cul\u003E\n\u003Cli dir=\"ltr\"\u003EEmbedding into the IDP: Golden path should be a part of the \u003Ca href=\"https:\u002F\u002Fplatformengineers.io\u002Fblog\u002Finternal-developer-platform\" target=\"_blank\" rel=\"noopener\"\u003EInternal Developer Platform\u003C\u002Fa\u003E, easily accessible and visible.\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\"\u003EVisibility and Tracking: Monitor how often and effectively these paths are being used.\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\"\u003EPromotion: Conduct training sessions, workshops, and other educational initiatives to encourage adoption.\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\"\u003EContinuous Improvement: \u003Ca href=\"https:\u002F\u002Fplatformengineers.io\u002Fcontact-us\" target=\"_blank\" rel=\"noopener\"\u003EUse feedback\u003C\u002Fa\u003E and data analysis to continuously refine and improve golden path.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch2 style=\"font-size: 20px;\"\u003E\u003Cstrong\u003EChallenges and Considerations\u003C\u002Fstrong\u003E\u003C\u002Fh2\u003E\n\u003Ch3 style=\"font-size: 18px;\"\u003EEvery innovation comes with challenges:\u003C\u002Fh3\u003E\n\u003Cul\u003E\n\u003Cli dir=\"ltr\"\u003EResistance to Change: Some developers may prefer their established workflows.\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\"\u003EFlexibility vs. Standardization: Striking a balance is crucial.\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\"\u003EEvolving Tech Landscape: Ensuring that golden path remain relevant amidst rapidly changing technologies.\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\"\u003E\u003Ca href=\"https:\u002F\u002Fplatformengineers.io\u002Fservices\u002Finfrastructure-maintenance-and-support\" target=\"_blank\" rel=\"noopener\"\u003EMonitoring and Maintenance\u003C\u002Fa\u003E: Continuous efforts are needed to ensure golden path retain their effectiveness over time.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp dir=\"ltr\"\u003EGolden path, when well-implemented within Internal Developer Platforms, can revolutionize the way startups and enterprises function. They promise consistency, efficiency, and a unified vision. However, the key lies in continuous improvement and adaptation, ensuring that these paths evolve as the organization grows.\u003C\u002Fp\u003E\n\u003Cp\u003E&nbsp;\u003C\u002Fp\u003E",tags:[aw,al,am,"golden path",bj],time_to_read:bk,user_created:{id:A,first_name:t,last_name:u,email:K,password:b,location:a,title:a,description:a,tags:a,avatar:v,language:B,tfa_secret:a,status:c,role:j,token:a,last_access:L,last_page:M,provider:d,external_identifier:a,auth_data:a,email_notifications:e,appearance:a,theme_dark:a,theme_light:a,theme_light_overrides:a,theme_dark_overrides:a},user_updated:{id:s,first_name:C,last_name:a,email:D,password:b,location:a,title:a,description:a,tags:a,avatar:E,language:a,tfa_secret:a,status:c,role:F,token:a,last_access:G,last_page:H,provider:d,external_identifier:a,auth_data:a,email_notifications:e,appearance:a,theme_dark:a,theme_light:a,theme_light_overrides:a,theme_dark_overrides:a},image:{id:"43794616-8a42-4819-aa70-3209f8c3921c",storage:p,filename_disk:"43794616-8a42-4819-aa70-3209f8c3921c.png",filename_download:"5454613-ai (1).png",title:"5454613 Ai (1)",type:r,folder:q,uploaded_by:s,created_on:bl,modified_by:a,modified_on:"2024-02-14T10:17:38.403Z",charset:a,filesize:"81247",width:bm,height:bm,duration:a,embed:a,description:a,location:a,tags:a,metadata:{},focal_point_x:a,focal_point_y:a,tus_id:a,tus_data:a,uploaded_on:bl},authors:[{id:_,pe_blog_id:az,directus_users_id:{first_name:t,last_name:u,avatar:v}}]},{id:$,status:o,sort:$,date_created:"2023-09-06T13:57:28.025Z",date_updated:"2024-05-20T06:47:52.536Z",slug:"platform-engineering-in-a-multi-cloud-world",title:"Platform Engineering in a Multi-Cloud World",description:"\u003Cp dir=\"ltr\"\u003EIn today's fast-paced tech world, using a multi-cloud strategy isn't just a trend; it's essential. Both SaaS startups and enterprises are finding that relying on just one cloud provider limits their potential. With every cloud provider offering specialized services, a multi-cloud approach provides the flexibility to harness the best of each. This not only optimizes operational efficiency but also broadens the horizon for innovation. Let's journey into this multi-cloud domain to understand its intricacies and the vast opportunities it presents.\u003C\u002Fp\u003E",seo_title:"Microservices & Containers: Conquer Multi-Cloud ",seo_description:"Simplify multi-cloud complexity! Learn practical tools and techniques like Terraform, Kubernetes, and unified security to navigate the multi-cloud landscape",content:"\u003Ch2 dir=\"ltr\" style=\"font-size: 22px;\"\u003E\u003Cstrong\u003EPlatform Engineering's Multi-Cloud Challenge\u003C\u002Fstrong\u003E\u003C\u002Fh2\u003E\n\u003Ch2 dir=\"ltr\" style=\"font-size: 20px;\"\u003E\u003Cstrong\u003E1. Inception&nbsp;\u003C\u002Fstrong\u003E\u003C\u002Fh2\u003E\n\u003Cp dir=\"ltr\"\u003EThe digital age presents us with an abundance of cloud options. While initially companies aligned with single cloud providers, the landscape has shifted. Now, with providers like AWS, GCP, and Azure offering specialized services, the push towards a multi-cloud approach has never been more compelling. But why is this trend accelerating?\u003C\u002Fp\u003E\n\u003Ch2 dir=\"ltr\" style=\"font-size: 20px;\"\u003E\u003Cstrong\u003E2. Why Multi-Cloud?\u003C\u002Fstrong\u003E\u003C\u002Fh2\u003E\n\u003Ch3 dir=\"ltr\" style=\"font-size: 18px;\"\u003Ea. Business Continuity\u003C\u002Fh3\u003E\n\u003Cul\u003E\n\u003Cli dir=\"ltr\"\u003EAvoiding Vendor Lock-in: \u003Ca href=\"https:\u002F\u002Fplatformengineers.io\u002Findustries\u002FSaaS\" target=\"_blank\" rel=\"noopener\"\u003EModern SaaS startups\u003C\u002Fa\u003E require agility in their operations. Single cloud dependency can prove detrimental, especially when contractual and service changes arise. By diversifying cloud providers, startups ensure they have the flexibility to adapt without major disruptions.\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\"\u003ERedundancy and Disaster Recovery: Enterprise operations are vast, and any downtime can translate into significant revenue loss. A multi-cloud strategy ensures there&rsquo;s always a backup. For example, If AWS faces an outage, perhaps your operations on \u003Ca href=\"https:\u002F\u002Fazure.microsoft.com\u002F\" target=\"_blank\" rel=\"noopener\"\u003EAzure\u003C\u002Fa\u003E remain unaffected.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch3 dir=\"ltr\" style=\"font-size: 18px;\"\u003Eb. Best-of-Breed Solutions\u003C\u002Fh3\u003E\n\u003Cul\u003E\n\u003Cli dir=\"ltr\"\u003ELeveraging Unique Features: Each cloud provider has its flagship offerings. \u003Ca href=\"https:\u002F\u002Faws.amazon.com\u002F\" target=\"_blank\" rel=\"noopener\"\u003EAWS\u003C\u002Fa\u003E might offer a robust computing infrastructure, while Google Cloud is renowned for machine learning capabilities. By not limiting themselves, businesses can tailor their tech stack, choosing the best from each provider.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch3 dir=\"ltr\" style=\"font-size: 18px;\"\u003Ec. Regulatory and Compliance Needs\u003C\u002Fh3\u003E\n\u003Cul\u003E\n\u003Cli dir=\"ltr\"\u003ERegional Data Storage: As businesses expand globally, they encounter varied data storage laws. For instance, GDPR in Europe mandates specific data storage and processing practices. Having a presence on a local cloud provider can simplify compliance.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch3 dir=\"ltr\" style=\"font-size: 18px;\"\u003Ed. Risk Mitigation\u003C\u002Fh3\u003E\n\u003Cul\u003E\n\u003Cli dir=\"ltr\"\u003ESpread of Assets: By distributing assets across multiple cloud providers, businesses effectively dilute the impact of any singular adverse event or decision made by one provider. This strategic dispersal mitigates potential risks, ensuring a safer and more stable operational environment.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch2 dir=\"ltr\" style=\"font-size: 20px;\"\u003E\u003Cstrong\u003E3. Challenges in Implementing Multi-Cloud Strategies\u003C\u002Fstrong\u003E\u003C\u002Fh2\u003E\n\u003Ch3 dir=\"ltr\" style=\"font-size: 18px;\"\u003Ea. Complexity in Management\u003C\u002Fh3\u003E\n\u003Cul\u003E\n\u003Cli dir=\"ltr\"\u003EUsing multiple providers means grappling with different dashboards, API configurations, and services. For startups, especially, this can prove overwhelming, slowing down deployments and increasing the learning curve.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch3 dir=\"ltr\" style=\"font-size: 18px;\"\u003Eb. Data Management and Transfer Costs\u003C\u002Fh3\u003E\n\u003Cul\u003E\n\u003Cli dir=\"ltr\"\u003ETransferring data, especially large datasets, between providers can be \u003Ca href=\"https:\u002F\u002Fplatformengineers.io\u002Fassessment\" target=\"_blank\" rel=\"noopener\"\u003Ecost-intensive\u003C\u002Fa\u003E due to egress fees. Additionally, latency can become an issue if data needs to traverse between providers frequently.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch3 dir=\"ltr\" style=\"font-size: 18px;\"\u003Ec. Security and Compliance\u003C\u002Fh3\u003E\n\u003Cul\u003E\n\u003Cli dir=\"ltr\"\u003EWhile multi-cloud offers redundancy, it also introduces multiple potential points of vulnerability. A security breach in one cloud can have cascading effects, especially if there's data interdependency.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch3 dir=\"ltr\"\u003Ed. Skill Set and Training\u003C\u002Fh3\u003E\n\u003Cul\u003E\n\u003Cli dir=\"ltr\" style=\"font-size: 18px;\"\u003ECloud specialization means a team proficient in AWS might struggle with Azure nuances. Training becomes essential, but it&rsquo;s also time-consuming and expensive.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch2 dir=\"ltr\" style=\"font-size: 20px;\"\u003E\u003Cstrong\u003E4. Strategies for Seamless Multi-Cloud Platform Engineering\u003C\u002Fstrong\u003E\u003C\u002Fh2\u003E\n\u003Ch3 dir=\"ltr\" style=\"font-size: 18px;\"\u003Ea. Unified Management Tools\u003C\u002Fh3\u003E\n\u003Cul\u003E\n\u003Cli dir=\"ltr\"\u003ETerraform: An open-source tool that allows you to define and provision infrastructure across multiple providers using a simple declarative configuration language.\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\"\u003EAnsible: A powerful automation tool that can handle application deployment and configuration management across varied cloud infrastructures.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch3 dir=\"ltr\" style=\"font-size: 18px;\"\u003Eb. Containerization and Kubernetes\u003C\u002Fh3\u003E\n\u003Cul\u003E\n\u003Cli dir=\"ltr\"\u003EContainers, with tools like Docker, provide a consistent environment for applications, making them cloud-agnostic.\u003C\u002Fli\u003E\n\u003Cli dir=\"ltr\"\u003EKubernetes takes this a step further, offering orchestration and scaling of containerized applications, irrespective of the cloud provider.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch3 dir=\"ltr\" style=\"font-size: 18px;\"\u003Ec. Cloud Agnostic Architecture\u003C\u002Fh3\u003E\n\u003Cul\u003E\n\u003Cli dir=\"ltr\"\u003EDesigning applications to be cloud-neutral is paramount. Utilize open-source tools and ensure minimal dependency on services exclusive to a single provider.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch3 dir=\"ltr\" style=\"font-size: 18px;\"\u003Ed. Consistent Security and Compliance Frameworks\u003C\u002Fh3\u003E\n\u003Cul\u003E\n\u003Cli dir=\"ltr\"\u003EA unified security posture across \u003Ca href=\"https:\u002F\u002Fplatformengineers.io\u002Fservices\u002Fplatform-engineering\" target=\"_blank\" rel=\"noopener\"\u003Eplatforms is crucial\u003C\u002Fa\u003E. Tools like Prisma Cloud can monitor and manage security consistently, sending alerts for any discrepancies.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch3 dir=\"ltr\" style=\"font-size: 18px;\"\u003Ee. Centralized Monitoring and Logging\u003C\u002Fh3\u003E\n\u003Cul\u003E\n\u003Cli dir=\"ltr\"\u003EGrafana: An open-source platform for monitoring and observability, it can seamlessly integrate with various data sources, giving a unified performance overview.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp dir=\"ltr\"\u003EFor SaaS startups and enterprises, the transition to a multi-cloud world might seem daunting. But, with a judicious mix of strategy, tools, and continuous learning, the rewards are significant. In this intricate dance across the cloud ecosystem, always keep your end-user experience as the focal point.\u003C\u002Fp\u003E\n\u003Cp dir=\"ltr\"\u003EYour journey through the multi-cloud expanse holds valuable insights. Share your experiences and challenges, and let&rsquo;s learn together. For those wanting to delve even deeper, visit us at PlatformEngineers.io\u003C\u002Fp\u003E",tags:[al,bn,"idp","multi-cloud","devops",bj,"Cloud platform engineering","platform engineering consulting"],time_to_read:"5 min",user_created:{id:A,first_name:t,last_name:u,email:K,password:b,location:a,title:a,description:a,tags:a,avatar:v,language:B,tfa_secret:a,status:c,role:j,token:a,last_access:L,last_page:M,provider:d,external_identifier:a,auth_data:a,email_notifications:e,appearance:a,theme_dark:a,theme_light:a,theme_light_overrides:a,theme_dark_overrides:a},user_updated:{id:bo,first_name:"Jinal",last_name:"Gajjar",email:"jinal.gajjar@improwised.com",password:b,location:a,title:a,description:a,tags:a,avatar:"a879a692-b6c3-4d84-8905-84ea51716508",language:a,tfa_secret:a,status:c,role:j,token:a,last_access:"2024-05-27T08:18:14.151Z",last_page:"\u002Fcontent\u002Fblog\u002F15",provider:d,external_identifier:a,auth_data:a,email_notifications:e,appearance:a,theme_dark:a,theme_light:a,theme_light_overrides:a,theme_dark_overrides:a},image:{id:"c04b74db-fd02-4019-881a-d6cfc685ebd1",storage:p,filename_disk:"c04b74db-fd02-4019-881a-d6cfc685ebd1.webp",filename_download:"blog 1 (1).webp",title:"Blog 1 (1)",type:W,folder:q,uploaded_by:bo,created_on:bp,modified_by:a,modified_on:"2024-05-20T06:47:48.132Z",charset:a,filesize:"54922",width:N,height:N,duration:a,embed:a,description:a,location:a,tags:a,metadata:{},focal_point_x:a,focal_point_y:a,tus_id:a,tus_data:a,uploaded_on:bp},authors:[{id:ab,pe_blog_id:$,directus_users_id:{first_name:t,last_name:u,avatar:v}}]},{id:ad,status:o,sort:ad,date_created:"2023-07-28T10:53:07.060Z",date_updated:"2024-04-16T06:35:42.842Z",slug:"internal-developer-platform",title:"From Code to Cloud: How Platform Engineers Craft the IDP Experience",description:"\u003Cp\u003EDo you ever worry that your team is making mistakes because they're spending too much time wrestling with infrastructure and taking too much time building great applications?&nbsp;\u003Cbr \u002F\u003EHave you ever felt like you're constantly playing catch-up in the world of software development?\u003C\u002Fp\u003E\n\u003Cp\u003EIf you answered yes to either of these questions, then you're not alone. Many organizations are facing the same challenges.\u003C\u002Fp\u003E\n\u003Cp\u003E&nbsp;\u003C\u002Fp\u003E",seo_title:"Internal Developer Platform (IDP)| Platform Engineers",seo_description:"Unleash dev potential! Discover how an IDP streamlines workflows, reduces errors, & accelerates delivery in today's fast-paced software world.",content:"\u003Ch2\u003E\u003Cstrong\u003E\u003Cspan style=\"font-size: 22px;\"\u003EThe Need for an Internal Developer Platform:\u003C\u002Fspan\u003E\u003C\u002Fstrong\u003E\u003C\u002Fh2\u003E\n\u003Cp\u003EIn the world of \u003Ca href=\"https:\u002F\u002Fplatformengineers.io\u002Fblog\u002Fevolving-from-software-to-platform-engineering\u002F\" target=\"_blank\" rel=\"noopener\"\u003Esoftware development\u003C\u002Fa\u003E, where the pace of change is ever-accelerating, organizations are under increasing pressure to deliver new applications faster and with fewer errors. This is especially true in industries where competition is fierce and customer expectations are high.\u003C\u002Fp\u003E\n\u003Cp\u003EAn Internal Developer Platform (IDP) can help organizations address these challenges by providing developers with the tools and infrastructure they need to be more productive. An IDP is a set of integrated tools, services, and processes that automate and streamline the development process. This can free up developers to focus on what they do best: \u003Ca href=\"https:\u002F\u002Fplatformengineers.io\u002Fservices\u002Fplatform-engineering\u002F\" target=\"_blank\" rel=\"noopener\"\u003Ebuilding great applications\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\n\u003Ch2 style=\"font-size: 22px;\"\u003E\u003Cstrong\u003EBenefits of Using an IDP:\u003C\u002Fstrong\u003E\u003C\u002Fh2\u003E\n\u003Cp\u003EAn IDP offers numerous benefits, including:\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003E\u003Cstrong\u003EIncreased productivity:\u003C\u002Fstrong\u003E Streamlining repetitive tasks and offering access to a centralized repository of tools and services can significantly boost developers' productivity.\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Cstrong\u003EReduced errors:\u003C\u002Fstrong\u003E Ensuring a consistent development environment and enforcing coding standards can effectively minimize errors in the development process.\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Cstrong\u003EImproved compliance\u003C\u002Fstrong\u003E: With a secure development environment and change tracking capabilities, organizations can better adhere to security and regulatory standards.\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Cstrong\u003EIncreased agility:\u003C\u002Fstrong\u003E An IDP facilitates swift deployment of new applications and updates, enabling organizations to respond rapidly to changing market demands.\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Cstrong\u003EReduced costs: \u003C\u002Fstrong\u003EThrough task automation and centralized resource management, IDPs contribute to \u003Ca href=\"https:\u002F\u002Fplatformengineers.io\u002Fassessment\u002F\" target=\"_blank\" rel=\"noopener\"\u003Ecost savings in the development\u003C\u002Fa\u003E workflow.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch3\u003E\u003Cstrong\u003ETechnical Details of an IDP:\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp\u003EAn IDP typically includes the following components:\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003ETooling:\u003C\u002Fstrong\u003E An IDP provides developers with access to a wide range of tools, including IDEs, build tools, testing tools, and deployment tools.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EInfrastructure: \u003C\u002Fstrong\u003EAn IDP provides developers with access to a variety of \u003Ca href=\"https:\u002F\u002Fplatformengineers.io\u002Fservices\u002Finfrastructure-maintenance-and-support\u002F\" target=\"_blank\" rel=\"noopener\"\u003Einfrastructure resources,\u003C\u002Fa\u003E such as compute, storage, and networking.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EProcesses: \u003C\u002Fstrong\u003EAn IDP provides developers with a set of processes for managing the development lifecycle, such as requirements gathering, design, development, testing, and deployment.\u003C\u002Fp\u003E\n\u003Cp\u003EThe specific components of an IDP will vary depending on the organization's needs. However, all IDPs should provide developers with the tools and infrastructure they need to be productive.\u003C\u002Fp\u003E\n\u003Ch3\u003E\u003Cstrong\u003EHow to Build an IDP:\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp\u003EThere are two main ways to build an IDP:\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EBuild it yourself: \u003C\u002Fstrong\u003EThis option involves building the IDP from scratch using open-source tools and frameworks.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EUse a commercial IDP:\u003C\u002Fstrong\u003E This option involves purchasing a pre-built and ready-to-use \u003Ca href=\"https:\u002F\u002Fplatformengineers.io\u002Findustries\u002Fecommerce\u002F\" target=\"_blank\" rel=\"noopener\"\u003Ecommercial\u003C\u002Fa\u003E IDP.\u003C\u002Fp\u003E\n\u003Cp\u003EThe best option for you will depend on your organization's specific needs and budget. If you have the resources and expertise, building your own IDP can be a good option. However, if you are short on time or resources, using a commercial IDP may be a better choice.\u003C\u002Fp\u003E\n\u003Ch3\u003E\u003Cstrong\u003EConclusion:\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp\u003EAn IDP can be a valuable asset for any organization that wants to improve its development process. By providing developers with the tools and infrastructure they need, IDPs can help organizations improve productivity, reduce risk, and ensure compliance.\u003C\u002Fp\u003E\n\u003Cp\u003ERemember, embracing an Internal Developer Platform could be the game-changer your organization needs to thrive in the fast-paced world of \u003Ca href=\"https:\u002F\u002Fplatformengineers.io\u002Findustries\u002FSaaS\u002F\" target=\"_blank\" rel=\"noopener\"\u003Esoftware development\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\n\u003Cp\u003E&nbsp;\u003C\u002Fp\u003E",tags:[bn,al,am,aw,"infrastructure","cloud","platform Engineers","Cloud Platform Engineers"],time_to_read:bk,user_created:{id:A,first_name:t,last_name:u,email:K,password:b,location:a,title:a,description:a,tags:a,avatar:v,language:B,tfa_secret:a,status:c,role:j,token:a,last_access:L,last_page:M,provider:d,external_identifier:a,auth_data:a,email_notifications:e,appearance:a,theme_dark:a,theme_light:a,theme_light_overrides:a,theme_dark_overrides:a},user_updated:{id:s,first_name:C,last_name:a,email:D,password:b,location:a,title:a,description:a,tags:a,avatar:E,language:a,tfa_secret:a,status:c,role:F,token:a,last_access:G,last_page:H,provider:d,external_identifier:a,auth_data:a,email_notifications:e,appearance:a,theme_dark:a,theme_light:a,theme_light_overrides:a,theme_dark_overrides:a},image:{id:"b3198b1a-dc3d-42c2-b019-5c5b6492347e",storage:p,filename_disk:"b3198b1a-dc3d-42c2-b019-5c5b6492347e.png",filename_download:"2584948-ai-modified (1).png",title:"2584948 Ai Modified (1)",type:r,folder:q,uploaded_by:s,created_on:bq,modified_by:a,modified_on:"2024-02-14T10:19:25.362Z",charset:a,filesize:"50561",width:N,height:N,duration:a,embed:a,description:a,location:a,tags:a,metadata:{},focal_point_x:a,focal_point_y:a,tus_id:a,tus_data:a,uploaded_on:bq},authors:[{id:ae,pe_blog_id:ad,directus_users_id:{first_name:t,last_name:u,avatar:v}}]},{id:ac,status:o,sort:ac,date_created:"2023-07-19T15:18:17.757Z",date_updated:"2024-04-16T06:35:26.617Z",slug:"platform-engineering-with-git-ops",title:"Platform Engineering with GitOps: A Modern Approach to Infrastructure Management",description:"\u003Cp\u003E\u003Cspan style=\"font-weight: 400;\"\u003EPlatform engineering is the discipline of building and managing the infrastructure that developers use to build and deploy applications. It is a complex and challenging task, but GitOps can help to make it easier.\u003C\u002Fspan\u003E\u003C\u002Fp\u003E",seo_title:"Manage Infra As Code: GitOps for Platform Engineers",seo_description:"GitOps can revolutionize platform engineering workflow. GitOps empowers automation, visibility, and security, leading to efficient & reliable infrastructure management.",content:"\u003Cp\u003E\u003Cspan style=\"font-weight: 400;\"\u003EPlatform engineering is the discipline of building and managing the infrastructure that developers use to build and deploy applications. It is a complex and challenging task, but GitOps can help to make it easier.\u003C\u002Fspan\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cspan style=\"font-weight: 400;\"\u003EGitOps is a set of principles that use Git as the single source of truth for managing infrastructure. This means that all changes to infrastructure are made through Git, and the state of infrastructure is always represented by the latest commit in the \u003Ca href=\"https:\u002F\u002Fgithub.com\u002F\" target=\"_blank\" rel=\"noopener\"\u003EGit repository\u003C\u002Fa\u003E.\u003C\u002Fspan\u003E\u003C\u002Fp\u003E\n\u003Ch2\u003E\u003Cspan style=\"font-weight: 400; font-size: 22px;\"\u003EThere are many benefits to using GitOps for platform engineering.&nbsp;\u003C\u002Fspan\u003E\u003C\u002Fh2\u003E\n\u003Ch3\u003E\u003Cspan style=\"font-weight: 400; font-size: 18px;\"\u003EHere are a few of the most important:\u003C\u002Fspan\u003E\u003C\u002Fh3\u003E\n\u003Cp\u003E\u003Cspan style=\"font-weight: 400;\"\u003E\u003Cstrong\u003EImproved collaboration: \u003C\u002Fstrong\u003EGitOps can help to improve collaboration between developers and operations teams. By using Git as the single source of truth, both teams can see the same information and work together to make changes to infrastructure. This can help to reduce the risk of errors and ensure that changes are made in a consistent and repeatable way.\u003C\u002Fspan\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cspan style=\"font-weight: 400;\"\u003E\u003Cstrong\u003EAutomated infrastructure provisioning and deployment: \u003C\u002Fstrong\u003EGitOps can be used to automate the process of provisioning infrastructure, such as \u003Ca href=\"https:\u002F\u002Fplatformengineers.io\u002Fservices\u002Fkubernetes-consulting-services\" target=\"_blank\" rel=\"noopener\"\u003EKubernetes clusters\u003C\u002Fa\u003E, and deploying applications. This can help to reduce the amount of manual work required to manage infrastructure, and it can help to ensure that infrastructure is always up-to-date.\u003C\u002Fspan\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cspan style=\"font-weight: 400;\"\u003E\u003Cstrong\u003EImproved visibility and auditing:\u003C\u002Fstrong\u003E By using Git as the single source of truth, it is easy to see who made changes to infrastructure and when those changes were made. This can help to improve visibility into infrastructure changes, and it can make it easier to troubleshoot problems.By incorporating Pull Request-based code reviews, treating \u003Ca href=\"https:\u002F\u002Fplatformengineers.io\u002Fassessment\" target=\"_blank\" rel=\"noopener\"\u003Einfrastructure changes\u003C\u002Fa\u003E just like code, organizations can enhance visibility and auditing capabilities.&nbsp;\u003C\u002Fspan\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cspan style=\"font-weight: 400;\"\u003E\u003Cstrong\u003EImproved security:\u003C\u002Fstrong\u003E GitOps can be used to enforce security policies by requiring that all changes to infrastructure be made through Git. This can help to ensure that only authorized users can make changes to infrastructure, and it can help to prevent unauthorized changes from being made.\u003C\u002Fspan\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cspan style=\"font-weight: 400;\"\u003EOverall, GitOps can be a valuable tool for platform engineering teams. It can help to improve collaboration, automation, visibility, auditing, and security.\u003C\u002Fspan\u003E\u003C\u002Fp\u003E\n\u003Ch3\u003E\u003Cspan style=\"font-weight: 400;\"\u003EHere are some specific examples of how GitOps can be used in platform engineering:\u003C\u002Fspan\u003E\u003C\u002Fh3\u003E\n\u003Cp\u003E\u003Cspan style=\"font-weight: 400;\"\u003E\u003Cstrong\u003EProvisioning Kubernetes clusters: \u003C\u002Fstrong\u003EGitOps can be used to automate the process of provisioning Kubernetes clusters. This can be done by creating a Git repository that contains the configuration files for the Kubernetes clusters. When a new cluster is needed, the configuration files can be committed to the Git repository, and the cluster will be provisioned automatically.\u003C\u002Fspan\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cspan style=\"font-weight: 400;\"\u003E\u003Cstrong\u003EDeploying applications: \u003C\u002Fstrong\u003EGitOps can also be used to automate the deployment of applications to Kubernetes clusters. This can be done by creating a Git repository that contains the \u003Ca href=\"https:\u002F\u002Fplatformengineers.io\u002Fservices\u002Fci-cd-implementation\u002F\" target=\"_blank\" rel=\"noopener\"\u003Edeployment manifests\u003C\u002Fa\u003E for the applications. When a new application is deployed, the deployment manifests can be committed to the Git repository, and the application will be deployed automatically.\u003C\u002Fspan\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cspan style=\"font-weight: 400;\"\u003E\u003Cstrong\u003EManaging infrastructure changes: \u003C\u002Fstrong\u003EGitOps can also be used to manage infrastructure changes. This can be done by creating a Git repository that contains the history of all infrastructure changes. This history can be used to track changes to infrastructure, and it can be used to roll back changes if necessary.\u003C\u002Fspan\u003E\u003C\u002Fp\u003E\n\u003Cp\u003EIn conclusion, GitOps is a valuable tool for platform engineering teams that can help to improve collaboration, automation, visibility, auditing, and security. By using Git as the single source of truth for managing infrastructure, GitOps can help to simplify the complex task of \u003Ca href=\"https:\u002F\u002Fplatformengineers.io\u002Fservices\u002Fplatform-engineering\u002F\" target=\"_blank\" rel=\"noopener\"\u003Eplatform engineering\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\n\u003Cp\u003E&nbsp;\u003C\u002Fp\u003E",tags:["GitOps",T,am,br,"Infrastructure Automation","CI\u002FCD"],time_to_read:"3 min ",user_created:{id:A,first_name:t,last_name:u,email:K,password:b,location:a,title:a,description:a,tags:a,avatar:v,language:B,tfa_secret:a,status:c,role:j,token:a,last_access:L,last_page:M,provider:d,external_identifier:a,auth_data:a,email_notifications:e,appearance:a,theme_dark:a,theme_light:a,theme_light_overrides:a,theme_dark_overrides:a},user_updated:{id:s,first_name:C,last_name:a,email:D,password:b,location:a,title:a,description:a,tags:a,avatar:E,language:a,tfa_secret:a,status:c,role:F,token:a,last_access:G,last_page:H,provider:d,external_identifier:a,auth_data:a,email_notifications:e,appearance:a,theme_dark:a,theme_light:a,theme_light_overrides:a,theme_dark_overrides:a},image:{id:"814f2519-bb85-42ed-a767-3ad6bc300ba9",storage:p,filename_disk:"814f2519-bb85-42ed-a767-3ad6bc300ba9.png",filename_download:"GitOps-modified (1).png",title:"Git Ops Modified (1)",type:r,folder:q,uploaded_by:s,created_on:bs,modified_by:a,modified_on:"2024-02-14T10:20:58.086Z",charset:a,filesize:"67160",width:bt,height:bt,duration:a,embed:a,description:a,location:a,tags:a,metadata:{},focal_point_x:a,focal_point_y:a,tus_id:a,tus_data:a,uploaded_on:bs},authors:[{id:aZ,pe_blog_id:ac,directus_users_id:{first_name:t,last_name:u,avatar:v}}]},{id:aA,status:o,sort:aA,date_created:"2023-04-20T04:38:48.000Z",date_updated:"2024-05-03T06:05:13.805Z",slug:"evolving-from-software-to-platform-engineering",title:"Evolving from Software to Platform Engineering: Our Journey and Why It Matters",description:"\u003Cp\u003E\u003Cspan style=\"font-weight: 400;\"\u003EIn today's fast-paced digital world, businesses rely on software applications that can handle millions of requests per second, provide real-time analytics, and seamlessly integrate with third-party systems. However, having a software application alone is not enough to cater to the constantly evolving needs of businesses and their customers. A robust and scalable platform is required to support the software application's development and deployment.\u003C\u002Fspan\u003E\u003C\u002Fp\u003E",seo_title:"Evolving from Software to Platform Engineering",seo_description:"Unleash Developer Productivity with Platform Engineering: Transformed from software development to empower teams with a powerful internal platform. ",content:"\u003Cp\u003E\u003Cspan style=\"font-weight: 400;\"\u003EIn today's fast-paced digital world, businesses rely on software applications that can handle millions of requests per second, provide real-time analytics, and seamlessly integrate with third-party systems. However, having a software application alone is not enough to cater to the constantly evolving needs of businesses and their customers. A robust and scalable platform is required to support the software \u003Ca href=\"https:\u002F\u002Fplatformengineers.io\u002Fservices\u002Fci-cd-implementation\u002F\" target=\"_blank\" rel=\"noopener\"\u003Eapplication's development and deployment\u003C\u002Fa\u003E.\u003C\u002Fspan\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cspan style=\"font-weight: 400;\"\u003EThis is where platform engineering comes into play. Platform engineering is the process of creating tools and workflows that allow software engineering teams to work more efficiently in the cloud-based environment. So a set of tools, workflows, and services that are integrated together to provide a unified platform referred as an \"Internal Developer Platform\" which helps developers with the entire lifecycle of an application.&nbsp;\u003C\u002Fspan\u003E\u003C\u002Fp\u003E\n\u003Cblockquote\u003E\n\u003Cp\u003E\u003Cem\u003E\u003Cstrong\u003E&ldquo;Empowering developers to focus on innovation, not infrastructure, with an Internal Developer Platform&rdquo;\u003C\u002Fstrong\u003E\u003C\u002Fem\u003E\u003C\u002Fp\u003E\n\u003C\u002Fblockquote\u003E\n\u003Cp\u003E\u003Cspan style=\"font-weight: 400;\"\u003EThe Internal Developer Platform is a layer on top of existing technology and tools that helps operations set up and allows developers to help themselves. The goal is to make the development process easier by providing clear instructions and easy-to-use tools that match the\u003Ca href=\"https:\u002F\u002Fplatformengineers.io\u002Fblog\u002Finternal-developer-platform\u002F\" target=\"_blank\" rel=\"noopener\"\u003E individual developer's\u003C\u002Fa\u003E preferred level of complexity.\u003C\u002Fspan\u003E\u003C\u002Fp\u003E\n\u003Ch2 style=\"font-size: 22px;\"\u003E\u003Cstrong\u003EWhy \u003Ca href=\"https:\u002F\u002Fimprowised.com\" target=\"_blank\" rel=\"noopener\"\u003EImprowised Technologies \u003C\u002Fa\u003E&nbsp;narrowed its focus to platform engineering\u003C\u002Fstrong\u003E\u003C\u002Fh2\u003E\n\u003Cp\u003E\u003Cspan style=\"font-weight: 400;\"\u003EImprowised Technologies has been providing software engineering services from 2011, including cloud native web application development, technology consulting, product modernization and cloud infrastructure management services. Our software engineering services are still ongoing, and we have discovered a niche in infrastructure management services, which has led to the launch of PlatformEngineers.io. We have gained a deep understanding of the challenges that businesses face when building and deploying complex software applications. We also realized that many of these challenges could be addressed by focusing on platform engineering.\u003C\u002Fspan\u003E\u003C\u002Fp\u003E\n\u003Cblockquote\u003E\n\u003Cp\u003E\u003Cstrong\u003E&ldquo;Unlock the full potential of your software applications with PlatformEngineers.io's expert platform engineering services&rdquo;&nbsp;\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003C\u002Fblockquote\u003E\n\u003Cp\u003E\u003Cspan style=\"font-weight: 400;\"\u003EPlatform engineering involves designing, building, and managing the infrastructure and tools that support software applications. By narrowing our focus to \u003Ca href=\"https:\u002F\u002Fplatformengineers.io\u002Fservices\u002Fplatform-engineering\u002F\" target=\"_blank\" rel=\"noopener\"\u003Eplatform engineering\u003C\u002Fa\u003E, we can help our clients build more reliable, scalable, and secure platforms that can support their business objectives.\u003C\u002Fspan\u003E\u003C\u002Fp\u003E\n\u003Ch3\u003E\u003Cspan style=\"font-weight: 400; font-size: 22px;\"\u003E\u003Cstrong\u003EThe importance of platform engineering in today's world\u003C\u002Fstrong\u003E\u003C\u002Fspan\u003E\u003C\u002Fh3\u003E\n\u003Cp\u003E\u003Cspan style=\"font-weight: 400;\"\u003EIn today's digital world, businesses are competing on a global scale, and the success of any business largely depends on its ability to provide reliable and scalable services to its customers. As a result, platform engineering has become a critical aspect of software development and deployment. Here are some reasons why platform engineering is essential in today's digital world:\u003C\u002Fspan\u003E\u003C\u002Fp\u003E\n\u003Ch3 style=\"font-size: 16px;\"\u003E\u003Cstrong\u003EScalability\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp\u003E\u003Cspan style=\"font-weight: 400;\"\u003EScalability is crucial for businesses that want to grow and expand their operations. Platform engineering ensures that the platform can scale seamlessly and provide the necessary resources to support the application's growth. A platform that can scale horizontally or vertically can handle increased traffic, data, and users without any degradation in performance.&nbsp;\u003C\u002Fspan\u003E\u003C\u002Fp\u003E\n\u003Ch3 style=\"font-size: 16px;\"\u003E\u003Cstrong\u003EReliability\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp\u003E\u003Cspan style=\"font-weight: 400;\"\u003EIn today's digital world, downtime is not an option. A reliable platform can guarantee high availability and ensure that the application is always up and running, and can handle any failures or outages without any impact on the application's availability.\u003C\u002Fspan\u003E\u003C\u002Fp\u003E\n\u003Ch3 style=\"font-size: 16px;\"\u003E\u003Cstrong\u003ESecurity\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp\u003E\u003Cspan style=\"font-weight: 400;\"\u003ESecurity is a top priority for businesses that want to protect their customers' data and intellectual property. A secure platform can prevent unauthorized access, data breaches, and cyber-attacks. Platform engineering ensures that the platform is designed with security in mind and can handle any security threats and vulnerabilities.\u003C\u002Fspan\u003E\u003C\u002Fp\u003E\n\u003Ch3 style=\"font-size: 16px;\"\u003E\u003Cstrong\u003EIntegration\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp\u003E\u003Cspan style=\"font-weight: 400;\"\u003EIntegration is critical for businesses that want to leverage third-party services, APIs, and data sources. The use of Platform engineering ensures that the platform can integrate with other services and APIs and provide a seamless user experience.\u003C\u002Fspan\u003E\u003C\u002Fp\u003E\n\u003Ch3\u003E\u003Cstrong\u003EPrevious Projects in Platform Engineering at Improwised Technologies\u003C\u002Fstrong\u003E\u003C\u002Fh3\u003E\n\u003Cp\u003E\u003Cspan style=\"font-weight: 400;\"\u003EAt Improwised Technologies, we have extensive experience in platform engineering, having worked on numerous projects for clients across various industries. Here is an example of our past work in platform engineering:\u003C\u002Fspan\u003E\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli style=\"font-weight: 400;\" aria-level=\"1\"\u003E\u003Cspan style=\"font-weight: 400;\"\u003EWe recently assisted a collaborative social media platform with a large user base, which was facing issues related to \u003Ca href=\"https:\u002F\u002Fplatformengineers.io\u002Fassessment\u002F\" target=\"_blank\" rel=\"noopener\"\u003Ecost and performance\u003C\u002Fa\u003E due to its complex infrastructure. \u003C\u002Fspan\u003E\u003C\u002Fli\u003E\n\u003Cli style=\"font-weight: 400;\" aria-level=\"1\"\u003E\u003Cspan style=\"font-weight: 400;\"\u003EOur platform engineering team addressed this problem by simplifying its infrastructure and automating its deployment process. To achieve this, we migrated the application to a single self-managed \u003Ca href=\"https:\u002F\u002Fplatformengineers.io\u002Fservices\u002Fkubernetes-consulting-services\u002F\" target=\"_blank\" rel=\"noopener\"\u003EKubernetes cluster\u003C\u002Fa\u003E, consolidated the environments, and implemented (CI\u002FCD) processes. \u003C\u002Fspan\u003E\u003C\u002Fli\u003E\n\u003Cli style=\"font-weight: 400;\" aria-level=\"1\"\u003E\u003Cspan style=\"font-weight: 400;\"\u003EAs a result of these efforts, the platform's monthly recurring cost was reduced by 70%. We were also able to streamline the infrastructure through automation and ensure zero downtime during the migration process.&nbsp;\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp\u003E\u003Cspan style=\"font-weight: 400;\"\u003EFor a more detailed case study on this project, please visit our case study - \u003Ca href=\"https:\u002F\u002Fplatformengineers.in\u002Fcase-studies\u002Ffrom-complexity-to-simplicity-case-study-of-infrastructure-consolidation\u002F\" target=\"_self\"\u003EFrom Complexity to Simplicity: A Cost Reduction Case Study through Infrastructure Consolidation and Automation for a Collaborative Social Media Platform\u003C\u002Fa\u003E\u003C\u002Fspan\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cbr\u003E\u003Cbr\u003E\u003Cspan style=\"font-weight: 400;\"\u003EImprowised Technologies has a successful history of providing top-notch software engineering services to clients in different industries since 2011. Our software engineering services are still ongoing, and we have discovered a niche in infrastructure management services, which has led to the launch of PlatformEngineers.io. Our proficiency in \u003Ca href=\"https:\u002F\u002Fplatformengineers.io\u002Fservices\u002Finfrastructure-maintenance-and-support\u002F\" target=\"_blank\" rel=\"noopener\"\u003Einfrastructure management\u003C\u002Fa\u003E services, coupled with our dedication to platform engineering, enables us to create tailored solutions that address the unique needs and goals of our clients. If you need an experienced team to assist you with your platform engineering requirements, you may want to consider collaborating with PlatformEngineers.io.\u003C\u002Fspan\u003E\u003C\u002Fp\u003E",tags:[al,am,br],time_to_read:"5 min ",user_created:{id:A,first_name:t,last_name:u,email:K,password:b,location:a,title:a,description:a,tags:a,avatar:v,language:B,tfa_secret:a,status:c,role:j,token:a,last_access:L,last_page:M,provider:d,external_identifier:a,auth_data:a,email_notifications:e,appearance:a,theme_dark:a,theme_light:a,theme_light_overrides:a,theme_dark_overrides:a},user_updated:{id:f,first_name:g,last_name:h,email:k,password:b,location:a,title:l,description:a,tags:a,avatar:i,language:a,tfa_secret:a,status:c,role:j,token:a,last_access:m,last_page:n,provider:d,external_identifier:a,auth_data:a,email_notifications:e,appearance:a,theme_dark:a,theme_light:a,theme_light_overrides:a,theme_dark_overrides:a},image:{id:"b1e34ced-c1d6-4113-8f9c-a0934db8b77e",storage:p,filename_disk:"b1e34ced-c1d6-4113-8f9c-a0934db8b77e.png",filename_download:"evolution (1).png",title:"Evolution (1)",type:r,folder:q,uploaded_by:s,created_on:bu,modified_by:a,modified_on:"2024-02-14T10:21:34.547Z",charset:a,filesize:"22641",width:bv,height:bv,duration:a,embed:a,description:a,location:a,tags:a,metadata:{},focal_point_x:a,focal_point_y:a,tus_id:a,tus_data:a,uploaded_on:bu},authors:[{id:ax,pe_blog_id:aA,directus_users_id:{first_name:t,last_name:u,avatar:v}}]}],_img:{"/_ipx/f_png/img/plateform-engineers.png":"\u002F_nuxt\u002Fimage\u002Fe7b705.png","/_ipx/f_png,s_68x55/img/plateform-engineers.png":"\u002F_nuxt\u002Fimage\u002F418082.png","/_ipx/h_400,f_png/img/blog.webp":"\u002F_nuxt\u002Fimage\u002Fe4bcc8.png","/_ipx/s_300x300/https://data.improwised.com/assets/300ef007-838f-4613-99e0-ab20b9ac6442":"\u002F_nuxt\u002Fimage\u002F64f307[ext]","/_ipx/s_300x300/https://data.improwised.com/assets/42f5d362-2eb5-44aa-8586-4e7bef5c211c":"\u002F_nuxt\u002Fimage\u002Fd3ba83[ext]","/_ipx/s_300x300/https://data.improwised.com/assets/2d44b1fa-25d3-48ef-9f31-f4f04000e6ce":"\u002F_nuxt\u002Fimage\u002F0d10c1[ext]","/_ipx/s_300x300/https://data.improwised.com/assets/b9c66e4c-b98e-4cd2-bb7e-05a10829e476":"\u002F_nuxt\u002Fimage\u002F6eee48[ext]","/_ipx/s_300x300/https://data.improwised.com/assets/da202f75-7671-4a1d-afc0-2f9852f8c6ee":"\u002F_nuxt\u002Fimage\u002F3e3a8f[ext]","/_ipx/s_300x300/https://data.improwised.com/assets/97e4ca74-ff1d-4859-8948-bf0f7152f44a":"\u002F_nuxt\u002Fimage\u002Fb69bcd[ext]","/_ipx/s_300x300/https://data.improwised.com/assets/84fa5dbb-6e76-4e31-8a69-127bd1d7fa58":"\u002F_nuxt\u002Fimage\u002F16dbae[ext]","/_ipx/s_300x300/https://data.improwised.com/assets/fd790772-386c-4d31-baa3-1bdeb3914fe6":"\u002F_nuxt\u002Fimage\u002Fb16aa8[ext]","/_ipx/s_300x300/https://data.improwised.com/assets/0736c072-8ebd-43ea-b349-b2132a79ac68":"\u002F_nuxt\u002Fimage\u002F58f3ca[ext]","/_ipx/s_300x300/https://data.improwised.com/assets/e3afc165-f038-4861-8824-66f9d983ec75":"\u002F_nuxt\u002Fimage\u002Ffe351f[ext]","/_ipx/s_300x300/https://data.improwised.com/assets/b962570e-21d3-4298-b6c9-eea9aefd4ace":"\u002F_nuxt\u002Fimage\u002Fe99d34[ext]","/_ipx/s_300x300/https://data.improwised.com/assets/2b05b0ee-dcdf-4d76-b3d9-def3315d280d":"\u002F_nuxt\u002Fimage\u002F0c851e[ext]","/_ipx/s_300x300/https://data.improwised.com/assets/a8653dea-0248-4787-8d52-063c8a6d850f":"\u002F_nuxt\u002Fimage\u002F7ceb63[ext]","/_ipx/s_300x300/https://data.improwised.com/assets/515ffe7a-5658-4bbd-a49d-459366019c72":"\u002F_nuxt\u002Fimage\u002F8e7a46[ext]","/_ipx/s_300x300/https://data.improwised.com/assets/23cd1138-a9ab-4007-beac-00f0330d2d08":"\u002F_nuxt\u002Fimage\u002Fb61fbc[ext]","/_ipx/s_300x300/https://data.improwised.com/assets/fabfce7f-6380-4132-b6f1-8232a386e3cc":"\u002F_nuxt\u002Fimage\u002F54ecb3[ext]","/_ipx/s_300x300/https://data.improwised.com/assets/037faa8b-fc0c-4585-83ed-648ead7ef34a":"\u002F_nuxt\u002Fimage\u002Fe1f573[ext]","/_ipx/s_300x300/https://data.improwised.com/assets/8c141490-592d-44e9-af15-f62f90bf32d1":"\u002F_nuxt\u002Fimage\u002F8c8afc[ext]","/_ipx/s_300x300/https://data.improwised.com/assets/43794616-8a42-4819-aa70-3209f8c3921c":"\u002F_nuxt\u002Fimage\u002F6f3bd3[ext]","/_ipx/s_300x300/https://data.improwised.com/assets/c04b74db-fd02-4019-881a-d6cfc685ebd1":"\u002F_nuxt\u002Fimage\u002Fe7b056[ext]","/_ipx/s_300x300/https://data.improwised.com/assets/b3198b1a-dc3d-42c2-b019-5c5b6492347e":"\u002F_nuxt\u002Fimage\u002F8db15b[ext]","/_ipx/s_300x300/https://data.improwised.com/assets/814f2519-bb85-42ed-a767-3ad6bc300ba9":"\u002F_nuxt\u002Fimage\u002F4fc7b6[ext]","/_ipx/s_300x300/https://data.improwised.com/assets/b1e34ced-c1d6-4113-8f9c-a0934db8b77e":"\u002F_nuxt\u002Fimage\u002Fc8975f[ext]","/_ipx/f_png,h_400/img/blog-bk-1.png":"\u002F_nuxt\u002Fimage\u002F9aec28.png"}}],fetch:{},mutations:[]}}(null,"**********","active","default",true,"f6ae4b64-c3c4-4f35-8b41-9f48088de4b1","Angita","Shah","20d037d1-41ee-4efd-b034-1350a3ce336d","5ef170ac-f2e9-4b93-a9ea-5c54fcf0fa40","angita.shah@improwised.com","SEO Specialist","2024-10-07T08:08:00.249Z","\u002Fcontent\u002Fpe_blog","published","AMZ","33b4de25-d0f2-4999-ba32-6883f9932c34","image\u002Fpng","ff10edca-7a1e-4fc2-8434-149a8830b755","Priyank","Dhami","d3f832d0-e7dc-42e5-8af7-7dd67d9ca481","5 mins","Satish","Annavar ","33d28adb-eb7a-47f1-adc1-2233c7f06711","37676ce8-2599-4132-9149-a05484760a76","en-US","payal","payal@improwised.com","11d7eec7-3ba5-40bc-bb42-1f76008c7a87","9d68ca39-0460-48bc-a533-6b999e303740","2024-07-09T13:07:32.442Z","\u002Ffiles\u002Ffolders\u002F6b91655c-b836-4886-93bf-d61814af3fb7",500,1190,"priyank@improwised.com","2024-09-30T14:59:22.549Z","\u002Fcontent\u002Fservices\u002F6",1000,22,21,20,19,17,"Platform Engineering",28,27,"image\u002Fwebp",26,25,23,15,4,16,14,2,3,13,9,10,"Mansi","Pancholi","86701c80-2aba-48e2-90c1-d47cda4fdcd3",1500,"platform engineering","DevOps","From Complexity to Control: Using Teleport for Simplified Kubernetes Access Management","Munir","Khakhi","9f47a2fb-c13e-4cf3-99fa-7117ef11e769","Secrets Management in CD Environments with GitOps and SOPS","Infrastructure Testing with OpenTofu and Acceptance Tests","Seeding a MongoDB Database using Docker Compose",2000,"5 minutes","IDP",11,6,5,1,"2024-09-10T06:10:33.301Z","Implementing Feature Flags with Flipt Using Golang and gRPC","2024-09-02T06:59:54.060Z",1024,"2024-07-24T07:18:38.329Z","2024-07-08T06:21:36.133Z","2024-06-25T07:25:11.692Z","Securing Kubernetes: Beyond RBAC and Pod Security Policies (PSP)","2024-06-17T11:46:18.871Z","Multi-Stage Build for CI\u002FCD Pipeline using Dockerfile","2024-06-10T10:32:00.061Z",18,"Best Practices for Writing Dockerfiles","2024-06-04T13:35:48.368Z","Debugging OpenTofu with Remote Execution","2024-04-26T11:24:37.567Z","Continuous Delivery using GitOps Principles with FluxCD","2024-04-12T10:25:27.082Z","Advanced Configurations for Kubernetes Network Policies","2024-04-04T06:35:45.473Z","4 minutes","2024-02-20T15:36:23.461Z","2024-02-09T10:32:41.262Z",3125,12,"2024-02-14T10:13:56.361Z",2380,"a8418846-5723-4563-86df-99615438090f","mansi@improwised.com","2024-07-23T08:33:31.218Z","\u002Fcontent\u002Fteam","2024-02-09T11:57:18.591Z","2024-02-14T10:16:18.495Z","2023-12-28T11:32:23.238Z","S6-Overlay quickstart","2023-11-09T08:23:10.354Z","cloud infrastructure","3 min","2024-02-14T10:17:38.089Z",773,"internal developer platform","13211b9e-e0e5-4c20-9807-0c506a3d3a2d","2024-05-20T06:47:47.557Z","2024-02-14T10:19:24.986Z","Kubernetes","2024-02-14T10:20:57.765Z",1206,"2024-02-14T10:21:34.252Z",540)));